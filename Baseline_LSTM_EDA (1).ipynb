{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2Pphl66WCXH"
   },
   "source": [
    "#Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T05:26:38.779024Z",
     "iopub.status.busy": "2022-08-25T05:26:38.778094Z",
     "iopub.status.idle": "2022-08-25T05:26:42.779612Z",
     "shell.execute_reply": "2022-08-25T05:26:42.777714Z",
     "shell.execute_reply.started": "2022-08-25T05:26:38.778957Z"
    },
    "tags": []
   },
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:14.391122Z",
     "iopub.status.busy": "2022-09-01T01:23:14.390156Z",
     "iopub.status.idle": "2022-09-01T01:23:14.410650Z",
     "shell.execute_reply": "2022-09-01T01:23:14.409346Z",
     "shell.execute_reply.started": "2022-09-01T01:23:14.391046Z"
    },
    "executionInfo": {
     "elapsed": 2451,
     "status": "ok",
     "timestamp": 1659688741529,
     "user": {
      "displayName": "SJ D",
      "userId": "12288886370317310736"
     },
     "user_tz": -540
    },
    "id": "xuQrCCfNzR_P",
    "outputId": "43181691-4fdb-44a8-cebe-73e6c1c82ba2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot #plotly offline mode \n",
    "init_notebook_mode(connected = True)\n",
    "import plotly.graph_objs as go #plotly graphical object\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "DATASET_PATH = os.path.join('/workspace/Competition/TRAFFIC/01_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:14.757899Z",
     "iopub.status.busy": "2022-09-01T01:23:14.757027Z",
     "iopub.status.idle": "2022-09-01T01:23:14.778266Z",
     "shell.execute_reply": "2022-09-01T01:23:14.776278Z",
     "shell.execute_reply.started": "2022-09-01T01:23:14.757838Z"
    },
    "id": "5UzPKtrpzR_S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 난수 생성기가 항상 일정한 값을 출력하게 하기 위해 seed 고정\n",
    "random_seed = 2021\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:15.295838Z",
     "iopub.status.busy": "2022-09-01T01:23:15.294957Z",
     "iopub.status.idle": "2022-09-01T01:23:15.348058Z",
     "shell.execute_reply": "2022-09-01T01:23:15.347194Z",
     "shell.execute_reply.started": "2022-09-01T01:23:15.295776Z"
    },
    "id": "JT9fdJA_Zui2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## EDA\n",
    "train=pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "\n",
    "val=pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))\n",
    "\n",
    "test=pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:15.548596Z",
     "iopub.status.busy": "2022-09-01T01:23:15.547741Z",
     "iopub.status.idle": "2022-09-01T01:23:15.556113Z",
     "shell.execute_reply": "2022-09-01T01:23:15.554641Z",
     "shell.execute_reply.started": "2022-09-01T01:23:15.548536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 무시\n",
    "pd.set_option('mode.chained_assignment',  None) # <==== 경고를 끈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:15.829315Z",
     "iopub.status.busy": "2022-09-01T01:23:15.828428Z",
     "iopub.status.idle": "2022-09-01T01:23:17.844061Z",
     "shell.execute_reply": "2022-09-01T01:23:17.843179Z",
     "shell.execute_reply.started": "2022-09-01T01:23:15.829254Z"
    },
    "executionInfo": {
     "elapsed": 3689,
     "status": "ok",
     "timestamp": 1659688745212,
     "user": {
      "displayName": "SJ D",
      "userId": "12288886370317310736"
     },
     "user_tz": -540
    },
    "id": "dCjJEyRgSnPV",
    "outputId": "de3d2d86-0224-44b1-ca66-5d899e35efbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#index 날짜화 for feature\n",
    "def datesetting(train): \n",
    "    train['날짜1']=train['날짜'].astype(str)\n",
    "    train['시간1']=train['시간'].astype(str)\n",
    "    train['date']=train['날짜']+train['시간']\n",
    "\n",
    "    for i in range (0,len(train['날짜'])):\n",
    "      a= train['날짜1'][i]\n",
    "      train['날짜1'][i] =dt.datetime.strptime(a, '%Y%m%d')\n",
    "\n",
    "    for i in range(0,len(train['시간'])):\n",
    "      b= train['시간1'][i]\n",
    "      train['시간1'][i]=dt.datetime.strptime(b, '%H')\n",
    "\n",
    "    for i in range(0,len(train['날짜'])):\n",
    "      train['date'][i]=train['날짜1'][i]+timedelta(hours=train['시간1'][i].hour)\n",
    "\n",
    "\n",
    "#날짜 index 적용\n",
    "datesetting(train)\n",
    "datesetting(val)\n",
    "datesetting(test)\n",
    "train=train.drop(['날짜1','시간1'],axis=1)\n",
    "val=val.drop(['날짜1','시간1'],axis=1)\n",
    "test=test.drop(['날짜1','시간1'],axis=1)\n",
    "\n",
    "\n",
    "# index 설정하여 각 기간별 columns 생성하는 함수(create_features)\n",
    "train=train.set_index('date')\n",
    "val=val.set_index('date')\n",
    "test=test.set_index('date')\n",
    "# test=test[test.index >= '2020-05-25']\n",
    "df= pd.concat([train,val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHacfwiDzR_Y"
   },
   "source": [
    "# Dataloader\n",
    "* 한 칼럼에 대한 7일(168행) 데이터를 input_data, 뒤따르는 7일 데이터를 output_data로 반환합니다.\n",
    "* 도로별 차이를 두지 않고 모든 도로를 동일한 타입의 데이터로 취급합니다.\n",
    "* 모든 csv 파일의 마지막 168행은 예측해야하는 값이므로 input으로 들어가지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:17.846838Z",
     "iopub.status.busy": "2022-09-01T01:23:17.846532Z",
     "iopub.status.idle": "2022-09-01T01:23:17.864020Z",
     "shell.execute_reply": "2022-09-01T01:23:17.863264Z",
     "shell.execute_reply.started": "2022-09-01T01:23:17.846813Z"
    },
    "id": "AXYxoiRbzR_Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):      # torch.utils.data.Dataset 클래스의 상속 클래스 CustomDataset 클래스 생성. 상속 클래스 생성시 __init__, __getitem__, __len__함수는 기본적으로 정의해줘야 함.\n",
    "                                        # CustomeDataset을 쓰는 이유 - 데이터의 크기가 클 때, 데이터 전체를 불러와서 계산하는 형태가 아닌 데이터 내에서 일부를 불러오면서 계산하기 위함.\n",
    "    \n",
    "    def __init__(self, root, seq_len, batch_size=64, phase='train'):      # 데이터 로드 단계에 사용될 여러 변수들을 'self.변수명'의 형태로 지정해두는 함수\n",
    "        \n",
    "        self.root = root      # CustomDataset 객체 생성 시 데이터 경로 앞부분(공통 부분)을 root로 입력받아 저장\n",
    "        self.phase = phase      # CustomDataset 객체 생성 시 데이터 경로 뒷부분(train/validate/test)을 phase로 입력받아 저장\n",
    "        self.label_path = os.path.join(self.root, self.phase + '.csv')      # 데이터 전체 경로 생성\n",
    "        df = pd.read_csv(self.label_path)      # 생성한 데이터 전체 경로로부터 데이터 로드\n",
    "        \n",
    "        self.seq_len = seq_len * 24      # 일 단위 기간을 입력 받은 후 시간 단위 기간으로 변환하여 저장\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "        # self구조의 함수를 설정해서 자기 함수를 초기화 해서 다시 사용할 수 있도록 init처리\n",
    "        \n",
    "        timestamps = [(i, j) for (i, j) in zip(list(df['날짜']), list(df['시간']))]      # 날짜와 시간 정보가 튜플로 들어 있는 리스트 생성\n",
    "        categories = df.columns.values.tolist()[2:]      # 도로명 column list 생성\n",
    "\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for t in range(len(timestamps)):\n",
    "            temp_input_data = []\n",
    "            temp_output_data = []\n",
    "            for col in categories:\n",
    "                road = df[col].tolist()\n",
    "                inp = [float(i) for i in road[t:t+self.seq_len]]      # input 데이터 시계열 구간 설정\n",
    "                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]      # output 데이터 시계열 구간 설정\n",
    "                temp_input_data.append(inp) \n",
    "                temp_output_data.append(outp)\n",
    "            input_data.append(temp_input_data)\n",
    "            output_data.append(temp_output_data)\n",
    "            \n",
    "# input_data : [[첫번째 input 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 input 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#               [마지막 input 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 input 기간 동안의 35번째 도로의 통행량 list]]\n",
    "# output_data : [[첫번째 output 기간 동안의 첫번째 도로의 통행량 list, ..., 첫번째 output 기간 동안의 35번째 도로의 통행량 list], ...,\n",
    "#                [마지막 output 기간 동안의 첫번째 도로의 통행량 list, ..., 마지막 output 기간 동안의 35번째 도로의 통행량 list]]\n",
    "        \n",
    "        self.labels['timestamp'] = timestamps\n",
    "        self.labels['category'] = categories\n",
    "        self.labels['input'] = input_data\n",
    "        self.labels['output'] = output_data\n",
    "\n",
    "    def __getitem__(self, index):      # index를 가지고 데이터를 하나씩 불러올 수 있게 하는 함수\n",
    "    \n",
    "#         데이터 내 index가 부여되는 형태\n",
    "\n",
    "#                 | road_1    road_2    ...  road_35\n",
    "#                -------------------------------------\n",
    "#         time_1  | index_0   index_1   ...  index_34\n",
    "#         time_2  | index_35  index_36  ...  index_69\n",
    "\n",
    "        row = index // 35      # index를 35(도로수)로 나눈 몫  ex) 71//35 -> 2\n",
    "        col = index % 35      # index를 35(도로수)로 나눈 나머지  ex) 71%35 -> 1\n",
    "\n",
    "        timestamp = self.labels['timestamp'][row]      # (날짜, 시간) 튜플이 들어있는 list에서 row번째 시점에 해당하는 튜플 - timestamp\n",
    "        category = self.labels['category'][col]      # 도로명 column list에서 col번째 도로에 해당하는 element - column\n",
    "        \n",
    "        input_data = torch.tensor(self.labels['input'][row][col])  \n",
    "           # input_data list에서, row번째 시점의 col번째 도로 교통량 정보\n",
    "        norm = input_data[-1]+1   # input_data list에서 마지막 도로 교통량 정보를 norm으로 담는다.\n",
    "        \n",
    "        input_data = input_data / norm\n",
    "        \n",
    "        if self.phase != 'test':\n",
    "            output_data = torch.tensor(self.labels['output'][row][col]) / norm\n",
    "        else:\n",
    "            output_data = []\n",
    "      \n",
    "        return timestamp, category, (input_data, output_data)\n",
    "\n",
    "    def __len__(self):      # getitem 함수를 통해 데이터를 불러오려면,전체 index 길이를 알아야 한다.\n",
    "        return (len(self.labels['timestamp']) - (self.seq_len * 2) + 1) * 35      # 특정 시점이 아닌 특정 기간을 하나의 data 단위로 설정하면, 전체 샘플 수는 감소함을 반영 \n",
    "\n",
    "\n",
    "def data_loader(root, phase='train', batch_size=64, seq_len=7, drop_last=False):\n",
    "    if phase == 'train':\n",
    "        shuffle = False\n",
    "        \n",
    "        ##shuffle이 False\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CustomDataset(root, seq_len, batch_size, phase)\n",
    "    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:17.865256Z",
     "iopub.status.busy": "2022-09-01T01:23:17.865010Z",
     "iopub.status.idle": "2022-09-01T01:23:17.870285Z",
     "shell.execute_reply": "2022-09-01T01:23:17.869557Z",
     "shell.execute_reply.started": "2022-09-01T01:23:17.865234Z"
    },
    "id": "u1w3zbScaXS2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:17.873017Z",
     "iopub.status.busy": "2022-09-01T01:23:17.872564Z",
     "iopub.status.idle": "2022-09-01T01:23:17.885992Z",
     "shell.execute_reply": "2022-09-01T01:23:17.885185Z",
     "shell.execute_reply.started": "2022-09-01T01:23:17.872989Z"
    },
    "id": "exrggT7VLH7H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "MinMax_train = scaler.fit_transform(train.drop(['날짜','시간'],axis=1))\n",
    "MinMax_train = pd.DataFrame(MinMax_train, columns =train.drop(['날짜','시간'],axis = 1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:17.887579Z",
     "iopub.status.busy": "2022-09-01T01:23:17.887275Z",
     "iopub.status.idle": "2022-09-01T01:23:17.920468Z",
     "shell.execute_reply": "2022-09-01T01:23:17.919737Z",
     "shell.execute_reply.started": "2022-09-01T01:23:17.887553Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1659688745647,
     "user": {
      "displayName": "SJ D",
      "userId": "12288886370317310736"
     },
     "user_tz": -540
    },
    "id": "dWnQKKNxbxOV",
    "outputId": "e2470b95-ea4f-46c1-8337-92c6294669ae",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>...</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143505</td>\n",
       "      <td>0.095255</td>\n",
       "      <td>0.056923</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.064563</td>\n",
       "      <td>0.036203</td>\n",
       "      <td>0.110488</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.056679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.200955</td>\n",
       "      <td>0.041218</td>\n",
       "      <td>0.083679</td>\n",
       "      <td>0.070693</td>\n",
       "      <td>0.102993</td>\n",
       "      <td>0.100476</td>\n",
       "      <td>0.102912</td>\n",
       "      <td>20200101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153955</td>\n",
       "      <td>0.094752</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>0.047913</td>\n",
       "      <td>0.067084</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.116077</td>\n",
       "      <td>0.204678</td>\n",
       "      <td>0.058864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171998</td>\n",
       "      <td>0.249233</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.082647</td>\n",
       "      <td>0.073906</td>\n",
       "      <td>0.091511</td>\n",
       "      <td>0.096896</td>\n",
       "      <td>0.108628</td>\n",
       "      <td>20200101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.114827</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>0.064750</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.042446</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>0.091851</td>\n",
       "      <td>0.077950</td>\n",
       "      <td>0.193241</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104216</td>\n",
       "      <td>0.154183</td>\n",
       "      <td>0.053836</td>\n",
       "      <td>0.070919</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.069205</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>0.091778</td>\n",
       "      <td>20200101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.091864</td>\n",
       "      <td>0.068488</td>\n",
       "      <td>0.049489</td>\n",
       "      <td>0.036941</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.073020</td>\n",
       "      <td>0.064181</td>\n",
       "      <td>0.207016</td>\n",
       "      <td>0.048779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074424</td>\n",
       "      <td>0.089679</td>\n",
       "      <td>0.038695</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>0.040733</td>\n",
       "      <td>0.065369</td>\n",
       "      <td>0.059730</td>\n",
       "      <td>0.099851</td>\n",
       "      <td>20200101</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089804</td>\n",
       "      <td>0.087721</td>\n",
       "      <td>0.052454</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>0.074809</td>\n",
       "      <td>0.037420</td>\n",
       "      <td>0.077776</td>\n",
       "      <td>0.095601</td>\n",
       "      <td>0.320524</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075898</td>\n",
       "      <td>0.083343</td>\n",
       "      <td>0.042270</td>\n",
       "      <td>0.053436</td>\n",
       "      <td>0.047160</td>\n",
       "      <td>0.077560</td>\n",
       "      <td>0.090772</td>\n",
       "      <td>0.151917</td>\n",
       "      <td>20200101</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>0.537370</td>\n",
       "      <td>0.504387</td>\n",
       "      <td>0.219865</td>\n",
       "      <td>0.266796</td>\n",
       "      <td>0.409010</td>\n",
       "      <td>0.355087</td>\n",
       "      <td>0.510044</td>\n",
       "      <td>0.480195</td>\n",
       "      <td>0.539846</td>\n",
       "      <td>0.280405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334038</td>\n",
       "      <td>0.406115</td>\n",
       "      <td>0.325961</td>\n",
       "      <td>0.488414</td>\n",
       "      <td>0.357811</td>\n",
       "      <td>0.444970</td>\n",
       "      <td>0.505394</td>\n",
       "      <td>0.547040</td>\n",
       "      <td>20200517</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>0.526383</td>\n",
       "      <td>0.455291</td>\n",
       "      <td>0.187643</td>\n",
       "      <td>0.226599</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.292138</td>\n",
       "      <td>0.467470</td>\n",
       "      <td>0.434018</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321823</td>\n",
       "      <td>0.419044</td>\n",
       "      <td>0.288774</td>\n",
       "      <td>0.452527</td>\n",
       "      <td>0.302712</td>\n",
       "      <td>0.394577</td>\n",
       "      <td>0.460078</td>\n",
       "      <td>0.486577</td>\n",
       "      <td>20200517</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>0.527511</td>\n",
       "      <td>0.374054</td>\n",
       "      <td>0.137893</td>\n",
       "      <td>0.173583</td>\n",
       "      <td>0.232192</td>\n",
       "      <td>0.182394</td>\n",
       "      <td>0.443186</td>\n",
       "      <td>0.362494</td>\n",
       "      <td>0.299820</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297290</td>\n",
       "      <td>0.400119</td>\n",
       "      <td>0.224037</td>\n",
       "      <td>0.393951</td>\n",
       "      <td>0.231264</td>\n",
       "      <td>0.325872</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.380089</td>\n",
       "      <td>20200517</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3277</th>\n",
       "      <td>0.409322</td>\n",
       "      <td>0.246494</td>\n",
       "      <td>0.091761</td>\n",
       "      <td>0.105762</td>\n",
       "      <td>0.158196</td>\n",
       "      <td>0.110313</td>\n",
       "      <td>0.331153</td>\n",
       "      <td>0.248959</td>\n",
       "      <td>0.193654</td>\n",
       "      <td>0.135953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217150</td>\n",
       "      <td>0.314446</td>\n",
       "      <td>0.155165</td>\n",
       "      <td>0.262568</td>\n",
       "      <td>0.141291</td>\n",
       "      <td>0.197236</td>\n",
       "      <td>0.253757</td>\n",
       "      <td>0.240796</td>\n",
       "      <td>20200517</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>0.259115</td>\n",
       "      <td>0.136499</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>0.055785</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.065506</td>\n",
       "      <td>0.191052</td>\n",
       "      <td>0.130551</td>\n",
       "      <td>0.118348</td>\n",
       "      <td>0.078136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131256</td>\n",
       "      <td>0.202859</td>\n",
       "      <td>0.078862</td>\n",
       "      <td>0.150315</td>\n",
       "      <td>0.069464</td>\n",
       "      <td>0.108142</td>\n",
       "      <td>0.143483</td>\n",
       "      <td>0.124773</td>\n",
       "      <td>20200517</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3279 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            10       100       101       120       121       140       150  \\\n",
       "0     0.143505  0.095255  0.056923  0.044944  0.064563  0.036203  0.110488   \n",
       "1     0.153955  0.094752  0.072751  0.047913  0.067084  0.042331  0.121947   \n",
       "2     0.114827  0.073254  0.064750  0.040328  0.042446  0.037380  0.091851   \n",
       "3     0.091864  0.068488  0.049489  0.036941  0.041511  0.032063  0.073020   \n",
       "4     0.089804  0.087721  0.052454  0.032125  0.074809  0.037420  0.077776   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3274  0.537370  0.504387  0.219865  0.266796  0.409010  0.355087  0.510044   \n",
       "3275  0.526383  0.455291  0.187643  0.226599  0.329119  0.292138  0.467470   \n",
       "3276  0.527511  0.374054  0.137893  0.173583  0.232192  0.182394  0.443186   \n",
       "3277  0.409322  0.246494  0.091761  0.105762  0.158196  0.110313  0.331153   \n",
       "3278  0.259115  0.136499  0.051233  0.055785  0.073305  0.065506  0.191052   \n",
       "\n",
       "           160       200       201  ...      1100      1200      1510  \\\n",
       "0     0.115512  0.164181  0.056679  ...  0.147451  0.200955  0.041218   \n",
       "1     0.116077  0.204678  0.058864  ...  0.171998  0.249233  0.044969   \n",
       "2     0.077950  0.193241  0.055004  ...  0.104216  0.154183  0.053836   \n",
       "3     0.064181  0.207016  0.048779  ...  0.074424  0.089679  0.038695   \n",
       "4     0.095601  0.320524  0.037826  ...  0.075898  0.083343  0.042270   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3274  0.480195  0.539846  0.280405  ...  0.334038  0.406115  0.325961   \n",
       "3275  0.434018  0.429357  0.252334  ...  0.321823  0.419044  0.288774   \n",
       "3276  0.362494  0.299820  0.201670  ...  0.297290  0.400119  0.224037   \n",
       "3277  0.248959  0.193654  0.135953  ...  0.217150  0.314446  0.155165   \n",
       "3278  0.130551  0.118348  0.078136  ...  0.131256  0.202859  0.078862   \n",
       "\n",
       "          2510      3000      4510      5510      6000        날짜  시간  \n",
       "0     0.083679  0.070693  0.102993  0.100476  0.102912  20200101   0  \n",
       "1     0.082647  0.073906  0.091511  0.096896  0.108628  20200101   1  \n",
       "2     0.070919  0.046404  0.069205  0.071883  0.091778  20200101   2  \n",
       "3     0.063298  0.040733  0.065369  0.059730  0.099851  20200101   3  \n",
       "4     0.053436  0.047160  0.077560  0.090772  0.151917  20200101   4  \n",
       "...        ...       ...       ...       ...       ...       ...  ..  \n",
       "3274  0.488414  0.357811  0.444970  0.505394  0.547040  20200517  19  \n",
       "3275  0.452527  0.302712  0.394577  0.460078  0.486577  20200517  20  \n",
       "3276  0.393951  0.231264  0.325872  0.368600  0.380089  20200517  21  \n",
       "3277  0.262568  0.141291  0.197236  0.253757  0.240796  20200517  22  \n",
       "3278  0.150315  0.069464  0.108142  0.143483  0.124773  20200517  23  \n",
       "\n",
       "[3279 rows x 37 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.reset_index()\n",
    "MinMax_train[['날짜','시간']]=train[['날짜','시간']]\n",
    "train=train.set_index('date')\n",
    "MinMax_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:18.000740Z",
     "iopub.status.busy": "2022-09-01T01:23:17.999975Z",
     "iopub.status.idle": "2022-09-01T01:23:18.010216Z",
     "shell.execute_reply": "2022-09-01T01:23:18.008806Z",
     "shell.execute_reply.started": "2022-09-01T01:23:18.000685Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1659688745648,
     "user": {
      "displayName": "SJ D",
      "userId": "12288886370317310736"
     },
     "user_tz": -540
    },
    "id": "IlbtP7OmHRMA",
    "outputId": "57c1bc2d-8b91-4e84-fe35-c8f171e3ac27",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.625"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3279/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbnadMN0NJBv"
   },
   "source": [
    "#Initialisation\n",
    "\n",
    "LSTM은 크게 3가지의 메인 파라미터를 가지고 있다.\n",
    "\n",
    "- input_size : input x에서의 expected features의 수\n",
    "- hidden_size : hidden state h에서의 features 수\n",
    "- bias : defaults to true\n",
    "\n",
    "#Forward method\n",
    "\n",
    "- input : tensor의 input의 shape는 (batch, input_size)\n",
    "- h_0 : initial hidden state를 담는 tensor(각 element의 배치마다) / shape (batch, hidden_size)\n",
    "- c_0 : initial hidden state를 담는 tensor(각 element의 배치마다) / shape (batch, hidden_size)\n",
    "\n",
    "여기서 LSTM의 Cells가 연결되기 위해선 outputs의 tensor shape이 (h_1, c_1)이 되어야한다.\n",
    "\n",
    "Ex에서는 batch size가 64. n_samples = x.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYJmzMr3zR_a"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:23:19.599817Z",
     "iopub.status.busy": "2022-09-01T01:23:19.598914Z",
     "iopub.status.idle": "2022-09-01T01:23:19.616862Z",
     "shell.execute_reply": "2022-09-01T01:23:19.615546Z",
     "shell.execute_reply.started": "2022-09-01T01:23:19.599754Z"
    },
    "id": "OIUFtPmTWZ0z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):  # init 함수는 2개의 선형레이어를 초기화 한다. 하나는 출력, 다른 하나는 상태 벡터 또는 히든 벡터를 계산한다.\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.i2h =  nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    self.i2o =  nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self,input, hidden):   #forward 함수는 입력 벡터와 히든 벡터를 cat함수로 결합해 출력벡터와 숨겨진 상태를 생성하는 두 선형 레이어를 통과시킨다. \n",
    "      combined = torch.cat((input, hidden), 1)\n",
    "      hidden = self.i2h(combined)\n",
    "      output = self.i2o(combined)\n",
    "      output = self.softmax(output)   # 출력 layer에는 log_softmax함수를 적용.\n",
    "      return output, hidden\n",
    "\n",
    "    def initHidden(self):   # RNN을 처음 호출할 경우에 상태 벡터를 만드는 기능을 제공한다.\n",
    "      return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "# torch.cat은 두 텐서를 결합하며, 두 번째 전달 인자인 dim은 결합방향을 설정했다. 예를 들어 input이 [[1,1,],[2,2,]]이고 hidden이 [[11,11],[22,22]]라면, torch.cat((input,hidden),1)의 결과는 [[1,1,1,11,11],[2,2,22,22]]이 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGWyZSuJidbC"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:53.769629Z",
     "iopub.status.busy": "2022-09-01T01:38:53.768742Z",
     "iopub.status.idle": "2022-09-01T01:38:53.790496Z",
     "shell.execute_reply": "2022-09-01T01:38:53.789358Z",
     "shell.execute_reply.started": "2022-09-01T01:38:53.769569Z"
    },
    "id": "VMuqLNfwzR_b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):      # torch.nn.Module 클래스의 상속 class LSTMNet class 생성. 상속 클래스 생성시 __init__, forward 함수는 기본적으로 정의해줘야 함.\n",
    "    def __init__(self,\n",
    "                 input_size=168,      # input 길이는 168시간(7일 X 24시간)\n",
    "                 hidden_size=1024,\n",
    "                 output_size=168,      # output 길이는 168시간(7일 X 24시간)\n",
    "                 batch_size=64,\n",
    "                 num_layers=3,\n",
    "                 dropout=0,\n",
    "                 batch_first=False):      # batch_first(default=False) : 배치 차원을 첫번째 차원으로 하여 데이터를 불러올 것인지 여부 RNN예시에서는 shuffle을 False로 바꾸고 batch_first를 True로 바꿈.\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        ##### Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "        \n",
    "        ##### Layer 2\n",
    "        self.lstm2 = nn.LSTM(hidden_size, \n",
    "                             hidden_size,\n",
    "                             dropout=0.2,\n",
    "                             num_layers=num_layers)\n",
    "        \n",
    "        ##### Finalize\n",
    "        self.linear = nn.Linear(hidden_size, \n",
    "                                output_size)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):      # forward 함수를 직접 호출할 일은 없음. model = LSTMNet() 객체를 생성한 다음, \n",
    "                                           # predictions, (h2, c_2) = model(input_data, h_in, c_in) 형태로 return을 받으면 됨\n",
    "        # h_in = nn.Parameter(h_in.type(dtype), requires_grad = True)\n",
    "        # c_in = nn.Parameter(c_in.type(dtype), requires_grad = True)   \n",
    "\n",
    "        lstm_out, (h_1, c_1) = self.lstm1(x)\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "\n",
    "\n",
    "        # Layer2\n",
    "        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n",
    "        lstm_out = self.activation(lstm_out)\n",
    "        \n",
    "\n",
    "        # Final\n",
    "        predictions = self.linear(lstm_out)\n",
    "        \n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:54.953346Z",
     "iopub.status.busy": "2022-09-01T01:38:54.952297Z",
     "iopub.status.idle": "2022-09-01T01:38:54.988603Z",
     "shell.execute_reply": "2022-09-01T01:38:54.987712Z",
     "shell.execute_reply.started": "2022-09-01T01:38:54.953279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>20200101</td>\n",
       "      <td>0</td>\n",
       "      <td>83247</td>\n",
       "      <td>19128</td>\n",
       "      <td>2611</td>\n",
       "      <td>5161</td>\n",
       "      <td>1588</td>\n",
       "      <td>892</td>\n",
       "      <td>32263</td>\n",
       "      <td>1636</td>\n",
       "      <td>...</td>\n",
       "      <td>1311</td>\n",
       "      <td>3482</td>\n",
       "      <td>11299</td>\n",
       "      <td>7072</td>\n",
       "      <td>1176</td>\n",
       "      <td>3810</td>\n",
       "      <td>748</td>\n",
       "      <td>3920</td>\n",
       "      <td>2133</td>\n",
       "      <td>3799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>20200101</td>\n",
       "      <td>1</td>\n",
       "      <td>89309</td>\n",
       "      <td>19027</td>\n",
       "      <td>3337</td>\n",
       "      <td>5502</td>\n",
       "      <td>1650</td>\n",
       "      <td>1043</td>\n",
       "      <td>35609</td>\n",
       "      <td>1644</td>\n",
       "      <td>...</td>\n",
       "      <td>1162</td>\n",
       "      <td>3849</td>\n",
       "      <td>13180</td>\n",
       "      <td>8771</td>\n",
       "      <td>1283</td>\n",
       "      <td>3763</td>\n",
       "      <td>782</td>\n",
       "      <td>3483</td>\n",
       "      <td>2057</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>20200101</td>\n",
       "      <td>2</td>\n",
       "      <td>66611</td>\n",
       "      <td>14710</td>\n",
       "      <td>2970</td>\n",
       "      <td>4631</td>\n",
       "      <td>1044</td>\n",
       "      <td>921</td>\n",
       "      <td>26821</td>\n",
       "      <td>1104</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>2299</td>\n",
       "      <td>7986</td>\n",
       "      <td>5426</td>\n",
       "      <td>1536</td>\n",
       "      <td>3229</td>\n",
       "      <td>491</td>\n",
       "      <td>2634</td>\n",
       "      <td>1526</td>\n",
       "      <td>3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>20200101</td>\n",
       "      <td>3</td>\n",
       "      <td>53290</td>\n",
       "      <td>13753</td>\n",
       "      <td>2270</td>\n",
       "      <td>4242</td>\n",
       "      <td>1021</td>\n",
       "      <td>790</td>\n",
       "      <td>21322</td>\n",
       "      <td>909</td>\n",
       "      <td>...</td>\n",
       "      <td>632</td>\n",
       "      <td>1716</td>\n",
       "      <td>5703</td>\n",
       "      <td>3156</td>\n",
       "      <td>1104</td>\n",
       "      <td>2882</td>\n",
       "      <td>431</td>\n",
       "      <td>2488</td>\n",
       "      <td>1268</td>\n",
       "      <td>3686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>20200101</td>\n",
       "      <td>4</td>\n",
       "      <td>52095</td>\n",
       "      <td>17615</td>\n",
       "      <td>2406</td>\n",
       "      <td>3689</td>\n",
       "      <td>1840</td>\n",
       "      <td>922</td>\n",
       "      <td>22711</td>\n",
       "      <td>1354</td>\n",
       "      <td>...</td>\n",
       "      <td>875</td>\n",
       "      <td>2421</td>\n",
       "      <td>5816</td>\n",
       "      <td>2933</td>\n",
       "      <td>1206</td>\n",
       "      <td>2433</td>\n",
       "      <td>499</td>\n",
       "      <td>2952</td>\n",
       "      <td>1927</td>\n",
       "      <td>5608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17 19:00:00</th>\n",
       "      <td>20200517</td>\n",
       "      <td>19</td>\n",
       "      <td>311727</td>\n",
       "      <td>101285</td>\n",
       "      <td>10085</td>\n",
       "      <td>30637</td>\n",
       "      <td>10060</td>\n",
       "      <td>8749</td>\n",
       "      <td>148935</td>\n",
       "      <td>6801</td>\n",
       "      <td>...</td>\n",
       "      <td>6726</td>\n",
       "      <td>15431</td>\n",
       "      <td>25597</td>\n",
       "      <td>14292</td>\n",
       "      <td>9300</td>\n",
       "      <td>22238</td>\n",
       "      <td>3786</td>\n",
       "      <td>16936</td>\n",
       "      <td>10729</td>\n",
       "      <td>20194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17 20:00:00</th>\n",
       "      <td>20200517</td>\n",
       "      <td>20</td>\n",
       "      <td>305354</td>\n",
       "      <td>91426</td>\n",
       "      <td>8607</td>\n",
       "      <td>26021</td>\n",
       "      <td>8095</td>\n",
       "      <td>7198</td>\n",
       "      <td>136503</td>\n",
       "      <td>6147</td>\n",
       "      <td>...</td>\n",
       "      <td>5501</td>\n",
       "      <td>15378</td>\n",
       "      <td>24661</td>\n",
       "      <td>14747</td>\n",
       "      <td>8239</td>\n",
       "      <td>20604</td>\n",
       "      <td>3203</td>\n",
       "      <td>15018</td>\n",
       "      <td>9767</td>\n",
       "      <td>17962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17 21:00:00</th>\n",
       "      <td>20200517</td>\n",
       "      <td>21</td>\n",
       "      <td>306008</td>\n",
       "      <td>75113</td>\n",
       "      <td>6325</td>\n",
       "      <td>19933</td>\n",
       "      <td>5711</td>\n",
       "      <td>4494</td>\n",
       "      <td>129412</td>\n",
       "      <td>5134</td>\n",
       "      <td>...</td>\n",
       "      <td>4216</td>\n",
       "      <td>12558</td>\n",
       "      <td>22781</td>\n",
       "      <td>14081</td>\n",
       "      <td>6392</td>\n",
       "      <td>17937</td>\n",
       "      <td>2447</td>\n",
       "      <td>12403</td>\n",
       "      <td>7825</td>\n",
       "      <td>14031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17 22:00:00</th>\n",
       "      <td>20200517</td>\n",
       "      <td>22</td>\n",
       "      <td>237447</td>\n",
       "      <td>49498</td>\n",
       "      <td>4209</td>\n",
       "      <td>12145</td>\n",
       "      <td>3891</td>\n",
       "      <td>2718</td>\n",
       "      <td>96698</td>\n",
       "      <td>3526</td>\n",
       "      <td>...</td>\n",
       "      <td>2578</td>\n",
       "      <td>8870</td>\n",
       "      <td>16640</td>\n",
       "      <td>11066</td>\n",
       "      <td>4427</td>\n",
       "      <td>11955</td>\n",
       "      <td>1495</td>\n",
       "      <td>7507</td>\n",
       "      <td>5387</td>\n",
       "      <td>8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17 23:00:00</th>\n",
       "      <td>20200517</td>\n",
       "      <td>23</td>\n",
       "      <td>150312</td>\n",
       "      <td>27410</td>\n",
       "      <td>2350</td>\n",
       "      <td>6406</td>\n",
       "      <td>1803</td>\n",
       "      <td>1614</td>\n",
       "      <td>55788</td>\n",
       "      <td>1849</td>\n",
       "      <td>...</td>\n",
       "      <td>1377</td>\n",
       "      <td>5021</td>\n",
       "      <td>10058</td>\n",
       "      <td>7139</td>\n",
       "      <td>2250</td>\n",
       "      <td>6844</td>\n",
       "      <td>735</td>\n",
       "      <td>4116</td>\n",
       "      <td>3046</td>\n",
       "      <td>4606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3279 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           날짜  시간      10     100    101    120    121   140  \\\n",
       "date                                                                           \n",
       "2020-01-01 00:00:00  20200101   0   83247   19128   2611   5161   1588   892   \n",
       "2020-01-01 01:00:00  20200101   1   89309   19027   3337   5502   1650  1043   \n",
       "2020-01-01 02:00:00  20200101   2   66611   14710   2970   4631   1044   921   \n",
       "2020-01-01 03:00:00  20200101   3   53290   13753   2270   4242   1021   790   \n",
       "2020-01-01 04:00:00  20200101   4   52095   17615   2406   3689   1840   922   \n",
       "...                       ...  ..     ...     ...    ...    ...    ...   ...   \n",
       "2020-05-17 19:00:00  20200517  19  311727  101285  10085  30637  10060  8749   \n",
       "2020-05-17 20:00:00  20200517  20  305354   91426   8607  26021   8095  7198   \n",
       "2020-05-17 21:00:00  20200517  21  306008   75113   6325  19933   5711  4494   \n",
       "2020-05-17 22:00:00  20200517  22  237447   49498   4209  12145   3891  2718   \n",
       "2020-05-17 23:00:00  20200517  23  150312   27410   2350   6406   1803  1614   \n",
       "\n",
       "                        150   160  ...  1020   1040   1100   1200  1510  \\\n",
       "date                               ...                                    \n",
       "2020-01-01 00:00:00   32263  1636  ...  1311   3482  11299   7072  1176   \n",
       "2020-01-01 01:00:00   35609  1644  ...  1162   3849  13180   8771  1283   \n",
       "2020-01-01 02:00:00   26821  1104  ...   768   2299   7986   5426  1536   \n",
       "2020-01-01 03:00:00   21322   909  ...   632   1716   5703   3156  1104   \n",
       "2020-01-01 04:00:00   22711  1354  ...   875   2421   5816   2933  1206   \n",
       "...                     ...   ...  ...   ...    ...    ...    ...   ...   \n",
       "2020-05-17 19:00:00  148935  6801  ...  6726  15431  25597  14292  9300   \n",
       "2020-05-17 20:00:00  136503  6147  ...  5501  15378  24661  14747  8239   \n",
       "2020-05-17 21:00:00  129412  5134  ...  4216  12558  22781  14081  6392   \n",
       "2020-05-17 22:00:00   96698  3526  ...  2578   8870  16640  11066  4427   \n",
       "2020-05-17 23:00:00   55788  1849  ...  1377   5021  10058   7139  2250   \n",
       "\n",
       "                      2510  3000   4510   5510   6000  \n",
       "date                                                   \n",
       "2020-01-01 00:00:00   3810   748   3920   2133   3799  \n",
       "2020-01-01 01:00:00   3763   782   3483   2057   4010  \n",
       "2020-01-01 02:00:00   3229   491   2634   1526   3388  \n",
       "2020-01-01 03:00:00   2882   431   2488   1268   3686  \n",
       "2020-01-01 04:00:00   2433   499   2952   1927   5608  \n",
       "...                    ...   ...    ...    ...    ...  \n",
       "2020-05-17 19:00:00  22238  3786  16936  10729  20194  \n",
       "2020-05-17 20:00:00  20604  3203  15018   9767  17962  \n",
       "2020-05-17 21:00:00  17937  2447  12403   7825  14031  \n",
       "2020-05-17 22:00:00  11955  1495   7507   5387   8889  \n",
       "2020-05-17 23:00:00   6844   735   4116   3046   4606  \n",
       "\n",
       "[3279 rows x 37 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.index>='2020-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:55.207569Z",
     "iopub.status.busy": "2022-09-01T01:38:55.206602Z",
     "iopub.status.idle": "2022-09-01T01:38:55.222838Z",
     "shell.execute_reply": "2022-09-01T01:38:55.221118Z",
     "shell.execute_reply.started": "2022-09-01T01:38:55.207504Z"
    },
    "id": "vzEKffK6zR_b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 결과 파일과 모델 가중치 파일 저장을 위해 log 디렉토리 생성. 중요한 파일이 덮어씌워지지 않도록 주의\n",
    "os.makedirs('log', exist_ok=True)      # log 폴더 생성, 이미 생성되었을 시 추가로 생성하지 않도록 exist_ok=True\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer):      # 모델 가중치 파일 저장 함수\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join('log', model_name + '.pth'))\n",
    "    print('model saved\\n')\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None):      # 모델 가중치 파일 로드 함수\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIl13C6PzR_c"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:55.704193Z",
     "iopub.status.busy": "2022-09-01T01:38:55.703276Z",
     "iopub.status.idle": "2022-09-01T01:38:55.714144Z",
     "shell.execute_reply": "2022-09-01T01:38:55.712375Z",
     "shell.execute_reply.started": "2022-09-01T01:38:55.704130Z"
    },
    "id": "dVArQ_rbzR_c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "num_epochs = 5\n",
    "base_lr = 0.001\n",
    "seq_len = 7\n",
    "\n",
    "input_size = 168\n",
    "hidden_size = 1008\n",
    "output_size = input_size\n",
    "batch_size = 64\n",
    "num_layers = 6  #시간축 방향으로는 vanishing 문제 어느정도 해결 but 세로축으로는 여전히 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwT7EXP-zR_c"
   },
   "source": [
    "## Training Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:57.087342Z",
     "iopub.status.busy": "2022-09-01T01:38:57.086447Z",
     "iopub.status.idle": "2022-09-01T01:38:57.792432Z",
     "shell.execute_reply": "2022-09-01T01:38:57.791571Z",
     "shell.execute_reply.started": "2022-09-01T01:38:57.087280Z"
    },
    "id": "9e6cMmVWzR_d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()      # 플랫폼 상 채점은 RMSE, 즉 MSE에 root를 씌운 값이기 때문에 사실상 평가지표와 같은 Loss입니다.\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=base_lr, weight_decay=1e-4)      # optimizer로는 Adam이 가장 무난합니다. Adam을 쓰면 learning_rate를 따로 지정해주지 않아도 알아서 조정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:57.794883Z",
     "iopub.status.busy": "2022-09-01T01:38:57.794437Z",
     "iopub.status.idle": "2022-09-01T01:38:57.800743Z",
     "shell.execute_reply": "2022-09-01T01:38:57.800033Z",
     "shell.execute_reply.started": "2022-09-01T01:38:57.794858Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 168])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:58.282079Z",
     "iopub.status.busy": "2022-09-01T01:38:58.281163Z",
     "iopub.status.idle": "2022-09-01T01:38:58.289786Z",
     "shell.execute_reply": "2022-09-01T01:38:58.288189Z",
     "shell.execute_reply.started": "2022-09-01T01:38:58.282017Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1659688748566,
     "user": {
      "displayName": "SJ D",
      "userId": "12288886370317310736"
     },
     "user_tz": -540
    },
    "id": "FepGagG_zR_d",
    "outputId": "87a42aff-2ff6-4672-fd92-a3417e012d04",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm1): LSTM(168, 1008, num_layers=6, dropout=0.2)\n",
      "  (lstm2): LSTM(1008, 1008, num_layers=6, dropout=0.2)\n",
      "  (linear): Linear(in_features=1008, out_features=168, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:38:58.582207Z",
     "iopub.status.busy": "2022-09-01T01:38:58.581382Z",
     "iopub.status.idle": "2022-09-01T01:38:58.592969Z",
     "shell.execute_reply": "2022-09-01T01:38:58.591765Z",
     "shell.execute_reply.started": "2022-09-01T01:38:58.582149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 168])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.unsqueeze(0).to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:39:02.426529Z",
     "iopub.status.busy": "2022-09-01T01:39:02.425630Z",
     "iopub.status.idle": "2022-09-01T01:39:17.853072Z",
     "shell.execute_reply": "2022-09-01T01:39:17.852179Z",
     "shell.execute_reply.started": "2022-09-01T01:39:02.426468Z"
    },
    "id": "Ny7m31uIzR_d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get data loader\n",
    "\n",
    "train_dataloader = data_loader(root=DATASET_PATH,\n",
    "                               phase='train',\n",
    "                               batch_size=1,\n",
    "                               seq_len=seq_len,\n",
    "                               drop_last=True\n",
    "                               )\n",
    "\n",
    "validate_dataloader = data_loader(root=DATASET_PATH,\n",
    "                                  phase='validate',\n",
    "                                  batch_size=1,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUQPrrJ_zR_d"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T01:39:34.097068Z",
     "iopub.status.busy": "2022-09-01T01:39:34.096055Z",
     "iopub.status.idle": "2022-09-01T01:39:34.117025Z",
     "shell.execute_reply": "2022-09-01T01:39:34.115662Z",
     "shell.execute_reply.started": "2022-09-01T01:39:34.097003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0205,  0.0254,  0.0226,  0.0052,  0.0306, -0.0265, -0.0191,\n",
       "           0.0015,  0.0181, -0.0165,  0.0189,  0.0065,  0.0270,  0.0156,\n",
       "          -0.0028, -0.0196, -0.0167,  0.0101,  0.0086,  0.0066,  0.0009,\n",
       "          -0.0178,  0.0255, -0.0055, -0.0016,  0.0211,  0.0043,  0.0223,\n",
       "          -0.0302,  0.0179,  0.0046, -0.0222, -0.0083, -0.0287,  0.0247,\n",
       "           0.0169, -0.0076, -0.0151,  0.0178,  0.0296,  0.0222,  0.0006,\n",
       "           0.0045,  0.0315,  0.0172, -0.0198,  0.0170, -0.0027, -0.0242,\n",
       "          -0.0171, -0.0199, -0.0161,  0.0091, -0.0306, -0.0196,  0.0307,\n",
       "           0.0166,  0.0173,  0.0300, -0.0189, -0.0138,  0.0252,  0.0174,\n",
       "          -0.0112,  0.0195,  0.0209, -0.0306, -0.0138,  0.0092,  0.0054,\n",
       "           0.0175,  0.0210, -0.0254,  0.0236, -0.0280, -0.0029,  0.0139,\n",
       "           0.0154, -0.0113,  0.0325, -0.0185,  0.0194,  0.0254,  0.0125,\n",
       "          -0.0074, -0.0205,  0.0154,  0.0193, -0.0165,  0.0312, -0.0230,\n",
       "           0.0243, -0.0289, -0.0276, -0.0220, -0.0197,  0.0109, -0.0045,\n",
       "          -0.0235, -0.0044, -0.0271,  0.0216, -0.0074,  0.0109,  0.0221,\n",
       "          -0.0151, -0.0340,  0.0004, -0.0263,  0.0034, -0.0009, -0.0173,\n",
       "           0.0286, -0.0250,  0.0195,  0.0153,  0.0113, -0.0167,  0.0140,\n",
       "           0.0073,  0.0087, -0.0108,  0.0241, -0.0116, -0.0216, -0.0182,\n",
       "          -0.0078, -0.0239,  0.0279,  0.0118,  0.0182, -0.0261,  0.0015,\n",
       "           0.0023,  0.0074, -0.0179,  0.0222, -0.0089,  0.0284,  0.0174,\n",
       "          -0.0050,  0.0132, -0.0158,  0.0303,  0.0143, -0.0196,  0.0062,\n",
       "          -0.0108, -0.0287,  0.0042,  0.0307, -0.0142, -0.0047,  0.0239,\n",
       "          -0.0339,  0.0055,  0.0296, -0.0325, -0.0190,  0.0313, -0.0283,\n",
       "          -0.0223, -0.0122, -0.0332, -0.0026,  0.0102,  0.0272, -0.0263]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-01T02:10:44.208117Z",
     "iopub.status.busy": "2022-09-01T02:10:44.207168Z",
     "iopub.status.idle": "2022-09-01T05:03:28.296178Z",
     "shell.execute_reply": "2022-09-01T05:03:28.294980Z",
     "shell.execute_reply.started": "2022-09-01T02:10:44.208053Z"
    },
    "id": "il0eSClBzR_e",
    "outputId": "a12b5b42-8ab0-49f7-b217-0b7b552270c2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4609.])\n",
      "6319.070092363283\n",
      "Train Epoch:  0 | Batch:  400 | Loss: 15.80\n",
      "tensor([4609.])\n",
      "69.07558047445491\n",
      "Train Epoch:  0 | Batch:  800 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "4802.375710861757\n",
      "Train Epoch:  0 | Batch: 1200 | Loss: 12.01\n",
      "tensor([4609.])\n",
      "29.6118335374631\n",
      "Train Epoch:  0 | Batch: 1600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3085.247596811503\n",
      "Train Epoch:  0 | Batch: 2000 | Loss: 7.71\n",
      "tensor([4609.])\n",
      "14.792682860046625\n",
      "Train Epoch:  0 | Batch: 2400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1409.4825026122853\n",
      "Train Epoch:  0 | Batch: 2800 | Loss: 3.52\n",
      "tensor([4609.])\n",
      "17.85138992033899\n",
      "Train Epoch:  0 | Batch: 3200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "3101.059446081985\n",
      "Train Epoch:  0 | Batch: 3600 | Loss: 7.75\n",
      "tensor([4609.])\n",
      "208.8867603421677\n",
      "Train Epoch:  0 | Batch: 4000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "2192.695110869361\n",
      "Train Epoch:  0 | Batch: 4400 | Loss: 5.48\n",
      "tensor([4609.])\n",
      "172.86042826855555\n",
      "Train Epoch:  0 | Batch: 4800 | Loss: 0.43\n",
      "tensor([4609.])\n",
      "1934.299561649328\n",
      "Train Epoch:  0 | Batch: 5200 | Loss: 4.84\n",
      "tensor([4609.])\n",
      "66980.26566950837\n",
      "Train Epoch:  0 | Batch: 5600 | Loss: 167.45\n",
      "tensor([4609.])\n",
      "1104.7219857275486\n",
      "Train Epoch:  0 | Batch: 6000 | Loss: 2.76\n",
      "tensor([4609.])\n",
      "1911.590331765823\n",
      "Train Epoch:  0 | Batch: 6400 | Loss: 4.78\n",
      "tensor([4609.])\n",
      "493.7203868853394\n",
      "Train Epoch:  0 | Batch: 6800 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "2032.0450571849942\n",
      "Train Epoch:  0 | Batch: 7200 | Loss: 5.08\n",
      "tensor([4609.])\n",
      "202.5664930017665\n",
      "Train Epoch:  0 | Batch: 7600 | Loss: 0.51\n",
      "tensor([4609.])\n",
      "2724.617253306322\n",
      "Train Epoch:  0 | Batch: 8000 | Loss: 6.81\n",
      "tensor([4609.])\n",
      "49.349117699079216\n",
      "Train Epoch:  0 | Batch: 8400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2023.218431873247\n",
      "Train Epoch:  0 | Batch: 8800 | Loss: 5.06\n",
      "tensor([4609.])\n",
      "54.326990684494376\n",
      "Train Epoch:  0 | Batch: 9200 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "6768.446043714881\n",
      "Train Epoch:  0 | Batch: 9600 | Loss: 16.92\n",
      "tensor([4609.])\n",
      "44.5494305184111\n",
      "Train Epoch:  0 | Batch: 10000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "5663.472509853542\n",
      "Train Epoch:  0 | Batch: 10400 | Loss: 14.16\n",
      "tensor([4609.])\n",
      "62.60693292412907\n",
      "Train Epoch:  0 | Batch: 10800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5722.000811781734\n",
      "Train Epoch:  0 | Batch: 11200 | Loss: 14.31\n",
      "tensor([4609.])\n",
      "89.76859356183559\n",
      "Train Epoch:  0 | Batch: 11600 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "6538.072986651212\n",
      "Train Epoch:  0 | Batch: 12000 | Loss: 16.35\n",
      "tensor([4609.])\n",
      "208.52365153003484\n",
      "Train Epoch:  0 | Batch: 12400 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "4412.895558687858\n",
      "Train Epoch:  0 | Batch: 12800 | Loss: 11.03\n",
      "tensor([4609.])\n",
      "937.4767126441002\n",
      "Train Epoch:  0 | Batch: 13200 | Loss: 2.34\n",
      "tensor([4609.])\n",
      "676.5994652109221\n",
      "Train Epoch:  0 | Batch: 13600 | Loss: 1.69\n",
      "tensor([4609.])\n",
      "753.7123484830372\n",
      "Train Epoch:  0 | Batch: 14000 | Loss: 1.88\n",
      "tensor([4609.])\n",
      "1258.4742729980499\n",
      "Train Epoch:  0 | Batch: 14400 | Loss: 3.15\n",
      "tensor([4609.])\n",
      "4096.642484834883\n",
      "Train Epoch:  0 | Batch: 14800 | Loss: 10.24\n",
      "tensor([4609.])\n",
      "201.40342434938066\n",
      "Train Epoch:  0 | Batch: 15200 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "3629.9412715565413\n",
      "Train Epoch:  0 | Batch: 15600 | Loss: 9.07\n",
      "tensor([4609.])\n",
      "74.33949433011003\n",
      "Train Epoch:  0 | Batch: 16000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3218.185085736215\n",
      "Train Epoch:  0 | Batch: 16400 | Loss: 8.05\n",
      "tensor([4609.])\n",
      "49.74269694508985\n",
      "Train Epoch:  0 | Batch: 16800 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2914.6369082042947\n",
      "Train Epoch:  0 | Batch: 17200 | Loss: 7.29\n",
      "tensor([4609.])\n",
      "35.662218033801764\n",
      "Train Epoch:  0 | Batch: 17600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2857.0117838829756\n",
      "Train Epoch:  0 | Batch: 18000 | Loss: 7.14\n",
      "tensor([4609.])\n",
      "25.689599173376337\n",
      "Train Epoch:  0 | Batch: 18400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2805.1201727585867\n",
      "Train Epoch:  0 | Batch: 18800 | Loss: 7.01\n",
      "tensor([4609.])\n",
      "17.751388985430822\n",
      "Train Epoch:  0 | Batch: 19200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2824.2563541829586\n",
      "Train Epoch:  0 | Batch: 19600 | Loss: 7.06\n",
      "tensor([4609.])\n",
      "27.9977504927665\n",
      "Train Epoch:  0 | Batch: 20000 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "1336.6602579243481\n",
      "Train Epoch:  0 | Batch: 20400 | Loss: 3.34\n",
      "tensor([4609.])\n",
      "51.85116551956162\n",
      "Train Epoch:  0 | Batch: 20800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "1993.8356909635477\n",
      "Train Epoch:  0 | Batch: 21200 | Loss: 4.98\n",
      "tensor([4609.])\n",
      "707.4525151788257\n",
      "Train Epoch:  0 | Batch: 21600 | Loss: 1.77\n",
      "tensor([4609.])\n",
      "1969.0346626602113\n",
      "Train Epoch:  0 | Batch: 22000 | Loss: 4.92\n",
      "tensor([4609.])\n",
      "765.3190527632833\n",
      "Train Epoch:  0 | Batch: 22400 | Loss: 1.91\n",
      "tensor([4609.])\n",
      "930.0260917847045\n",
      "Train Epoch:  0 | Batch: 22800 | Loss: 2.33\n",
      "tensor([4609.])\n",
      "1792.6460667257197\n",
      "Train Epoch:  0 | Batch: 23200 | Loss: 4.48\n",
      "tensor([4609.])\n",
      "468.0446035936475\n",
      "Train Epoch:  0 | Batch: 23600 | Loss: 1.17\n",
      "tensor([4609.])\n",
      "2691.9979196563363\n",
      "Train Epoch:  0 | Batch: 24000 | Loss: 6.73\n",
      "tensor([4609.])\n",
      "230.7079533864744\n",
      "Train Epoch:  0 | Batch: 24400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "3439.191177313216\n",
      "Train Epoch:  0 | Batch: 24800 | Loss: 8.60\n",
      "tensor([4609.])\n",
      "85211571887.81564\n",
      "Train Epoch:  0 | Batch: 25200 | Loss: 213028929.72\n",
      "tensor([4609.])\n",
      "3979715.6606445312\n",
      "Train Epoch:  0 | Batch: 25600 | Loss: 9949.29\n",
      "tensor([4609.])\n",
      "3360965.7412109375\n",
      "Train Epoch:  0 | Batch: 26000 | Loss: 8402.41\n",
      "tensor([4609.])\n",
      "2515741.478515625\n",
      "Train Epoch:  0 | Batch: 26400 | Loss: 6289.35\n",
      "tensor([4609.])\n",
      "2074983.419921875\n",
      "Train Epoch:  0 | Batch: 26800 | Loss: 5187.46\n",
      "tensor([4609.])\n",
      "1298946.5242919922\n",
      "Train Epoch:  0 | Batch: 27200 | Loss: 3247.37\n",
      "tensor([4609.])\n",
      "961237.9240722656\n",
      "Train Epoch:  0 | Batch: 27600 | Loss: 2403.09\n",
      "tensor([4609.])\n",
      "506969.57818603516\n",
      "Train Epoch:  0 | Batch: 28000 | Loss: 1267.42\n",
      "tensor([4609.])\n",
      "321961.78942871094\n",
      "Train Epoch:  0 | Batch: 28400 | Loss: 804.90\n",
      "tensor([4609.])\n",
      "135822.35093688965\n",
      "Train Epoch:  0 | Batch: 28800 | Loss: 339.56\n",
      "tensor([4609.])\n",
      "76441.41145324707\n",
      "Train Epoch:  0 | Batch: 29200 | Loss: 191.10\n",
      "tensor([4609.])\n",
      "27860.822198867798\n",
      "Train Epoch:  0 | Batch: 29600 | Loss: 69.65\n",
      "tensor([4609.])\n",
      "13575.106833457947\n",
      "Train Epoch:  0 | Batch: 30000 | Loss: 33.94\n",
      "tensor([4609.])\n",
      "5700.135144710541\n",
      "Train Epoch:  0 | Batch: 30400 | Loss: 14.25\n",
      "tensor([4609.])\n",
      "7097.525761842728\n",
      "Train Epoch:  0 | Batch: 30800 | Loss: 17.74\n",
      "tensor([4609.])\n",
      "1669.2540857717395\n",
      "Train Epoch:  0 | Batch: 31200 | Loss: 4.17\n",
      "tensor([4609.])\n",
      "4332.006143093109\n",
      "Train Epoch:  0 | Batch: 31600 | Loss: 10.83\n",
      "tensor([4609.])\n",
      "373.07638733740896\n",
      "Train Epoch:  0 | Batch: 32000 | Loss: 0.93\n",
      "tensor([4609.])\n",
      "2848.9955323264003\n",
      "Train Epoch:  0 | Batch: 32400 | Loss: 7.12\n",
      "tensor([4609.])\n",
      "195.14868122991174\n",
      "Train Epoch:  0 | Batch: 32800 | Loss: 0.49\n",
      "tensor([4609.])\n",
      "6424.705008611083\n",
      "Train Epoch:  0 | Batch: 33200 | Loss: 16.06\n",
      "tensor([4609.])\n",
      "557.989122597035\n",
      "Train Epoch:  0 | Batch: 33600 | Loss: 1.39\n",
      "tensor([4609.])\n",
      "4186.346756736748\n",
      "Train Epoch:  0 | Batch: 34000 | Loss: 10.47\n",
      "tensor([4609.])\n",
      "66.9929404000286\n",
      "Train Epoch:  0 | Batch: 34400 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "5207.862778428942\n",
      "Train Epoch:  0 | Batch: 34800 | Loss: 13.02\n",
      "tensor([4609.])\n",
      "26.58934803120792\n",
      "Train Epoch:  0 | Batch: 35200 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3087.3318125326186\n",
      "Train Epoch:  0 | Batch: 35600 | Loss: 7.72\n",
      "tensor([4609.])\n",
      "17.608172203414142\n",
      "Train Epoch:  0 | Batch: 36000 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2284.617426345125\n",
      "Train Epoch:  0 | Batch: 36400 | Loss: 5.71\n",
      "tensor([4609.])\n",
      "21.327075547538698\n",
      "Train Epoch:  0 | Batch: 36800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3162.4200874632224\n",
      "Train Epoch:  0 | Batch: 37200 | Loss: 7.91\n",
      "tensor([4609.])\n",
      "103.09397613280453\n",
      "Train Epoch:  0 | Batch: 37600 | Loss: 0.26\n",
      "tensor([4609.])\n",
      "1313.9151459911373\n",
      "Train Epoch:  0 | Batch: 38000 | Loss: 3.28\n",
      "tensor([4609.])\n",
      "267.3187421830371\n",
      "Train Epoch:  0 | Batch: 38400 | Loss: 0.67\n",
      "tensor([4609.])\n",
      "1594.1197664798237\n",
      "Train Epoch:  0 | Batch: 38800 | Loss: 3.99\n",
      "tensor([4609.])\n",
      "1438.0162239817437\n",
      "Train Epoch:  0 | Batch: 39200 | Loss: 3.60\n",
      "tensor([4609.])\n",
      "855.6869180887006\n",
      "Train Epoch:  0 | Batch: 39600 | Loss: 2.14\n",
      "tensor([4609.])\n",
      "1479.940924098948\n",
      "Train Epoch:  0 | Batch: 40000 | Loss: 3.70\n",
      "tensor([4609.])\n",
      "410.78831335529685\n",
      "Train Epoch:  0 | Batch: 40400 | Loss: 1.03\n",
      "tensor([4609.])\n",
      "2214.5633392776363\n",
      "Train Epoch:  0 | Batch: 40800 | Loss: 5.54\n",
      "tensor([4609.])\n",
      "240.33158674789593\n",
      "Train Epoch:  0 | Batch: 41200 | Loss: 0.60\n",
      "tensor([4609.])\n",
      "2849.7402630252764\n",
      "Train Epoch:  0 | Batch: 41600 | Loss: 7.12\n",
      "tensor([4609.])\n",
      "80.5897329391446\n",
      "Train Epoch:  0 | Batch: 42000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "2655.7883279258385\n",
      "Train Epoch:  0 | Batch: 42400 | Loss: 6.64\n",
      "tensor([4609.])\n",
      "29.667982374085113\n",
      "Train Epoch:  0 | Batch: 42800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2203.322022021748\n",
      "Train Epoch:  0 | Batch: 43200 | Loss: 5.51\n",
      "tensor([4609.])\n",
      "14.446737001882866\n",
      "Train Epoch:  0 | Batch: 43600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1547.331726308912\n",
      "Train Epoch:  0 | Batch: 44000 | Loss: 3.87\n",
      "tensor([4609.])\n",
      "4503.939432729967\n",
      "Train Epoch:  0 | Batch: 44400 | Loss: 11.26\n",
      "tensor([4609.])\n",
      "90.55082300188951\n",
      "Train Epoch:  0 | Batch: 44800 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "3154.4369314573705\n",
      "Train Epoch:  0 | Batch: 45200 | Loss: 7.89\n",
      "tensor([4609.])\n",
      "122.64373178966343\n",
      "Train Epoch:  0 | Batch: 45600 | Loss: 0.31\n",
      "tensor([4609.])\n",
      "2733.089235630352\n",
      "Train Epoch:  0 | Batch: 46000 | Loss: 6.83\n",
      "tensor([4609.])\n",
      "670.3124650190584\n",
      "Train Epoch:  0 | Batch: 46400 | Loss: 1.68\n",
      "tensor([4609.])\n",
      "2220.3908499479294\n",
      "Train Epoch:  0 | Batch: 46800 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "1524.4136887760833\n",
      "Train Epoch:  0 | Batch: 47200 | Loss: 3.81\n",
      "tensor([4609.])\n",
      "1086.2843474419788\n",
      "Train Epoch:  0 | Batch: 47600 | Loss: 2.72\n",
      "tensor([4609.])\n",
      "2630.051406520419\n",
      "Train Epoch:  0 | Batch: 48000 | Loss: 6.58\n",
      "tensor([4609.])\n",
      "430.56929358886555\n",
      "Train Epoch:  0 | Batch: 48400 | Loss: 1.08\n",
      "tensor([4609.])\n",
      "2982.6270459811203\n",
      "Train Epoch:  0 | Batch: 48800 | Loss: 7.46\n",
      "tensor([4609.])\n",
      "83.07950366404839\n",
      "Train Epoch:  0 | Batch: 49200 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "1642.819615391083\n",
      "Train Epoch:  0 | Batch: 49600 | Loss: 4.11\n",
      "tensor([4609.])\n",
      "73.01705178874545\n",
      "Train Epoch:  0 | Batch: 50000 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "4301.260656835511\n",
      "Train Epoch:  0 | Batch: 50400 | Loss: 10.75\n",
      "tensor([4609.])\n",
      "28.83793453266844\n",
      "Train Epoch:  0 | Batch: 50800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2596.2906648917124\n",
      "Train Epoch:  0 | Batch: 51200 | Loss: 6.49\n",
      "tensor([4609.])\n",
      "28.59268126823008\n",
      "Train Epoch:  0 | Batch: 51600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "4002.926296260208\n",
      "Train Epoch:  0 | Batch: 52000 | Loss: 10.01\n",
      "tensor([4609.])\n",
      "36.650492707267404\n",
      "Train Epoch:  0 | Batch: 52400 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3547.725673062727\n",
      "Train Epoch:  0 | Batch: 52800 | Loss: 8.87\n",
      "tensor([4609.])\n",
      "49.04490670654923\n",
      "Train Epoch:  0 | Batch: 53200 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "3478.026955314912\n",
      "Train Epoch:  0 | Batch: 53600 | Loss: 8.70\n",
      "tensor([4609.])\n",
      "143.64043136360124\n",
      "Train Epoch:  0 | Batch: 54000 | Loss: 0.36\n",
      "tensor([4609.])\n",
      "2293.221667535603\n",
      "Train Epoch:  0 | Batch: 54400 | Loss: 5.73\n",
      "tensor([4609.])\n",
      "496.8480041490402\n",
      "Train Epoch:  0 | Batch: 54800 | Loss: 1.24\n",
      "tensor([4609.])\n",
      "969.6360001454595\n",
      "Train Epoch:  0 | Batch: 55200 | Loss: 2.42\n",
      "tensor([4609.])\n",
      "649.0321905058809\n",
      "Train Epoch:  0 | Batch: 55600 | Loss: 1.62\n",
      "tensor([4609.])\n",
      "868.2429014092777\n",
      "Train Epoch:  0 | Batch: 56000 | Loss: 2.17\n",
      "tensor([4609.])\n",
      "3175.924821058754\n",
      "Train Epoch:  0 | Batch: 56400 | Loss: 7.94\n",
      "tensor([4609.])\n",
      "322.1126726255752\n",
      "Train Epoch:  0 | Batch: 56800 | Loss: 0.81\n",
      "tensor([4609.])\n",
      "3497.0158245796338\n",
      "Train Epoch:  0 | Batch: 57200 | Loss: 8.74\n",
      "tensor([4609.])\n",
      "202.9992708163336\n",
      "Train Epoch:  0 | Batch: 57600 | Loss: 0.51\n",
      "tensor([4609.])\n",
      "3723.8082447750494\n",
      "Train Epoch:  0 | Batch: 58000 | Loss: 9.31\n",
      "tensor([4609.])\n",
      "77.70744903990999\n",
      "Train Epoch:  0 | Batch: 58400 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3982.4470571419224\n",
      "Train Epoch:  0 | Batch: 58800 | Loss: 9.96\n",
      "tensor([4609.])\n",
      "34.35896003106609\n",
      "Train Epoch:  0 | Batch: 59200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3618.7346667628735\n",
      "Train Epoch:  0 | Batch: 59600 | Loss: 9.05\n",
      "tensor([4609.])\n",
      "39.794507710728794\n",
      "Train Epoch:  0 | Batch: 60000 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "4679.546654418111\n",
      "Train Epoch:  0 | Batch: 60400 | Loss: 11.70\n",
      "tensor([4609.])\n",
      "19.000567444832996\n",
      "Train Epoch:  0 | Batch: 60800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "1421.7012322442606\n",
      "Train Epoch:  0 | Batch: 61200 | Loss: 3.55\n",
      "tensor([4609.])\n",
      "25.2319352326449\n",
      "Train Epoch:  0 | Batch: 61600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2757.455210491549\n",
      "Train Epoch:  0 | Batch: 62000 | Loss: 6.89\n",
      "tensor([4609.])\n",
      "237.68109058309346\n",
      "Train Epoch:  0 | Batch: 62400 | Loss: 0.59\n",
      "tensor([4609.])\n",
      "2206.677548912354\n",
      "Train Epoch:  0 | Batch: 62800 | Loss: 5.52\n",
      "tensor([4609.])\n",
      "293.7570559838787\n",
      "Train Epoch:  0 | Batch: 63200 | Loss: 0.73\n",
      "tensor([4609.])\n",
      "1431.186947429087\n",
      "Train Epoch:  0 | Batch: 63600 | Loss: 3.58\n",
      "tensor([4609.])\n",
      "1098.616108759772\n",
      "Train Epoch:  0 | Batch: 64000 | Loss: 2.75\n",
      "tensor([4609.])\n",
      "730.4367709443904\n",
      "Train Epoch:  0 | Batch: 64400 | Loss: 1.83\n",
      "tensor([4609.])\n",
      "2145.822148628067\n",
      "Train Epoch:  0 | Batch: 64800 | Loss: 5.36\n",
      "tensor([4609.])\n",
      "332.7325491057709\n",
      "Train Epoch:  0 | Batch: 65200 | Loss: 0.83\n",
      "tensor([4609.])\n",
      "2176.3095271526836\n",
      "Train Epoch:  0 | Batch: 65600 | Loss: 5.44\n",
      "tensor([4609.])\n",
      "209.08294214867055\n",
      "Train Epoch:  0 | Batch: 66000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "4200.64152480755\n",
      "Train Epoch:  0 | Batch: 66400 | Loss: 10.50\n",
      "tensor([4609.])\n",
      "41.11578257381916\n",
      "Train Epoch:  0 | Batch: 66800 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2113.4538792446256\n",
      "Train Epoch:  0 | Batch: 67200 | Loss: 5.28\n",
      "tensor([4609.])\n",
      "19.92847071448341\n",
      "Train Epoch:  0 | Batch: 67600 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3652.966282427311\n",
      "Train Epoch:  0 | Batch: 68000 | Loss: 9.13\n",
      "tensor([4609.])\n",
      "24.171978812664747\n",
      "Train Epoch:  0 | Batch: 68400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "9194.594439401291\n",
      "Train Epoch:  0 | Batch: 68800 | Loss: 22.99\n",
      "tensor([4609.])\n",
      "66.24509102199227\n",
      "Train Epoch:  0 | Batch: 69200 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "2461.490284200292\n",
      "Train Epoch:  0 | Batch: 69600 | Loss: 6.15\n",
      "tensor([4609.])\n",
      "397.4951301207766\n",
      "Train Epoch:  0 | Batch: 70000 | Loss: 0.99\n",
      "tensor([4609.])\n",
      "2059.1058659418486\n",
      "Train Epoch:  0 | Batch: 70400 | Loss: 5.15\n",
      "tensor([4609.])\n",
      "883.0087702979799\n",
      "Train Epoch:  0 | Batch: 70800 | Loss: 2.21\n",
      "tensor([4609.])\n",
      "1007.544931733748\n",
      "Train Epoch:  0 | Batch: 71200 | Loss: 2.52\n",
      "tensor([4609.])\n",
      "1787.0821882276796\n",
      "Train Epoch:  0 | Batch: 71600 | Loss: 4.47\n",
      "tensor([4609.])\n",
      "219.33906667656265\n",
      "Train Epoch:  0 | Batch: 72000 | Loss: 0.55\n",
      "tensor([4609.])\n",
      "1097.8145313388668\n",
      "Train Epoch:  0 | Batch: 72400 | Loss: 2.74\n",
      "tensor([4609.])\n",
      "117.00033627427183\n",
      "Train Epoch:  0 | Batch: 72800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3154.768241720274\n",
      "Train Epoch:  0 | Batch: 73200 | Loss: 7.89\n",
      "tensor([4609.])\n",
      "32.063197870738804\n",
      "Train Epoch:  0 | Batch: 73600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2471.3730214745738\n",
      "Train Epoch:  0 | Batch: 74000 | Loss: 6.18\n",
      "tensor([4609.])\n",
      "40.600186304189265\n",
      "Train Epoch:  0 | Batch: 74400 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2368.204178960994\n",
      "Train Epoch:  0 | Batch: 74800 | Loss: 5.92\n",
      "tensor([4609.])\n",
      "24.990495095960796\n",
      "Train Epoch:  0 | Batch: 75200 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2594.122154150158\n",
      "Train Epoch:  0 | Batch: 75600 | Loss: 6.49\n",
      "tensor([4609.])\n",
      "20.142840853892267\n",
      "Train Epoch:  0 | Batch: 76000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2564.8001673612744\n",
      "Train Epoch:  0 | Batch: 76400 | Loss: 6.41\n",
      "tensor([4609.])\n",
      "27.3425519336015\n",
      "Train Epoch:  0 | Batch: 76800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2460.555886955932\n",
      "Train Epoch:  0 | Batch: 77200 | Loss: 6.15\n",
      "tensor([4609.])\n",
      "65.25943337241188\n",
      "Train Epoch:  0 | Batch: 77600 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "1049.3290335047059\n",
      "Train Epoch:  0 | Batch: 78000 | Loss: 2.62\n",
      "tensor([4609.])\n",
      "178.90640490548685\n",
      "Train Epoch:  0 | Batch: 78400 | Loss: 0.45\n",
      "tensor([4609.])\n",
      "2424.8257353282534\n",
      "Train Epoch:  0 | Batch: 78800 | Loss: 6.06\n",
      "tensor([4609.])\n",
      "2150.1563104302622\n",
      "Train Epoch:  0 | Batch: 79200 | Loss: 5.38\n",
      "tensor([4609.])\n",
      "1163.4567368263379\n",
      "Train Epoch:  0 | Batch: 79600 | Loss: 2.91\n",
      "tensor([4609.])\n",
      "2134.5969045758247\n",
      "Train Epoch:  0 | Batch: 80000 | Loss: 5.34\n",
      "tensor([4609.])\n",
      "443.5554876481183\n",
      "Train Epoch:  0 | Batch: 80400 | Loss: 1.11\n",
      "tensor([4609.])\n",
      "1917.030934694223\n",
      "Train Epoch:  0 | Batch: 80800 | Loss: 4.79\n",
      "tensor([4609.])\n",
      "86.72641625441611\n",
      "Train Epoch:  0 | Batch: 81200 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "1255.3341942704283\n",
      "Train Epoch:  0 | Batch: 81600 | Loss: 3.14\n",
      "tensor([4609.])\n",
      "42.04147817892954\n",
      "Train Epoch:  0 | Batch: 82000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "1962.171950283926\n",
      "Train Epoch:  0 | Batch: 82400 | Loss: 4.91\n",
      "tensor([4609.])\n",
      "29.73126791836694\n",
      "Train Epoch:  0 | Batch: 82800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2555.0715989973396\n",
      "Train Epoch:  0 | Batch: 83200 | Loss: 6.39\n",
      "tensor([4609.])\n",
      "14.711917366134003\n",
      "Train Epoch:  0 | Batch: 83600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1709.7363313725218\n",
      "Train Epoch:  0 | Batch: 84000 | Loss: 4.27\n",
      "tensor([4609.])\n",
      "12.97655533393845\n",
      "Train Epoch:  0 | Batch: 84400 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2774.8097516791895\n",
      "Train Epoch:  0 | Batch: 84800 | Loss: 6.94\n",
      "tensor([4609.])\n",
      "57.76103242766112\n",
      "Train Epoch:  0 | Batch: 85200 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "3622.3318057786673\n",
      "Train Epoch:  0 | Batch: 85600 | Loss: 9.06\n",
      "tensor([4609.])\n",
      "37.02483787154779\n",
      "Train Epoch:  0 | Batch: 86000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2931.7774426126853\n",
      "Train Epoch:  0 | Batch: 86400 | Loss: 7.33\n",
      "tensor([4609.])\n",
      "452.51650451961905\n",
      "Train Epoch:  0 | Batch: 86800 | Loss: 1.13\n",
      "tensor([4609.])\n",
      "2260.3869345798157\n",
      "Train Epoch:  0 | Batch: 87200 | Loss: 5.65\n",
      "tensor([4609.])\n",
      "1620.976996766869\n",
      "Train Epoch:  0 | Batch: 87600 | Loss: 4.05\n",
      "tensor([4609.])\n",
      "1423.6877667168155\n",
      "Train Epoch:  0 | Batch: 88000 | Loss: 3.56\n",
      "tensor([4609.])\n",
      "2110.9997287020087\n",
      "Train Epoch:  0 | Batch: 88400 | Loss: 5.28\n",
      "tensor([4609.])\n",
      "793.8144337399863\n",
      "Train Epoch:  0 | Batch: 88800 | Loss: 1.98\n",
      "tensor([4609.])\n",
      "3507.3010361352935\n",
      "Train Epoch:  0 | Batch: 89200 | Loss: 8.77\n",
      "tensor([4609.])\n",
      "150.67020978708751\n",
      "Train Epoch:  0 | Batch: 89600 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "2586.751348051708\n",
      "Train Epoch:  0 | Batch: 90000 | Loss: 6.47\n",
      "tensor([4609.])\n",
      "100.57248179335147\n",
      "Train Epoch:  0 | Batch: 90400 | Loss: 0.25\n",
      "tensor([4609.])\n",
      "5613.33323562704\n",
      "Train Epoch:  0 | Batch: 90800 | Loss: 14.03\n",
      "tensor([4609.])\n",
      "36.778700690250844\n",
      "Train Epoch:  0 | Batch: 91200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "4985.996699722484\n",
      "Train Epoch:  0 | Batch: 91600 | Loss: 12.46\n",
      "tensor([4609.])\n",
      "81.14623685367405\n",
      "Train Epoch:  0 | Batch: 92000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "6561.7972647845745\n",
      "Train Epoch:  0 | Batch: 92400 | Loss: 16.40\n",
      "tensor([4609.])\n",
      "62.4708290444687\n",
      "Train Epoch:  0 | Batch: 92800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5491.104711320251\n",
      "Train Epoch:  0 | Batch: 93200 | Loss: 13.73\n",
      "tensor([4609.])\n",
      "69.95233438257128\n",
      "Train Epoch:  0 | Batch: 93600 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "1784.6324320901185\n",
      "Train Epoch:  0 | Batch: 94000 | Loss: 4.46\n",
      "tensor([4609.])\n",
      "56.37825040239841\n",
      "Train Epoch:  0 | Batch: 94400 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "3564.3439051299356\n",
      "Train Epoch:  0 | Batch: 94800 | Loss: 8.91\n",
      "tensor([4609.])\n",
      "387.19516342366114\n",
      "Train Epoch:  0 | Batch: 95200 | Loss: 0.97\n",
      "tensor([4609.])\n",
      "1225.261094051879\n",
      "Train Epoch:  0 | Batch: 95600 | Loss: 3.06\n",
      "tensor([4609.])\n",
      "992.1553731563035\n",
      "Train Epoch:  0 | Batch: 96000 | Loss: 2.48\n",
      "tensor([4609.])\n",
      "1165.1782504743896\n",
      "Train Epoch:  0 | Batch: 96400 | Loss: 2.91\n",
      "tensor([4609.])\n",
      "3237.325992116239\n",
      "Train Epoch:  0 | Batch: 96800 | Loss: 8.09\n",
      "tensor([4609.])\n",
      "488.4668923681602\n",
      "Train Epoch:  0 | Batch: 97200 | Loss: 1.22\n",
      "tensor([4609.])\n",
      "1863.6961587816477\n",
      "Train Epoch:  0 | Batch: 97600 | Loss: 4.66\n",
      "tensor([4609.])\n",
      "78.27800265955739\n",
      "Train Epoch:  0 | Batch: 98000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "1624.3003706801683\n",
      "Train Epoch:  0 | Batch: 98400 | Loss: 4.06\n",
      "tensor([4609.])\n",
      "30.339458875823766\n",
      "Train Epoch:  0 | Batch: 98800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "3061.5164177557454\n",
      "Train Epoch:  0 | Batch: 99200 | Loss: 7.65\n",
      "tensor([4609.])\n",
      "33.491003138478845\n",
      "Train Epoch:  0 | Batch: 99600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2197.6435573538765\n",
      "Train Epoch:  0 | Batch: 100000 | Loss: 5.49\n",
      "tensor([4609.])\n",
      "16.165670276153833\n",
      "Train Epoch:  0 | Batch: 100400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2129.4940920565277\n",
      "Train Epoch:  0 | Batch: 100800 | Loss: 5.32\n",
      "tensor([4609.])\n",
      "13.11476191971451\n",
      "Train Epoch:  0 | Batch: 101200 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2061.7365161995403\n",
      "Train Epoch:  0 | Batch: 101600 | Loss: 5.15\n",
      "tensor([4609.])\n",
      "19.172060159035027\n",
      "Train Epoch:  0 | Batch: 102000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2809.691765132826\n",
      "Train Epoch:  0 | Batch: 102400 | Loss: 7.02\n",
      "tensor([4609.])\n",
      "117.21315664122812\n",
      "Train Epoch:  0 | Batch: 102800 | Loss: 0.29\n",
      "35\n",
      "\n",
      "Valid Epoch:  0 | Loss: 1.05\n",
      "model saved\n",
      "\n",
      "tensor([4609.])\n",
      "6185.806253945921\n",
      "Train Epoch:  1 | Batch:  400 | Loss: 15.46\n",
      "tensor([4609.])\n",
      "67.54818690335378\n",
      "Train Epoch:  1 | Batch:  800 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "4445.85408431571\n",
      "Train Epoch:  1 | Batch: 1200 | Loss: 11.11\n",
      "tensor([4609.])\n",
      "29.199192714411765\n",
      "Train Epoch:  1 | Batch: 1600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3081.0787956379354\n",
      "Train Epoch:  1 | Batch: 2000 | Loss: 7.70\n",
      "tensor([4609.])\n",
      "15.137967063114047\n",
      "Train Epoch:  1 | Batch: 2400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1432.2989061549306\n",
      "Train Epoch:  1 | Batch: 2800 | Loss: 3.58\n",
      "tensor([4609.])\n",
      "17.131618363549933\n",
      "Train Epoch:  1 | Batch: 3200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "3022.2869298956357\n",
      "Train Epoch:  1 | Batch: 3600 | Loss: 7.56\n",
      "tensor([4609.])\n",
      "214.54906476172619\n",
      "Train Epoch:  1 | Batch: 4000 | Loss: 0.54\n",
      "tensor([4609.])\n",
      "2184.149653333472\n",
      "Train Epoch:  1 | Batch: 4400 | Loss: 5.46\n",
      "tensor([4609.])\n",
      "192.6053483581636\n",
      "Train Epoch:  1 | Batch: 4800 | Loss: 0.48\n",
      "tensor([4609.])\n",
      "1903.6970655606128\n",
      "Train Epoch:  1 | Batch: 5200 | Loss: 4.76\n",
      "tensor([4609.])\n",
      "67258.1294473582\n",
      "Train Epoch:  1 | Batch: 5600 | Loss: 168.15\n",
      "tensor([4609.])\n",
      "1135.1119887088425\n",
      "Train Epoch:  1 | Batch: 6000 | Loss: 2.84\n",
      "tensor([4609.])\n",
      "1913.1909203047398\n",
      "Train Epoch:  1 | Batch: 6400 | Loss: 4.78\n",
      "tensor([4609.])\n",
      "493.58700764365494\n",
      "Train Epoch:  1 | Batch: 6800 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "2000.9681399688125\n",
      "Train Epoch:  1 | Batch: 7200 | Loss: 5.00\n",
      "tensor([4609.])\n",
      "202.94338923227042\n",
      "Train Epoch:  1 | Batch: 7600 | Loss: 0.51\n",
      "tensor([4609.])\n",
      "2741.161697437521\n",
      "Train Epoch:  1 | Batch: 8000 | Loss: 6.85\n",
      "tensor([4609.])\n",
      "47.68550459947437\n",
      "Train Epoch:  1 | Batch: 8400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2082.1300322236493\n",
      "Train Epoch:  1 | Batch: 8800 | Loss: 5.21\n",
      "tensor([4609.])\n",
      "55.44123712647706\n",
      "Train Epoch:  1 | Batch: 9200 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "6843.681862741709\n",
      "Train Epoch:  1 | Batch: 9600 | Loss: 17.11\n",
      "tensor([4609.])\n",
      "44.40513876732439\n",
      "Train Epoch:  1 | Batch: 10000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "5518.098423168063\n",
      "Train Epoch:  1 | Batch: 10400 | Loss: 13.80\n",
      "tensor([4609.])\n",
      "61.60588694922626\n",
      "Train Epoch:  1 | Batch: 10800 | Loss: 0.15\n",
      "tensor([4609.])\n",
      "5707.298365864903\n",
      "Train Epoch:  1 | Batch: 11200 | Loss: 14.27\n",
      "tensor([4609.])\n",
      "91.84533461555839\n",
      "Train Epoch:  1 | Batch: 11600 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "6782.2681774795055\n",
      "Train Epoch:  1 | Batch: 12000 | Loss: 16.96\n",
      "tensor([4609.])\n",
      "245.37011896073818\n",
      "Train Epoch:  1 | Batch: 12400 | Loss: 0.61\n",
      "tensor([4609.])\n",
      "4350.317303780466\n",
      "Train Epoch:  1 | Batch: 12800 | Loss: 10.88\n",
      "tensor([4609.])\n",
      "850.0462075956166\n",
      "Train Epoch:  1 | Batch: 13200 | Loss: 2.13\n",
      "tensor([4609.])\n",
      "674.3262007450685\n",
      "Train Epoch:  1 | Batch: 13600 | Loss: 1.69\n",
      "tensor([4609.])\n",
      "777.4740200014785\n",
      "Train Epoch:  1 | Batch: 14000 | Loss: 1.94\n",
      "tensor([4609.])\n",
      "1228.4452831074595\n",
      "Train Epoch:  1 | Batch: 14400 | Loss: 3.07\n",
      "tensor([4609.])\n",
      "3915.586131431395\n",
      "Train Epoch:  1 | Batch: 14800 | Loss: 9.79\n",
      "tensor([4609.])\n",
      "199.39476799452677\n",
      "Train Epoch:  1 | Batch: 15200 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "3702.1662188176997\n",
      "Train Epoch:  1 | Batch: 15600 | Loss: 9.26\n",
      "tensor([4609.])\n",
      "74.98691182723269\n",
      "Train Epoch:  1 | Batch: 16000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3280.530201934278\n",
      "Train Epoch:  1 | Batch: 16400 | Loss: 8.20\n",
      "tensor([4609.])\n",
      "51.258816903457046\n",
      "Train Epoch:  1 | Batch: 16800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "2723.528300411068\n",
      "Train Epoch:  1 | Batch: 17200 | Loss: 6.81\n",
      "tensor([4609.])\n",
      "35.579720731126145\n",
      "Train Epoch:  1 | Batch: 17600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2842.3589485031553\n",
      "Train Epoch:  1 | Batch: 18000 | Loss: 7.11\n",
      "tensor([4609.])\n",
      "26.170539632905275\n",
      "Train Epoch:  1 | Batch: 18400 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2775.4201873121783\n",
      "Train Epoch:  1 | Batch: 18800 | Loss: 6.94\n",
      "tensor([4609.])\n",
      "18.82583267474547\n",
      "Train Epoch:  1 | Batch: 19200 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2820.586426222697\n",
      "Train Epoch:  1 | Batch: 19600 | Loss: 7.05\n",
      "tensor([4609.])\n",
      "28.312024291604757\n",
      "Train Epoch:  1 | Batch: 20000 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "1341.9957809564658\n",
      "Train Epoch:  1 | Batch: 20400 | Loss: 3.35\n",
      "tensor([4609.])\n",
      "50.54660759144463\n",
      "Train Epoch:  1 | Batch: 20800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "1986.9164064424112\n",
      "Train Epoch:  1 | Batch: 21200 | Loss: 4.97\n",
      "tensor([4609.])\n",
      "696.3532245825045\n",
      "Train Epoch:  1 | Batch: 21600 | Loss: 1.74\n",
      "tensor([4609.])\n",
      "1955.743789251428\n",
      "Train Epoch:  1 | Batch: 22000 | Loss: 4.89\n",
      "tensor([4609.])\n",
      "797.7698667189106\n",
      "Train Epoch:  1 | Batch: 22400 | Loss: 1.99\n",
      "tensor([4609.])\n",
      "890.985268112272\n",
      "Train Epoch:  1 | Batch: 22800 | Loss: 2.23\n",
      "tensor([4609.])\n",
      "1772.4057339043356\n",
      "Train Epoch:  1 | Batch: 23200 | Loss: 4.43\n",
      "tensor([4609.])\n",
      "465.52166315354407\n",
      "Train Epoch:  1 | Batch: 23600 | Loss: 1.16\n",
      "tensor([4609.])\n",
      "2722.0537680611014\n",
      "Train Epoch:  1 | Batch: 24000 | Loss: 6.81\n",
      "tensor([4609.])\n",
      "232.12998410733417\n",
      "Train Epoch:  1 | Batch: 24400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "3519.0065161380917\n",
      "Train Epoch:  1 | Batch: 24800 | Loss: 8.80\n",
      "tensor([4609.])\n",
      "85220137410.42409\n",
      "Train Epoch:  1 | Batch: 25200 | Loss: 213050343.53\n",
      "tensor([4609.])\n",
      "1236673.0712217744\n",
      "Train Epoch:  1 | Batch: 25600 | Loss: 3091.68\n",
      "tensor([4609.])\n",
      "17.27391846734099\n",
      "Train Epoch:  1 | Batch: 26000 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1535.0949775548652\n",
      "Train Epoch:  1 | Batch: 26400 | Loss: 3.84\n",
      "tensor([4609.])\n",
      "15.834289224818349\n",
      "Train Epoch:  1 | Batch: 26800 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "9869.414429754019\n",
      "Train Epoch:  1 | Batch: 27200 | Loss: 24.67\n",
      "tensor([4609.])\n",
      "49.335117034148425\n",
      "Train Epoch:  1 | Batch: 27600 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "3241.0419480381534\n",
      "Train Epoch:  1 | Batch: 28000 | Loss: 8.10\n",
      "tensor([4609.])\n",
      "18.089844717644155\n",
      "Train Epoch:  1 | Batch: 28400 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3141.349414505996\n",
      "Train Epoch:  1 | Batch: 28800 | Loss: 7.85\n",
      "tensor([4609.])\n",
      "71.59958550706506\n",
      "Train Epoch:  1 | Batch: 29200 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "4010.835163285956\n",
      "Train Epoch:  1 | Batch: 29600 | Loss: 10.03\n",
      "tensor([4609.])\n",
      "687.9350045695901\n",
      "Train Epoch:  1 | Batch: 30000 | Loss: 1.72\n",
      "tensor([4609.])\n",
      "2406.1687443880364\n",
      "Train Epoch:  1 | Batch: 30400 | Loss: 6.02\n",
      "tensor([4609.])\n",
      "1402.8537512284238\n",
      "Train Epoch:  1 | Batch: 30800 | Loss: 3.51\n",
      "tensor([4609.])\n",
      "903.4276508153416\n",
      "Train Epoch:  1 | Batch: 31200 | Loss: 2.26\n",
      "tensor([4609.])\n",
      "1864.6717524584383\n",
      "Train Epoch:  1 | Batch: 31600 | Loss: 4.66\n",
      "tensor([4609.])\n",
      "206.74611724284478\n",
      "Train Epoch:  1 | Batch: 32000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "1181.5416441425914\n",
      "Train Epoch:  1 | Batch: 32400 | Loss: 2.95\n",
      "tensor([4609.])\n",
      "118.4698425460374\n",
      "Train Epoch:  1 | Batch: 32800 | Loss: 0.30\n",
      "tensor([4609.])\n",
      "3549.6881496515125\n",
      "Train Epoch:  1 | Batch: 33200 | Loss: 8.87\n",
      "tensor([4609.])\n",
      "39.5457719003316\n",
      "Train Epoch:  1 | Batch: 33600 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "3466.5393969477154\n",
      "Train Epoch:  1 | Batch: 34000 | Loss: 8.67\n",
      "tensor([4609.])\n",
      "67.88600912457332\n",
      "Train Epoch:  1 | Batch: 34400 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "4991.799270775169\n",
      "Train Epoch:  1 | Batch: 34800 | Loss: 12.48\n",
      "tensor([4609.])\n",
      "28.32581312698312\n",
      "Train Epoch:  1 | Batch: 35200 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "5943.788969280198\n",
      "Train Epoch:  1 | Batch: 35600 | Loss: 14.86\n",
      "tensor([4609.])\n",
      "15.993008346995339\n",
      "Train Epoch:  1 | Batch: 36000 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2220.3070065118372\n",
      "Train Epoch:  1 | Batch: 36400 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "22.331588610773906\n",
      "Train Epoch:  1 | Batch: 36800 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "3173.0057233064435\n",
      "Train Epoch:  1 | Batch: 37200 | Loss: 7.93\n",
      "tensor([4609.])\n",
      "106.65131170465611\n",
      "Train Epoch:  1 | Batch: 37600 | Loss: 0.27\n",
      "tensor([4609.])\n",
      "1342.868495886214\n",
      "Train Epoch:  1 | Batch: 38000 | Loss: 3.36\n",
      "tensor([4609.])\n",
      "254.48792227660306\n",
      "Train Epoch:  1 | Batch: 38400 | Loss: 0.64\n",
      "tensor([4609.])\n",
      "1667.92221098952\n",
      "Train Epoch:  1 | Batch: 38800 | Loss: 4.17\n",
      "tensor([4609.])\n",
      "1500.686711593531\n",
      "Train Epoch:  1 | Batch: 39200 | Loss: 3.75\n",
      "tensor([4609.])\n",
      "878.527106957743\n",
      "Train Epoch:  1 | Batch: 39600 | Loss: 2.20\n",
      "tensor([4609.])\n",
      "1450.2961078640074\n",
      "Train Epoch:  1 | Batch: 40000 | Loss: 3.63\n",
      "tensor([4609.])\n",
      "419.648145487532\n",
      "Train Epoch:  1 | Batch: 40400 | Loss: 1.05\n",
      "tensor([4609.])\n",
      "2255.951580708381\n",
      "Train Epoch:  1 | Batch: 40800 | Loss: 5.64\n",
      "tensor([4609.])\n",
      "244.2612544046715\n",
      "Train Epoch:  1 | Batch: 41200 | Loss: 0.61\n",
      "tensor([4609.])\n",
      "2936.2448048163205\n",
      "Train Epoch:  1 | Batch: 41600 | Loss: 7.34\n",
      "tensor([4609.])\n",
      "84.87208342552185\n",
      "Train Epoch:  1 | Batch: 42000 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "2884.0894231684506\n",
      "Train Epoch:  1 | Batch: 42400 | Loss: 7.21\n",
      "tensor([4609.])\n",
      "29.200579554308206\n",
      "Train Epoch:  1 | Batch: 42800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2218.102779276669\n",
      "Train Epoch:  1 | Batch: 43200 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "14.075138644082472\n",
      "Train Epoch:  1 | Batch: 43600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1618.6672585923225\n",
      "Train Epoch:  1 | Batch: 44000 | Loss: 4.05\n",
      "tensor([4609.])\n",
      "4744.413729629479\n",
      "Train Epoch:  1 | Batch: 44400 | Loss: 11.86\n",
      "tensor([4609.])\n",
      "102.76280266838148\n",
      "Train Epoch:  1 | Batch: 44800 | Loss: 0.26\n",
      "tensor([4609.])\n",
      "3049.4984674993902\n",
      "Train Epoch:  1 | Batch: 45200 | Loss: 7.62\n",
      "tensor([4609.])\n",
      "100.63894213642925\n",
      "Train Epoch:  1 | Batch: 45600 | Loss: 0.25\n",
      "tensor([4609.])\n",
      "2833.483606902417\n",
      "Train Epoch:  1 | Batch: 46000 | Loss: 7.08\n",
      "tensor([4609.])\n",
      "775.0786995333619\n",
      "Train Epoch:  1 | Batch: 46400 | Loss: 1.94\n",
      "tensor([4609.])\n",
      "2286.6709666857496\n",
      "Train Epoch:  1 | Batch: 46800 | Loss: 5.72\n",
      "tensor([4609.])\n",
      "1615.9001699322835\n",
      "Train Epoch:  1 | Batch: 47200 | Loss: 4.04\n",
      "tensor([4609.])\n",
      "1107.1346766944043\n",
      "Train Epoch:  1 | Batch: 47600 | Loss: 2.77\n",
      "tensor([4609.])\n",
      "2612.0470335930586\n",
      "Train Epoch:  1 | Batch: 48000 | Loss: 6.53\n",
      "tensor([4609.])\n",
      "438.3066487801261\n",
      "Train Epoch:  1 | Batch: 48400 | Loss: 1.10\n",
      "tensor([4609.])\n",
      "2931.2660215431824\n",
      "Train Epoch:  1 | Batch: 48800 | Loss: 7.33\n",
      "tensor([4609.])\n",
      "83.63400370837189\n",
      "Train Epoch:  1 | Batch: 49200 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "1684.2144411271438\n",
      "Train Epoch:  1 | Batch: 49600 | Loss: 4.21\n",
      "tensor([4609.])\n",
      "73.29774741502479\n",
      "Train Epoch:  1 | Batch: 50000 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "4479.9227152206\n",
      "Train Epoch:  1 | Batch: 50400 | Loss: 11.20\n",
      "tensor([4609.])\n",
      "27.649907952174544\n",
      "Train Epoch:  1 | Batch: 50800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2711.1517639521044\n",
      "Train Epoch:  1 | Batch: 51200 | Loss: 6.78\n",
      "tensor([4609.])\n",
      "26.68712003203109\n",
      "Train Epoch:  1 | Batch: 51600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "4119.969944020733\n",
      "Train Epoch:  1 | Batch: 52000 | Loss: 10.30\n",
      "tensor([4609.])\n",
      "36.028894362039864\n",
      "Train Epoch:  1 | Batch: 52400 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3632.9490735512227\n",
      "Train Epoch:  1 | Batch: 52800 | Loss: 9.08\n",
      "tensor([4609.])\n",
      "53.06902322312817\n",
      "Train Epoch:  1 | Batch: 53200 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "3386.0154326814227\n",
      "Train Epoch:  1 | Batch: 53600 | Loss: 8.47\n",
      "tensor([4609.])\n",
      "153.70599591173232\n",
      "Train Epoch:  1 | Batch: 54000 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "2261.839470989071\n",
      "Train Epoch:  1 | Batch: 54400 | Loss: 5.65\n",
      "tensor([4609.])\n",
      "450.9317127154209\n",
      "Train Epoch:  1 | Batch: 54800 | Loss: 1.13\n",
      "tensor([4609.])\n",
      "966.3057998861186\n",
      "Train Epoch:  1 | Batch: 55200 | Loss: 2.42\n",
      "tensor([4609.])\n",
      "643.5080312336795\n",
      "Train Epoch:  1 | Batch: 55600 | Loss: 1.61\n",
      "tensor([4609.])\n",
      "850.4244888008106\n",
      "Train Epoch:  1 | Batch: 56000 | Loss: 2.13\n",
      "tensor([4609.])\n",
      "3202.734850798268\n",
      "Train Epoch:  1 | Batch: 56400 | Loss: 8.01\n",
      "tensor([4609.])\n",
      "314.6155056450516\n",
      "Train Epoch:  1 | Batch: 56800 | Loss: 0.79\n",
      "tensor([4609.])\n",
      "3436.393563142512\n",
      "Train Epoch:  1 | Batch: 57200 | Loss: 8.59\n",
      "tensor([4609.])\n",
      "203.50887255417183\n",
      "Train Epoch:  1 | Batch: 57600 | Loss: 0.51\n",
      "tensor([4609.])\n",
      "3675.7134364927188\n",
      "Train Epoch:  1 | Batch: 58000 | Loss: 9.19\n",
      "tensor([4609.])\n",
      "78.81209312472492\n",
      "Train Epoch:  1 | Batch: 58400 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "3939.0494005074725\n",
      "Train Epoch:  1 | Batch: 58800 | Loss: 9.85\n",
      "tensor([4609.])\n",
      "34.68686016974971\n",
      "Train Epoch:  1 | Batch: 59200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3608.0998361995444\n",
      "Train Epoch:  1 | Batch: 59600 | Loss: 9.02\n",
      "tensor([4609.])\n",
      "37.62351300846785\n",
      "Train Epoch:  1 | Batch: 60000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "4649.409486295655\n",
      "Train Epoch:  1 | Batch: 60400 | Loss: 11.62\n",
      "tensor([4609.])\n",
      "18.648974048905075\n",
      "Train Epoch:  1 | Batch: 60800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "1426.3113177232444\n",
      "Train Epoch:  1 | Batch: 61200 | Loss: 3.57\n",
      "tensor([4609.])\n",
      "25.188156640157104\n",
      "Train Epoch:  1 | Batch: 61600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2741.8274424783885\n",
      "Train Epoch:  1 | Batch: 62000 | Loss: 6.85\n",
      "tensor([4609.])\n",
      "233.0591065818444\n",
      "Train Epoch:  1 | Batch: 62400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "1985.0599394144956\n",
      "Train Epoch:  1 | Batch: 62800 | Loss: 4.96\n",
      "tensor([4609.])\n",
      "313.9394772895612\n",
      "Train Epoch:  1 | Batch: 63200 | Loss: 0.78\n",
      "tensor([4609.])\n",
      "1507.3497406356037\n",
      "Train Epoch:  1 | Batch: 63600 | Loss: 3.77\n",
      "tensor([4609.])\n",
      "1118.061563597992\n",
      "Train Epoch:  1 | Batch: 64000 | Loss: 2.80\n",
      "tensor([4609.])\n",
      "734.6866126586683\n",
      "Train Epoch:  1 | Batch: 64400 | Loss: 1.84\n",
      "tensor([4609.])\n",
      "2147.177077293396\n",
      "Train Epoch:  1 | Batch: 64800 | Loss: 5.37\n",
      "tensor([4609.])\n",
      "332.6629317542538\n",
      "Train Epoch:  1 | Batch: 65200 | Loss: 0.83\n",
      "tensor([4609.])\n",
      "2173.2846454461105\n",
      "Train Epoch:  1 | Batch: 65600 | Loss: 5.43\n",
      "tensor([4609.])\n",
      "209.05947755509987\n",
      "Train Epoch:  1 | Batch: 66000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "4192.795201078057\n",
      "Train Epoch:  1 | Batch: 66400 | Loss: 10.48\n",
      "tensor([4609.])\n",
      "39.65821917401627\n",
      "Train Epoch:  1 | Batch: 66800 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2115.573464186862\n",
      "Train Epoch:  1 | Batch: 67200 | Loss: 5.29\n",
      "tensor([4609.])\n",
      "20.010699167381972\n",
      "Train Epoch:  1 | Batch: 67600 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3818.110217679292\n",
      "Train Epoch:  1 | Batch: 68000 | Loss: 9.55\n",
      "tensor([4609.])\n",
      "24.936530423350632\n",
      "Train Epoch:  1 | Batch: 68400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "8811.173005074263\n",
      "Train Epoch:  1 | Batch: 68800 | Loss: 22.03\n",
      "tensor([4609.])\n",
      "80.19129958515987\n",
      "Train Epoch:  1 | Batch: 69200 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "2457.0615341770463\n",
      "Train Epoch:  1 | Batch: 69600 | Loss: 6.14\n",
      "tensor([4609.])\n",
      "386.56622775411233\n",
      "Train Epoch:  1 | Batch: 70000 | Loss: 0.97\n",
      "tensor([4609.])\n",
      "2068.6906394520774\n",
      "Train Epoch:  1 | Batch: 70400 | Loss: 5.17\n",
      "tensor([4609.])\n",
      "933.7147804973647\n",
      "Train Epoch:  1 | Batch: 70800 | Loss: 2.33\n",
      "tensor([4609.])\n",
      "1017.735156507697\n",
      "Train Epoch:  1 | Batch: 71200 | Loss: 2.54\n",
      "tensor([4609.])\n",
      "1709.9280698341317\n",
      "Train Epoch:  1 | Batch: 71600 | Loss: 4.27\n",
      "tensor([4609.])\n",
      "221.36961073428392\n",
      "Train Epoch:  1 | Batch: 72000 | Loss: 0.55\n",
      "tensor([4609.])\n",
      "1086.521482759621\n",
      "Train Epoch:  1 | Batch: 72400 | Loss: 2.72\n",
      "tensor([4609.])\n",
      "116.50462645268999\n",
      "Train Epoch:  1 | Batch: 72800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3162.4255260089412\n",
      "Train Epoch:  1 | Batch: 73200 | Loss: 7.91\n",
      "tensor([4609.])\n",
      "31.998042189050466\n",
      "Train Epoch:  1 | Batch: 73600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2437.8354303627275\n",
      "Train Epoch:  1 | Batch: 74000 | Loss: 6.09\n",
      "tensor([4609.])\n",
      "40.91618664748967\n",
      "Train Epoch:  1 | Batch: 74400 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2410.4658440751955\n",
      "Train Epoch:  1 | Batch: 74800 | Loss: 6.03\n",
      "tensor([4609.])\n",
      "24.426501902285963\n",
      "Train Epoch:  1 | Batch: 75200 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2602.511425317265\n",
      "Train Epoch:  1 | Batch: 75600 | Loss: 6.51\n",
      "tensor([4609.])\n",
      "20.719978051725775\n",
      "Train Epoch:  1 | Batch: 76000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2603.9098288360983\n",
      "Train Epoch:  1 | Batch: 76400 | Loss: 6.51\n",
      "tensor([4609.])\n",
      "27.787904545664787\n",
      "Train Epoch:  1 | Batch: 76800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2439.3577872663736\n",
      "Train Epoch:  1 | Batch: 77200 | Loss: 6.10\n",
      "tensor([4609.])\n",
      "73.97394413175061\n",
      "Train Epoch:  1 | Batch: 77600 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "1143.298586905934\n",
      "Train Epoch:  1 | Batch: 78000 | Loss: 2.86\n",
      "tensor([4609.])\n",
      "174.2922223880887\n",
      "Train Epoch:  1 | Batch: 78400 | Loss: 0.44\n",
      "tensor([4609.])\n",
      "2297.133121797815\n",
      "Train Epoch:  1 | Batch: 78800 | Loss: 5.74\n",
      "tensor([4609.])\n",
      "2216.024288599845\n",
      "Train Epoch:  1 | Batch: 79200 | Loss: 5.54\n",
      "tensor([4609.])\n",
      "1156.6995116961189\n",
      "Train Epoch:  1 | Batch: 79600 | Loss: 2.89\n",
      "tensor([4609.])\n",
      "1983.27142214356\n",
      "Train Epoch:  1 | Batch: 80000 | Loss: 4.96\n",
      "tensor([4609.])\n",
      "442.79993230337277\n",
      "Train Epoch:  1 | Batch: 80400 | Loss: 1.11\n",
      "tensor([4609.])\n",
      "1905.6480651791207\n",
      "Train Epoch:  1 | Batch: 80800 | Loss: 4.76\n",
      "tensor([4609.])\n",
      "87.57598070753738\n",
      "Train Epoch:  1 | Batch: 81200 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "1243.3774126749486\n",
      "Train Epoch:  1 | Batch: 81600 | Loss: 3.11\n",
      "tensor([4609.])\n",
      "42.19657378550619\n",
      "Train Epoch:  1 | Batch: 82000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "2084.3115649651736\n",
      "Train Epoch:  1 | Batch: 82400 | Loss: 5.21\n",
      "tensor([4609.])\n",
      "30.336210718378425\n",
      "Train Epoch:  1 | Batch: 82800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2407.8628905871883\n",
      "Train Epoch:  1 | Batch: 83200 | Loss: 6.02\n",
      "tensor([4609.])\n",
      "15.202232902403921\n",
      "Train Epoch:  1 | Batch: 83600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1682.9914306141436\n",
      "Train Epoch:  1 | Batch: 84000 | Loss: 4.21\n",
      "tensor([4609.])\n",
      "13.17121436051093\n",
      "Train Epoch:  1 | Batch: 84400 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2748.0456796763465\n",
      "Train Epoch:  1 | Batch: 84800 | Loss: 6.87\n",
      "tensor([4609.])\n",
      "58.25936571089551\n",
      "Train Epoch:  1 | Batch: 85200 | Loss: 0.15\n",
      "tensor([4609.])\n",
      "3580.6074818400666\n",
      "Train Epoch:  1 | Batch: 85600 | Loss: 8.95\n",
      "tensor([4609.])\n",
      "34.623437359929085\n",
      "Train Epoch:  1 | Batch: 86000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2767.8882091213018\n",
      "Train Epoch:  1 | Batch: 86400 | Loss: 6.92\n",
      "tensor([4609.])\n",
      "404.86338522518054\n",
      "Train Epoch:  1 | Batch: 86800 | Loss: 1.01\n",
      "tensor([4609.])\n",
      "2228.349473312497\n",
      "Train Epoch:  1 | Batch: 87200 | Loss: 5.57\n",
      "tensor([4609.])\n",
      "1512.679392392747\n",
      "Train Epoch:  1 | Batch: 87600 | Loss: 3.78\n",
      "tensor([4609.])\n",
      "1420.3511883821338\n",
      "Train Epoch:  1 | Batch: 88000 | Loss: 3.55\n",
      "tensor([4609.])\n",
      "2108.353373623453\n",
      "Train Epoch:  1 | Batch: 88400 | Loss: 5.27\n",
      "tensor([4609.])\n",
      "795.5906335362233\n",
      "Train Epoch:  1 | Batch: 88800 | Loss: 1.99\n",
      "tensor([4609.])\n",
      "3559.2394553148188\n",
      "Train Epoch:  1 | Batch: 89200 | Loss: 8.90\n",
      "tensor([4609.])\n",
      "151.18732244335115\n",
      "Train Epoch:  1 | Batch: 89600 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "2559.393794742413\n",
      "Train Epoch:  1 | Batch: 90000 | Loss: 6.40\n",
      "tensor([4609.])\n",
      "100.35348978824914\n",
      "Train Epoch:  1 | Batch: 90400 | Loss: 0.25\n",
      "tensor([4609.])\n",
      "5681.746088286862\n",
      "Train Epoch:  1 | Batch: 90800 | Loss: 14.20\n",
      "tensor([4609.])\n",
      "36.80000862060115\n",
      "Train Epoch:  1 | Batch: 91200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "5035.515990595333\n",
      "Train Epoch:  1 | Batch: 91600 | Loss: 12.59\n",
      "tensor([4609.])\n",
      "81.1025349223055\n",
      "Train Epoch:  1 | Batch: 92000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "6524.126287093386\n",
      "Train Epoch:  1 | Batch: 92400 | Loss: 16.31\n",
      "tensor([4609.])\n",
      "63.365744144655764\n",
      "Train Epoch:  1 | Batch: 92800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5435.619404155761\n",
      "Train Epoch:  1 | Batch: 93200 | Loss: 13.59\n",
      "tensor([4609.])\n",
      "71.01967617776245\n",
      "Train Epoch:  1 | Batch: 93600 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "1779.4668847997673\n",
      "Train Epoch:  1 | Batch: 94000 | Loss: 4.45\n",
      "tensor([4609.])\n",
      "54.79584388411604\n",
      "Train Epoch:  1 | Batch: 94400 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "3450.060296788346\n",
      "Train Epoch:  1 | Batch: 94800 | Loss: 8.63\n",
      "tensor([4609.])\n",
      "382.9012205793988\n",
      "Train Epoch:  1 | Batch: 95200 | Loss: 0.96\n",
      "tensor([4609.])\n",
      "1226.0479142591357\n",
      "Train Epoch:  1 | Batch: 95600 | Loss: 3.07\n",
      "tensor([4609.])\n",
      "894.3923640749417\n",
      "Train Epoch:  1 | Batch: 96000 | Loss: 2.24\n",
      "tensor([4609.])\n",
      "1161.0620649703778\n",
      "Train Epoch:  1 | Batch: 96400 | Loss: 2.90\n",
      "tensor([4609.])\n",
      "3295.1742317541502\n",
      "Train Epoch:  1 | Batch: 96800 | Loss: 8.24\n",
      "tensor([4609.])\n",
      "490.1981960423291\n",
      "Train Epoch:  1 | Batch: 97200 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "1889.0128980556037\n",
      "Train Epoch:  1 | Batch: 97600 | Loss: 4.72\n",
      "tensor([4609.])\n",
      "78.75305930059403\n",
      "Train Epoch:  1 | Batch: 98000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "1578.6861346866935\n",
      "Train Epoch:  1 | Batch: 98400 | Loss: 3.95\n",
      "tensor([4609.])\n",
      "30.419048579176888\n",
      "Train Epoch:  1 | Batch: 98800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2913.69710431993\n",
      "Train Epoch:  1 | Batch: 99200 | Loss: 7.28\n",
      "tensor([4609.])\n",
      "34.006338238483295\n",
      "Train Epoch:  1 | Batch: 99600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2188.829730250407\n",
      "Train Epoch:  1 | Batch: 100000 | Loss: 5.47\n",
      "tensor([4609.])\n",
      "15.831909850705415\n",
      "Train Epoch:  1 | Batch: 100400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2116.1353764757514\n",
      "Train Epoch:  1 | Batch: 100800 | Loss: 5.29\n",
      "tensor([4609.])\n",
      "13.716392842354253\n",
      "Train Epoch:  1 | Batch: 101200 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2005.8926475471817\n",
      "Train Epoch:  1 | Batch: 101600 | Loss: 5.01\n",
      "tensor([4609.])\n",
      "18.647353478474542\n",
      "Train Epoch:  1 | Batch: 102000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2917.385993868578\n",
      "Train Epoch:  1 | Batch: 102400 | Loss: 7.29\n",
      "tensor([4609.])\n",
      "128.33459330396727\n",
      "Train Epoch:  1 | Batch: 102800 | Loss: 0.32\n",
      "35\n",
      "\n",
      "Valid Epoch:  1 | Loss: 1.06\n",
      "tensor([4609.])\n",
      "6147.8553298823535\n",
      "Train Epoch:  2 | Batch:  400 | Loss: 15.37\n",
      "tensor([4609.])\n",
      "72.11234362144023\n",
      "Train Epoch:  2 | Batch:  800 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "4420.357114484534\n",
      "Train Epoch:  2 | Batch: 1200 | Loss: 11.05\n",
      "tensor([4609.])\n",
      "28.82747156266123\n",
      "Train Epoch:  2 | Batch: 1600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3071.7139634266496\n",
      "Train Epoch:  2 | Batch: 2000 | Loss: 7.68\n",
      "tensor([4609.])\n",
      "15.028834897559136\n",
      "Train Epoch:  2 | Batch: 2400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1394.7898987913504\n",
      "Train Epoch:  2 | Batch: 2800 | Loss: 3.49\n",
      "tensor([4609.])\n",
      "16.81704400619492\n",
      "Train Epoch:  2 | Batch: 3200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "3021.941582325846\n",
      "Train Epoch:  2 | Batch: 3600 | Loss: 7.55\n",
      "tensor([4609.])\n",
      "219.27857031149324\n",
      "Train Epoch:  2 | Batch: 4000 | Loss: 0.55\n",
      "tensor([4609.])\n",
      "2192.345016119536\n",
      "Train Epoch:  2 | Batch: 4400 | Loss: 5.48\n",
      "tensor([4609.])\n",
      "176.2064690203406\n",
      "Train Epoch:  2 | Batch: 4800 | Loss: 0.44\n",
      "tensor([4609.])\n",
      "1906.3427681419998\n",
      "Train Epoch:  2 | Batch: 5200 | Loss: 4.77\n",
      "tensor([4609.])\n",
      "67294.75381866097\n",
      "Train Epoch:  2 | Batch: 5600 | Loss: 168.24\n",
      "tensor([4609.])\n",
      "1114.4604496718384\n",
      "Train Epoch:  2 | Batch: 6000 | Loss: 2.79\n",
      "tensor([4609.])\n",
      "1929.6858011020813\n",
      "Train Epoch:  2 | Batch: 6400 | Loss: 4.82\n",
      "tensor([4609.])\n",
      "493.1011640317738\n",
      "Train Epoch:  2 | Batch: 6800 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "2028.0756720968056\n",
      "Train Epoch:  2 | Batch: 7200 | Loss: 5.07\n",
      "tensor([4609.])\n",
      "199.6598587804474\n",
      "Train Epoch:  2 | Batch: 7600 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "2718.765562020242\n",
      "Train Epoch:  2 | Batch: 8000 | Loss: 6.80\n",
      "tensor([4609.])\n",
      "46.812596674542874\n",
      "Train Epoch:  2 | Batch: 8400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "1918.8270039949566\n",
      "Train Epoch:  2 | Batch: 8800 | Loss: 4.80\n",
      "tensor([4609.])\n",
      "53.16756859887391\n",
      "Train Epoch:  2 | Batch: 9200 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "6927.441916182637\n",
      "Train Epoch:  2 | Batch: 9600 | Loss: 17.32\n",
      "tensor([4609.])\n",
      "44.08229667972773\n",
      "Train Epoch:  2 | Batch: 10000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "6150.778266459703\n",
      "Train Epoch:  2 | Batch: 10400 | Loss: 15.38\n",
      "tensor([4609.])\n",
      "60.84365339204669\n",
      "Train Epoch:  2 | Batch: 10800 | Loss: 0.15\n",
      "tensor([4609.])\n",
      "5662.663441386074\n",
      "Train Epoch:  2 | Batch: 11200 | Loss: 14.16\n",
      "tensor([4609.])\n",
      "90.71177844889462\n",
      "Train Epoch:  2 | Batch: 11600 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "6580.274529349059\n",
      "Train Epoch:  2 | Batch: 12000 | Loss: 16.45\n",
      "tensor([4609.])\n",
      "230.23207842372358\n",
      "Train Epoch:  2 | Batch: 12400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "4344.410864860751\n",
      "Train Epoch:  2 | Batch: 12800 | Loss: 10.86\n",
      "tensor([4609.])\n",
      "958.2916849646717\n",
      "Train Epoch:  2 | Batch: 13200 | Loss: 2.40\n",
      "tensor([4609.])\n",
      "646.3662796425633\n",
      "Train Epoch:  2 | Batch: 13600 | Loss: 1.62\n",
      "tensor([4609.])\n",
      "710.8381603837479\n",
      "Train Epoch:  2 | Batch: 14000 | Loss: 1.78\n",
      "tensor([4609.])\n",
      "1231.3606755672954\n",
      "Train Epoch:  2 | Batch: 14400 | Loss: 3.08\n",
      "tensor([4609.])\n",
      "3889.781855428824\n",
      "Train Epoch:  2 | Batch: 14800 | Loss: 9.72\n",
      "tensor([4609.])\n",
      "199.45603195903823\n",
      "Train Epoch:  2 | Batch: 15200 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "3668.578679198865\n",
      "Train Epoch:  2 | Batch: 15600 | Loss: 9.17\n",
      "tensor([4609.])\n",
      "75.49143486074172\n",
      "Train Epoch:  2 | Batch: 16000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3221.891694717109\n",
      "Train Epoch:  2 | Batch: 16400 | Loss: 8.05\n",
      "tensor([4609.])\n",
      "49.886326206382364\n",
      "Train Epoch:  2 | Batch: 16800 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2753.3004632354714\n",
      "Train Epoch:  2 | Batch: 17200 | Loss: 6.88\n",
      "tensor([4609.])\n",
      "34.83911301684566\n",
      "Train Epoch:  2 | Batch: 17600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2874.541151754558\n",
      "Train Epoch:  2 | Batch: 18000 | Loss: 7.19\n",
      "tensor([4609.])\n",
      "26.234885909594595\n",
      "Train Epoch:  2 | Batch: 18400 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2766.1754849255085\n",
      "Train Epoch:  2 | Batch: 18800 | Loss: 6.92\n",
      "tensor([4609.])\n",
      "17.842384403571486\n",
      "Train Epoch:  2 | Batch: 19200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2758.067790975794\n",
      "Train Epoch:  2 | Batch: 19600 | Loss: 6.90\n",
      "tensor([4609.])\n",
      "28.18317819107324\n",
      "Train Epoch:  2 | Batch: 20000 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "1331.800032439176\n",
      "Train Epoch:  2 | Batch: 20400 | Loss: 3.33\n",
      "tensor([4609.])\n",
      "52.432228963356465\n",
      "Train Epoch:  2 | Batch: 20800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "2060.487485352438\n",
      "Train Epoch:  2 | Batch: 21200 | Loss: 5.15\n",
      "tensor([4609.])\n",
      "667.5906005138531\n",
      "Train Epoch:  2 | Batch: 21600 | Loss: 1.67\n",
      "tensor([4609.])\n",
      "2036.7883696788922\n",
      "Train Epoch:  2 | Batch: 22000 | Loss: 5.09\n",
      "tensor([4609.])\n",
      "819.1693905657157\n",
      "Train Epoch:  2 | Batch: 22400 | Loss: 2.05\n",
      "tensor([4609.])\n",
      "874.6493049934506\n",
      "Train Epoch:  2 | Batch: 22800 | Loss: 2.19\n",
      "tensor([4609.])\n",
      "1759.5290378271602\n",
      "Train Epoch:  2 | Batch: 23200 | Loss: 4.40\n",
      "tensor([4609.])\n",
      "461.2590738381259\n",
      "Train Epoch:  2 | Batch: 23600 | Loss: 1.15\n",
      "tensor([4609.])\n",
      "2686.470693862997\n",
      "Train Epoch:  2 | Batch: 24000 | Loss: 6.72\n",
      "tensor([4609.])\n",
      "231.7581009636633\n",
      "Train Epoch:  2 | Batch: 24400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "3555.1419662460685\n",
      "Train Epoch:  2 | Batch: 24800 | Loss: 8.89\n",
      "tensor([4609.])\n",
      "85234543067.70612\n",
      "Train Epoch:  2 | Batch: 25200 | Loss: 213086357.67\n",
      "tensor([4609.])\n",
      "146465.12760543823\n",
      "Train Epoch:  2 | Batch: 25600 | Loss: 366.16\n",
      "tensor([4609.])\n",
      "5239.628104418516\n",
      "Train Epoch:  2 | Batch: 26000 | Loss: 13.10\n",
      "tensor([4609.])\n",
      "2218.1228809915483\n",
      "Train Epoch:  2 | Batch: 26400 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "618.8677321132272\n",
      "Train Epoch:  2 | Batch: 26800 | Loss: 1.55\n",
      "tensor([4609.])\n",
      "3543.187053071335\n",
      "Train Epoch:  2 | Batch: 27200 | Loss: 8.86\n",
      "tensor([4609.])\n",
      "25.10857610614039\n",
      "Train Epoch:  2 | Batch: 27600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2652.7534418730065\n",
      "Train Epoch:  2 | Batch: 28000 | Loss: 6.63\n",
      "tensor([4609.])\n",
      "18.78270295308903\n",
      "Train Epoch:  2 | Batch: 28400 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2535.0205325405113\n",
      "Train Epoch:  2 | Batch: 28800 | Loss: 6.34\n",
      "tensor([4609.])\n",
      "75.3248779757414\n",
      "Train Epoch:  2 | Batch: 29200 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3990.007458945969\n",
      "Train Epoch:  2 | Batch: 29600 | Loss: 9.98\n",
      "tensor([4609.])\n",
      "406.8019181159325\n",
      "Train Epoch:  2 | Batch: 30000 | Loss: 1.02\n",
      "tensor([4609.])\n",
      "2287.568235252984\n",
      "Train Epoch:  2 | Batch: 30400 | Loss: 5.72\n",
      "tensor([4609.])\n",
      "1217.7704182872549\n",
      "Train Epoch:  2 | Batch: 30800 | Loss: 3.04\n",
      "tensor([4609.])\n",
      "892.732518943958\n",
      "Train Epoch:  2 | Batch: 31200 | Loss: 2.23\n",
      "tensor([4609.])\n",
      "1830.7734605241567\n",
      "Train Epoch:  2 | Batch: 31600 | Loss: 4.58\n",
      "tensor([4609.])\n",
      "208.3324655862525\n",
      "Train Epoch:  2 | Batch: 32000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "1171.6666428835597\n",
      "Train Epoch:  2 | Batch: 32400 | Loss: 2.93\n",
      "tensor([4609.])\n",
      "117.92341558786575\n",
      "Train Epoch:  2 | Batch: 32800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3666.2521956153214\n",
      "Train Epoch:  2 | Batch: 33200 | Loss: 9.17\n",
      "tensor([4609.])\n",
      "39.65594081627205\n",
      "Train Epoch:  2 | Batch: 33600 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "3610.4585552467033\n",
      "Train Epoch:  2 | Batch: 34000 | Loss: 9.03\n",
      "tensor([4609.])\n",
      "67.47677412303165\n",
      "Train Epoch:  2 | Batch: 34400 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "5069.351448021829\n",
      "Train Epoch:  2 | Batch: 34800 | Loss: 12.67\n",
      "tensor([4609.])\n",
      "32.94495897181332\n",
      "Train Epoch:  2 | Batch: 35200 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "3415.6223894655704\n",
      "Train Epoch:  2 | Batch: 35600 | Loss: 8.54\n",
      "tensor([4609.])\n",
      "16.70827749709133\n",
      "Train Epoch:  2 | Batch: 36000 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2093.667823191732\n",
      "Train Epoch:  2 | Batch: 36400 | Loss: 5.23\n",
      "tensor([4609.])\n",
      "22.137735729571432\n",
      "Train Epoch:  2 | Batch: 36800 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "3184.3673085477203\n",
      "Train Epoch:  2 | Batch: 37200 | Loss: 7.96\n",
      "tensor([4609.])\n",
      "93.08282160479575\n",
      "Train Epoch:  2 | Batch: 37600 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "1307.5382434937637\n",
      "Train Epoch:  2 | Batch: 38000 | Loss: 3.27\n",
      "tensor([4609.])\n",
      "253.17326590325683\n",
      "Train Epoch:  2 | Batch: 38400 | Loss: 0.63\n",
      "tensor([4609.])\n",
      "1595.9879332829732\n",
      "Train Epoch:  2 | Batch: 38800 | Loss: 3.99\n",
      "tensor([4609.])\n",
      "1487.0172764901072\n",
      "Train Epoch:  2 | Batch: 39200 | Loss: 3.72\n",
      "tensor([4609.])\n",
      "866.0983108563814\n",
      "Train Epoch:  2 | Batch: 39600 | Loss: 2.17\n",
      "tensor([4609.])\n",
      "1525.5430059155915\n",
      "Train Epoch:  2 | Batch: 40000 | Loss: 3.81\n",
      "tensor([4609.])\n",
      "405.0877334573306\n",
      "Train Epoch:  2 | Batch: 40400 | Loss: 1.01\n",
      "tensor([4609.])\n",
      "2215.434454689035\n",
      "Train Epoch:  2 | Batch: 40800 | Loss: 5.54\n",
      "tensor([4609.])\n",
      "239.27496336586773\n",
      "Train Epoch:  2 | Batch: 41200 | Loss: 0.60\n",
      "tensor([4609.])\n",
      "3156.2026514848694\n",
      "Train Epoch:  2 | Batch: 41600 | Loss: 7.89\n",
      "tensor([4609.])\n",
      "81.53989121643826\n",
      "Train Epoch:  2 | Batch: 42000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "2611.552877186332\n",
      "Train Epoch:  2 | Batch: 42400 | Loss: 6.53\n",
      "tensor([4609.])\n",
      "29.045068078674376\n",
      "Train Epoch:  2 | Batch: 42800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2134.99796131067\n",
      "Train Epoch:  2 | Batch: 43200 | Loss: 5.34\n",
      "tensor([4609.])\n",
      "14.026248451089486\n",
      "Train Epoch:  2 | Batch: 43600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1537.591294914484\n",
      "Train Epoch:  2 | Batch: 44000 | Loss: 3.84\n",
      "tensor([4609.])\n",
      "4540.448429474607\n",
      "Train Epoch:  2 | Batch: 44400 | Loss: 11.35\n",
      "tensor([4609.])\n",
      "89.36601044703275\n",
      "Train Epoch:  2 | Batch: 44800 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "3002.7565147057176\n",
      "Train Epoch:  2 | Batch: 45200 | Loss: 7.51\n",
      "tensor([4609.])\n",
      "71.9477205411531\n",
      "Train Epoch:  2 | Batch: 45600 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "2726.0848248330876\n",
      "Train Epoch:  2 | Batch: 46000 | Loss: 6.82\n",
      "tensor([4609.])\n",
      "619.5328656593338\n",
      "Train Epoch:  2 | Batch: 46400 | Loss: 1.55\n",
      "tensor([4609.])\n",
      "2216.4460456720553\n",
      "Train Epoch:  2 | Batch: 46800 | Loss: 5.54\n",
      "tensor([4609.])\n",
      "1519.8058325992897\n",
      "Train Epoch:  2 | Batch: 47200 | Loss: 3.80\n",
      "tensor([4609.])\n",
      "1067.3356260159053\n",
      "Train Epoch:  2 | Batch: 47600 | Loss: 2.67\n",
      "tensor([4609.])\n",
      "2926.94786496507\n",
      "Train Epoch:  2 | Batch: 48000 | Loss: 7.32\n",
      "tensor([4609.])\n",
      "443.4485021303408\n",
      "Train Epoch:  2 | Batch: 48400 | Loss: 1.11\n",
      "tensor([4609.])\n",
      "2931.4946192735806\n",
      "Train Epoch:  2 | Batch: 48800 | Loss: 7.33\n",
      "tensor([4609.])\n",
      "92.47079813480377\n",
      "Train Epoch:  2 | Batch: 49200 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "1630.7756967172027\n",
      "Train Epoch:  2 | Batch: 49600 | Loss: 4.08\n",
      "tensor([4609.])\n",
      "76.41107839019969\n",
      "Train Epoch:  2 | Batch: 50000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "4422.1445048600435\n",
      "Train Epoch:  2 | Batch: 50400 | Loss: 11.06\n",
      "tensor([4609.])\n",
      "29.621878262609243\n",
      "Train Epoch:  2 | Batch: 50800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2610.251197468955\n",
      "Train Epoch:  2 | Batch: 51200 | Loss: 6.53\n",
      "tensor([4609.])\n",
      "28.4403803832829\n",
      "Train Epoch:  2 | Batch: 51600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3952.9164809398353\n",
      "Train Epoch:  2 | Batch: 52000 | Loss: 9.88\n",
      "tensor([4609.])\n",
      "36.32910903263837\n",
      "Train Epoch:  2 | Batch: 52400 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3579.693325139582\n",
      "Train Epoch:  2 | Batch: 52800 | Loss: 8.95\n",
      "tensor([4609.])\n",
      "49.99091554898769\n",
      "Train Epoch:  2 | Batch: 53200 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "3469.2721593691967\n",
      "Train Epoch:  2 | Batch: 53600 | Loss: 8.67\n",
      "tensor([4609.])\n",
      "144.62213796935976\n",
      "Train Epoch:  2 | Batch: 54000 | Loss: 0.36\n",
      "tensor([4609.])\n",
      "2220.577896077186\n",
      "Train Epoch:  2 | Batch: 54400 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "505.57724226103164\n",
      "Train Epoch:  2 | Batch: 54800 | Loss: 1.26\n",
      "tensor([4609.])\n",
      "1129.1159473566804\n",
      "Train Epoch:  2 | Batch: 55200 | Loss: 2.82\n",
      "tensor([4609.])\n",
      "627.1722949726973\n",
      "Train Epoch:  2 | Batch: 55600 | Loss: 1.57\n",
      "tensor([4609.])\n",
      "842.5557629799005\n",
      "Train Epoch:  2 | Batch: 56000 | Loss: 2.11\n",
      "tensor([4609.])\n",
      "3233.4979500803165\n",
      "Train Epoch:  2 | Batch: 56400 | Loss: 8.08\n",
      "tensor([4609.])\n",
      "310.7791452682577\n",
      "Train Epoch:  2 | Batch: 56800 | Loss: 0.78\n",
      "tensor([4609.])\n",
      "3468.359600114636\n",
      "Train Epoch:  2 | Batch: 57200 | Loss: 8.67\n",
      "tensor([4609.])\n",
      "202.2707155025564\n",
      "Train Epoch:  2 | Batch: 57600 | Loss: 0.51\n",
      "tensor([4609.])\n",
      "3714.140652681701\n",
      "Train Epoch:  2 | Batch: 58000 | Loss: 9.29\n",
      "tensor([4609.])\n",
      "75.77611139509827\n",
      "Train Epoch:  2 | Batch: 58400 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "4058.448671160266\n",
      "Train Epoch:  2 | Batch: 58800 | Loss: 10.15\n",
      "tensor([4609.])\n",
      "33.749693849589676\n",
      "Train Epoch:  2 | Batch: 59200 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "3670.34974434413\n",
      "Train Epoch:  2 | Batch: 59600 | Loss: 9.18\n",
      "tensor([4609.])\n",
      "36.66898768115789\n",
      "Train Epoch:  2 | Batch: 60000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "4689.472544474527\n",
      "Train Epoch:  2 | Batch: 60400 | Loss: 11.72\n",
      "tensor([4609.])\n",
      "19.398780210409313\n",
      "Train Epoch:  2 | Batch: 60800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "1409.8469577771612\n",
      "Train Epoch:  2 | Batch: 61200 | Loss: 3.52\n",
      "tensor([4609.])\n",
      "25.06388483941555\n",
      "Train Epoch:  2 | Batch: 61600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2700.8453143024817\n",
      "Train Epoch:  2 | Batch: 62000 | Loss: 6.75\n",
      "tensor([4609.])\n",
      "234.54919485282153\n",
      "Train Epoch:  2 | Batch: 62400 | Loss: 0.59\n",
      "tensor([4609.])\n",
      "1985.3721624398604\n",
      "Train Epoch:  2 | Batch: 62800 | Loss: 4.96\n",
      "tensor([4609.])\n",
      "339.82291067414917\n",
      "Train Epoch:  2 | Batch: 63200 | Loss: 0.85\n",
      "tensor([4609.])\n",
      "1442.9306220686994\n",
      "Train Epoch:  2 | Batch: 63600 | Loss: 3.61\n",
      "tensor([4609.])\n",
      "1132.2900022282265\n",
      "Train Epoch:  2 | Batch: 64000 | Loss: 2.83\n",
      "tensor([4609.])\n",
      "736.3124914369546\n",
      "Train Epoch:  2 | Batch: 64400 | Loss: 1.84\n",
      "tensor([4609.])\n",
      "2128.864496595692\n",
      "Train Epoch:  2 | Batch: 64800 | Loss: 5.32\n",
      "tensor([4609.])\n",
      "330.9761902350001\n",
      "Train Epoch:  2 | Batch: 65200 | Loss: 0.83\n",
      "tensor([4609.])\n",
      "2149.8804449047893\n",
      "Train Epoch:  2 | Batch: 65600 | Loss: 5.37\n",
      "tensor([4609.])\n",
      "209.07407761504874\n",
      "Train Epoch:  2 | Batch: 66000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "4231.035871103406\n",
      "Train Epoch:  2 | Batch: 66400 | Loss: 10.58\n",
      "tensor([4609.])\n",
      "41.24185961764306\n",
      "Train Epoch:  2 | Batch: 66800 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2300.890011943411\n",
      "Train Epoch:  2 | Batch: 67200 | Loss: 5.75\n",
      "tensor([4609.])\n",
      "20.040808567311615\n",
      "Train Epoch:  2 | Batch: 67600 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3665.738929338753\n",
      "Train Epoch:  2 | Batch: 68000 | Loss: 9.16\n",
      "tensor([4609.])\n",
      "24.592665932141244\n",
      "Train Epoch:  2 | Batch: 68400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "9054.450267994776\n",
      "Train Epoch:  2 | Batch: 68800 | Loss: 22.64\n",
      "tensor([4609.])\n",
      "67.09575276030228\n",
      "Train Epoch:  2 | Batch: 69200 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "2499.7094971295446\n",
      "Train Epoch:  2 | Batch: 69600 | Loss: 6.25\n",
      "tensor([4609.])\n",
      "367.2870563417673\n",
      "Train Epoch:  2 | Batch: 70000 | Loss: 0.92\n",
      "tensor([4609.])\n",
      "2050.7353810872883\n",
      "Train Epoch:  2 | Batch: 70400 | Loss: 5.13\n",
      "tensor([4609.])\n",
      "939.8983263541013\n",
      "Train Epoch:  2 | Batch: 70800 | Loss: 2.35\n",
      "tensor([4609.])\n",
      "997.4114609630778\n",
      "Train Epoch:  2 | Batch: 71200 | Loss: 2.49\n",
      "tensor([4609.])\n",
      "1683.7518829326145\n",
      "Train Epoch:  2 | Batch: 71600 | Loss: 4.21\n",
      "tensor([4609.])\n",
      "223.13694610237144\n",
      "Train Epoch:  2 | Batch: 72000 | Loss: 0.56\n",
      "tensor([4609.])\n",
      "1102.176555234706\n",
      "Train Epoch:  2 | Batch: 72400 | Loss: 2.76\n",
      "tensor([4609.])\n",
      "112.28433259110898\n",
      "Train Epoch:  2 | Batch: 72800 | Loss: 0.28\n",
      "tensor([4609.])\n",
      "3252.3099472234026\n",
      "Train Epoch:  2 | Batch: 73200 | Loss: 8.13\n",
      "tensor([4609.])\n",
      "31.793545943917707\n",
      "Train Epoch:  2 | Batch: 73600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2422.9044076441787\n",
      "Train Epoch:  2 | Batch: 74000 | Loss: 6.06\n",
      "tensor([4609.])\n",
      "40.619203821523115\n",
      "Train Epoch:  2 | Batch: 74400 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2432.9005025988445\n",
      "Train Epoch:  2 | Batch: 74800 | Loss: 6.08\n",
      "tensor([4609.])\n",
      "24.69494138727896\n",
      "Train Epoch:  2 | Batch: 75200 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2588.6789604816586\n",
      "Train Epoch:  2 | Batch: 75600 | Loss: 6.47\n",
      "tensor([4609.])\n",
      "20.608362898929045\n",
      "Train Epoch:  2 | Batch: 76000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2588.02090398781\n",
      "Train Epoch:  2 | Batch: 76400 | Loss: 6.47\n",
      "tensor([4609.])\n",
      "26.750339230522513\n",
      "Train Epoch:  2 | Batch: 76800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2463.594746978022\n",
      "Train Epoch:  2 | Batch: 77200 | Loss: 6.16\n",
      "tensor([4609.])\n",
      "64.61722549097613\n",
      "Train Epoch:  2 | Batch: 77600 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "1048.7232796279714\n",
      "Train Epoch:  2 | Batch: 78000 | Loss: 2.62\n",
      "tensor([4609.])\n",
      "167.57594535290264\n",
      "Train Epoch:  2 | Batch: 78400 | Loss: 0.42\n",
      "tensor([4609.])\n",
      "2289.848705390934\n",
      "Train Epoch:  2 | Batch: 78800 | Loss: 5.72\n",
      "tensor([4609.])\n",
      "2144.220474853646\n",
      "Train Epoch:  2 | Batch: 79200 | Loss: 5.36\n",
      "tensor([4609.])\n",
      "1156.16915165307\n",
      "Train Epoch:  2 | Batch: 79600 | Loss: 2.89\n",
      "tensor([4609.])\n",
      "2042.2427169680595\n",
      "Train Epoch:  2 | Batch: 80000 | Loss: 5.11\n",
      "tensor([4609.])\n",
      "441.1183561673388\n",
      "Train Epoch:  2 | Batch: 80400 | Loss: 1.10\n",
      "tensor([4609.])\n",
      "1895.8806889173575\n",
      "Train Epoch:  2 | Batch: 80800 | Loss: 4.74\n",
      "tensor([4609.])\n",
      "87.53584758983925\n",
      "Train Epoch:  2 | Batch: 81200 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "1265.769414533861\n",
      "Train Epoch:  2 | Batch: 81600 | Loss: 3.16\n",
      "tensor([4609.])\n",
      "41.91005233535543\n",
      "Train Epoch:  2 | Batch: 82000 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2084.6563405077904\n",
      "Train Epoch:  2 | Batch: 82400 | Loss: 5.21\n",
      "tensor([4609.])\n",
      "29.611346022225916\n",
      "Train Epoch:  2 | Batch: 82800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2519.7300242623314\n",
      "Train Epoch:  2 | Batch: 83200 | Loss: 6.30\n",
      "tensor([4609.])\n",
      "15.293983443174511\n",
      "Train Epoch:  2 | Batch: 83600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1701.2177863959223\n",
      "Train Epoch:  2 | Batch: 84000 | Loss: 4.25\n",
      "tensor([4609.])\n",
      "13.785722207510844\n",
      "Train Epoch:  2 | Batch: 84400 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2748.834626208991\n",
      "Train Epoch:  2 | Batch: 84800 | Loss: 6.87\n",
      "tensor([4609.])\n",
      "55.28633610391989\n",
      "Train Epoch:  2 | Batch: 85200 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "3382.9905374385417\n",
      "Train Epoch:  2 | Batch: 85600 | Loss: 8.46\n",
      "tensor([4609.])\n",
      "41.04483347944915\n",
      "Train Epoch:  2 | Batch: 86000 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2750.365938734263\n",
      "Train Epoch:  2 | Batch: 86400 | Loss: 6.88\n",
      "tensor([4609.])\n",
      "450.52694038534537\n",
      "Train Epoch:  2 | Batch: 86800 | Loss: 1.13\n",
      "tensor([4609.])\n",
      "2222.076178505551\n",
      "Train Epoch:  2 | Batch: 87200 | Loss: 5.56\n",
      "tensor([4609.])\n",
      "1671.1626880620606\n",
      "Train Epoch:  2 | Batch: 87600 | Loss: 4.18\n",
      "tensor([4609.])\n",
      "1423.810114134103\n",
      "Train Epoch:  2 | Batch: 88000 | Loss: 3.56\n",
      "tensor([4609.])\n",
      "2070.588048181962\n",
      "Train Epoch:  2 | Batch: 88400 | Loss: 5.18\n",
      "tensor([4609.])\n",
      "793.1671306109056\n",
      "Train Epoch:  2 | Batch: 88800 | Loss: 1.98\n",
      "tensor([4609.])\n",
      "3484.037373773288\n",
      "Train Epoch:  2 | Batch: 89200 | Loss: 8.71\n",
      "tensor([4609.])\n",
      "151.60655450075865\n",
      "Train Epoch:  2 | Batch: 89600 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "2594.186976019293\n",
      "Train Epoch:  2 | Batch: 90000 | Loss: 6.49\n",
      "tensor([4609.])\n",
      "101.42520400090143\n",
      "Train Epoch:  2 | Batch: 90400 | Loss: 0.25\n",
      "tensor([4609.])\n",
      "5589.057287396863\n",
      "Train Epoch:  2 | Batch: 90800 | Loss: 13.97\n",
      "tensor([4609.])\n",
      "38.493657058570534\n",
      "Train Epoch:  2 | Batch: 91200 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "4889.204251237214\n",
      "Train Epoch:  2 | Batch: 91600 | Loss: 12.22\n",
      "tensor([4609.])\n",
      "83.04790531191975\n",
      "Train Epoch:  2 | Batch: 92000 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "6451.108931582421\n",
      "Train Epoch:  2 | Batch: 92400 | Loss: 16.13\n",
      "tensor([4609.])\n",
      "62.17733004456386\n",
      "Train Epoch:  2 | Batch: 92800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5470.057256851345\n",
      "Train Epoch:  2 | Batch: 93200 | Loss: 13.68\n",
      "tensor([4609.])\n",
      "68.38271672045812\n",
      "Train Epoch:  2 | Batch: 93600 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "1788.4902006639168\n",
      "Train Epoch:  2 | Batch: 94000 | Loss: 4.47\n",
      "tensor([4609.])\n",
      "53.57153213582933\n",
      "Train Epoch:  2 | Batch: 94400 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "3649.0954418564215\n",
      "Train Epoch:  2 | Batch: 94800 | Loss: 9.12\n",
      "tensor([4609.])\n",
      "382.6652356274426\n",
      "Train Epoch:  2 | Batch: 95200 | Loss: 0.96\n",
      "tensor([4609.])\n",
      "1228.5439903843217\n",
      "Train Epoch:  2 | Batch: 95600 | Loss: 3.07\n",
      "tensor([4609.])\n",
      "872.2966328752227\n",
      "Train Epoch:  2 | Batch: 96000 | Loss: 2.18\n",
      "tensor([4609.])\n",
      "1398.1655430956744\n",
      "Train Epoch:  2 | Batch: 96400 | Loss: 3.50\n",
      "tensor([4609.])\n",
      "3230.769870935008\n",
      "Train Epoch:  2 | Batch: 96800 | Loss: 8.08\n",
      "tensor([4609.])\n",
      "503.38391447416507\n",
      "Train Epoch:  2 | Batch: 97200 | Loss: 1.26\n",
      "tensor([4609.])\n",
      "1909.2456069556065\n",
      "Train Epoch:  2 | Batch: 97600 | Loss: 4.77\n",
      "tensor([4609.])\n",
      "76.88378550834022\n",
      "Train Epoch:  2 | Batch: 98000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "1593.9513861555606\n",
      "Train Epoch:  2 | Batch: 98400 | Loss: 3.98\n",
      "tensor([4609.])\n",
      "29.747422893997282\n",
      "Train Epoch:  2 | Batch: 98800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3063.806864542421\n",
      "Train Epoch:  2 | Batch: 99200 | Loss: 7.66\n",
      "tensor([4609.])\n",
      "32.49423016235232\n",
      "Train Epoch:  2 | Batch: 99600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2178.520644918084\n",
      "Train Epoch:  2 | Batch: 100000 | Loss: 5.45\n",
      "tensor([4609.])\n",
      "15.416608580388129\n",
      "Train Epoch:  2 | Batch: 100400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2167.439510446973\n",
      "Train Epoch:  2 | Batch: 100800 | Loss: 5.42\n",
      "tensor([4609.])\n",
      "12.536366823595017\n",
      "Train Epoch:  2 | Batch: 101200 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "1981.976494901348\n",
      "Train Epoch:  2 | Batch: 101600 | Loss: 4.95\n",
      "tensor([4609.])\n",
      "19.268440522486344\n",
      "Train Epoch:  2 | Batch: 102000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2909.427948128432\n",
      "Train Epoch:  2 | Batch: 102400 | Loss: 7.27\n",
      "tensor([4609.])\n",
      "147.17943819588982\n",
      "Train Epoch:  2 | Batch: 102800 | Loss: 0.37\n",
      "35\n",
      "\n",
      "Valid Epoch:  2 | Loss: 1.32\n",
      "tensor([4609.])\n",
      "6424.192314952146\n",
      "Train Epoch:  3 | Batch:  400 | Loss: 16.06\n",
      "tensor([4609.])\n",
      "66.82499129883945\n",
      "Train Epoch:  3 | Batch:  800 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "4455.556578317657\n",
      "Train Epoch:  3 | Batch: 1200 | Loss: 11.14\n",
      "tensor([4609.])\n",
      "28.464545846683905\n",
      "Train Epoch:  3 | Batch: 1600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3107.355937551707\n",
      "Train Epoch:  3 | Batch: 2000 | Loss: 7.77\n",
      "tensor([4609.])\n",
      "14.704527746187523\n",
      "Train Epoch:  3 | Batch: 2400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1402.4801333332434\n",
      "Train Epoch:  3 | Batch: 2800 | Loss: 3.51\n",
      "tensor([4609.])\n",
      "18.323666408192366\n",
      "Train Epoch:  3 | Batch: 3200 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3187.8655941919424\n",
      "Train Epoch:  3 | Batch: 3600 | Loss: 7.97\n",
      "tensor([4609.])\n",
      "278.2336790058762\n",
      "Train Epoch:  3 | Batch: 4000 | Loss: 0.70\n",
      "tensor([4609.])\n",
      "2175.95053372602\n",
      "Train Epoch:  3 | Batch: 4400 | Loss: 5.44\n",
      "tensor([4609.])\n",
      "210.3186762263067\n",
      "Train Epoch:  3 | Batch: 4800 | Loss: 0.53\n",
      "tensor([4609.])\n",
      "1956.7065757487435\n",
      "Train Epoch:  3 | Batch: 5200 | Loss: 4.89\n",
      "tensor([4609.])\n",
      "67168.24868232757\n",
      "Train Epoch:  3 | Batch: 5600 | Loss: 167.92\n",
      "tensor([4609.])\n",
      "1146.931698255008\n",
      "Train Epoch:  3 | Batch: 6000 | Loss: 2.87\n",
      "tensor([4609.])\n",
      "1911.4163882876746\n",
      "Train Epoch:  3 | Batch: 6400 | Loss: 4.78\n",
      "tensor([4609.])\n",
      "493.6025697775185\n",
      "Train Epoch:  3 | Batch: 6800 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "2027.735857867403\n",
      "Train Epoch:  3 | Batch: 7200 | Loss: 5.07\n",
      "tensor([4609.])\n",
      "196.91573201259598\n",
      "Train Epoch:  3 | Batch: 7600 | Loss: 0.49\n",
      "tensor([4609.])\n",
      "2795.2408569222316\n",
      "Train Epoch:  3 | Batch: 8000 | Loss: 6.99\n",
      "tensor([4609.])\n",
      "48.55714798439294\n",
      "Train Epoch:  3 | Batch: 8400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "1932.1315937926993\n",
      "Train Epoch:  3 | Batch: 8800 | Loss: 4.83\n",
      "tensor([4609.])\n",
      "52.49694372853264\n",
      "Train Epoch:  3 | Batch: 9200 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "6881.08462267369\n",
      "Train Epoch:  3 | Batch: 9600 | Loss: 17.20\n",
      "tensor([4609.])\n",
      "44.74531581159681\n",
      "Train Epoch:  3 | Batch: 10000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "5642.791996195912\n",
      "Train Epoch:  3 | Batch: 10400 | Loss: 14.11\n",
      "tensor([4609.])\n",
      "63.34195846505463\n",
      "Train Epoch:  3 | Batch: 10800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5680.537469573319\n",
      "Train Epoch:  3 | Batch: 11200 | Loss: 14.20\n",
      "tensor([4609.])\n",
      "85.75742453709245\n",
      "Train Epoch:  3 | Batch: 11600 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "6516.489518232644\n",
      "Train Epoch:  3 | Batch: 12000 | Loss: 16.29\n",
      "tensor([4609.])\n",
      "267.04919589683414\n",
      "Train Epoch:  3 | Batch: 12400 | Loss: 0.67\n",
      "tensor([4609.])\n",
      "4366.164849861525\n",
      "Train Epoch:  3 | Batch: 12800 | Loss: 10.92\n",
      "tensor([4609.])\n",
      "987.6616855487227\n",
      "Train Epoch:  3 | Batch: 13200 | Loss: 2.47\n",
      "tensor([4609.])\n",
      "653.4567089807242\n",
      "Train Epoch:  3 | Batch: 13600 | Loss: 1.63\n",
      "tensor([4609.])\n",
      "733.1764925909229\n",
      "Train Epoch:  3 | Batch: 14000 | Loss: 1.83\n",
      "tensor([4609.])\n",
      "1253.1608359692618\n",
      "Train Epoch:  3 | Batch: 14400 | Loss: 3.13\n",
      "tensor([4609.])\n",
      "4009.769612118602\n",
      "Train Epoch:  3 | Batch: 14800 | Loss: 10.02\n",
      "tensor([4609.])\n",
      "195.08723671594635\n",
      "Train Epoch:  3 | Batch: 15200 | Loss: 0.49\n",
      "tensor([4609.])\n",
      "3773.330054647289\n",
      "Train Epoch:  3 | Batch: 15600 | Loss: 9.43\n",
      "tensor([4609.])\n",
      "74.23860885202885\n",
      "Train Epoch:  3 | Batch: 16000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3226.816677261144\n",
      "Train Epoch:  3 | Batch: 16400 | Loss: 8.07\n",
      "tensor([4609.])\n",
      "47.14914580248296\n",
      "Train Epoch:  3 | Batch: 16800 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2702.7537078065798\n",
      "Train Epoch:  3 | Batch: 17200 | Loss: 6.76\n",
      "tensor([4609.])\n",
      "35.197306108428165\n",
      "Train Epoch:  3 | Batch: 17600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3040.3757801651955\n",
      "Train Epoch:  3 | Batch: 18000 | Loss: 7.60\n",
      "tensor([4609.])\n",
      "26.53338628052734\n",
      "Train Epoch:  3 | Batch: 18400 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2758.8358167400584\n",
      "Train Epoch:  3 | Batch: 18800 | Loss: 6.90\n",
      "tensor([4609.])\n",
      "18.394571671262383\n",
      "Train Epoch:  3 | Batch: 19200 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2723.1313351597637\n",
      "Train Epoch:  3 | Batch: 19600 | Loss: 6.81\n",
      "tensor([4609.])\n",
      "28.206993397092447\n",
      "Train Epoch:  3 | Batch: 20000 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "1331.5011993218213\n",
      "Train Epoch:  3 | Batch: 20400 | Loss: 3.33\n",
      "tensor([4609.])\n",
      "50.18040004000068\n",
      "Train Epoch:  3 | Batch: 20800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "2042.4570192978717\n",
      "Train Epoch:  3 | Batch: 21200 | Loss: 5.11\n",
      "tensor([4609.])\n",
      "687.2502947952598\n",
      "Train Epoch:  3 | Batch: 21600 | Loss: 1.72\n",
      "tensor([4609.])\n",
      "2034.912995801773\n",
      "Train Epoch:  3 | Batch: 22000 | Loss: 5.09\n",
      "tensor([4609.])\n",
      "784.4681567647494\n",
      "Train Epoch:  3 | Batch: 22400 | Loss: 1.96\n",
      "tensor([4609.])\n",
      "873.358446999453\n",
      "Train Epoch:  3 | Batch: 22800 | Loss: 2.18\n",
      "tensor([4609.])\n",
      "1855.9204305754974\n",
      "Train Epoch:  3 | Batch: 23200 | Loss: 4.64\n",
      "tensor([4609.])\n",
      "470.51843616506085\n",
      "Train Epoch:  3 | Batch: 23600 | Loss: 1.18\n",
      "tensor([4609.])\n",
      "2723.4122000858188\n",
      "Train Epoch:  3 | Batch: 24000 | Loss: 6.81\n",
      "tensor([4609.])\n",
      "233.4972072043456\n",
      "Train Epoch:  3 | Batch: 24400 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "3469.0597618008032\n",
      "Train Epoch:  3 | Batch: 24800 | Loss: 8.67\n",
      "tensor([4609.])\n",
      "85222142247.23227\n",
      "Train Epoch:  3 | Batch: 25200 | Loss: 213055355.62\n",
      "tensor([4609.])\n",
      "2043084.25390625\n",
      "Train Epoch:  3 | Batch: 25600 | Loss: 5107.71\n",
      "tensor([4609.])\n",
      "413745.45864868164\n",
      "Train Epoch:  3 | Batch: 26000 | Loss: 1034.36\n",
      "tensor([4609.])\n",
      "27469.589462280273\n",
      "Train Epoch:  3 | Batch: 26400 | Loss: 68.67\n",
      "tensor([4609.])\n",
      "3867.4057772159576\n",
      "Train Epoch:  3 | Batch: 26800 | Loss: 9.67\n",
      "tensor([4609.])\n",
      "8729.56705302\n",
      "Train Epoch:  3 | Batch: 27200 | Loss: 21.82\n",
      "tensor([4609.])\n",
      "1107.2666098047048\n",
      "Train Epoch:  3 | Batch: 27600 | Loss: 2.77\n",
      "tensor([4609.])\n",
      "5027.965430594981\n",
      "Train Epoch:  3 | Batch: 28000 | Loss: 12.57\n",
      "tensor([4609.])\n",
      "172.03095882106572\n",
      "Train Epoch:  3 | Batch: 28400 | Loss: 0.43\n",
      "tensor([4609.])\n",
      "3086.6655170386657\n",
      "Train Epoch:  3 | Batch: 28800 | Loss: 7.72\n",
      "tensor([4609.])\n",
      "150.11108002671972\n",
      "Train Epoch:  3 | Batch: 29200 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "4181.865783842746\n",
      "Train Epoch:  3 | Batch: 29600 | Loss: 10.45\n",
      "tensor([4609.])\n",
      "548.7993748621084\n",
      "Train Epoch:  3 | Batch: 30000 | Loss: 1.37\n",
      "tensor([4609.])\n",
      "2412.9478262378834\n",
      "Train Epoch:  3 | Batch: 30400 | Loss: 6.03\n",
      "tensor([4609.])\n",
      "1541.2096549789421\n",
      "Train Epoch:  3 | Batch: 30800 | Loss: 3.85\n",
      "tensor([4609.])\n",
      "1034.776710537728\n",
      "Train Epoch:  3 | Batch: 31200 | Loss: 2.59\n",
      "tensor([4609.])\n",
      "2016.0705159332138\n",
      "Train Epoch:  3 | Batch: 31600 | Loss: 5.04\n",
      "tensor([4609.])\n",
      "231.27126361057162\n",
      "Train Epoch:  3 | Batch: 32000 | Loss: 0.58\n",
      "tensor([4609.])\n",
      "1254.4019587195944\n",
      "Train Epoch:  3 | Batch: 32400 | Loss: 3.14\n",
      "tensor([4609.])\n",
      "116.37569758796599\n",
      "Train Epoch:  3 | Batch: 32800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3793.74313063547\n",
      "Train Epoch:  3 | Batch: 33200 | Loss: 9.48\n",
      "tensor([4609.])\n",
      "40.752563000889495\n",
      "Train Epoch:  3 | Batch: 33600 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "3639.6039640638046\n",
      "Train Epoch:  3 | Batch: 34000 | Loss: 9.10\n",
      "tensor([4609.])\n",
      "68.56352282548323\n",
      "Train Epoch:  3 | Batch: 34400 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "5064.280308354646\n",
      "Train Epoch:  3 | Batch: 34800 | Loss: 12.66\n",
      "tensor([4609.])\n",
      "26.943380148382857\n",
      "Train Epoch:  3 | Batch: 35200 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3007.6374627854675\n",
      "Train Epoch:  3 | Batch: 35600 | Loss: 7.52\n",
      "tensor([4609.])\n",
      "15.415169259300455\n",
      "Train Epoch:  3 | Batch: 36000 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2120.00489205122\n",
      "Train Epoch:  3 | Batch: 36400 | Loss: 5.30\n",
      "tensor([4609.])\n",
      "25.187997978413478\n",
      "Train Epoch:  3 | Batch: 36800 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "3417.060539650498\n",
      "Train Epoch:  3 | Batch: 37200 | Loss: 8.54\n",
      "tensor([4609.])\n",
      "91.28931097174063\n",
      "Train Epoch:  3 | Batch: 37600 | Loss: 0.23\n",
      "tensor([4609.])\n",
      "1309.0650675708894\n",
      "Train Epoch:  3 | Batch: 38000 | Loss: 3.27\n",
      "tensor([4609.])\n",
      "261.11533252336085\n",
      "Train Epoch:  3 | Batch: 38400 | Loss: 0.65\n",
      "tensor([4609.])\n",
      "1679.5856401128694\n",
      "Train Epoch:  3 | Batch: 38800 | Loss: 4.20\n",
      "tensor([4609.])\n",
      "1445.3868583682925\n",
      "Train Epoch:  3 | Batch: 39200 | Loss: 3.61\n",
      "tensor([4609.])\n",
      "899.2254259374458\n",
      "Train Epoch:  3 | Batch: 39600 | Loss: 2.25\n",
      "tensor([4609.])\n",
      "1549.6130368188024\n",
      "Train Epoch:  3 | Batch: 40000 | Loss: 3.87\n",
      "tensor([4609.])\n",
      "427.91217513452284\n",
      "Train Epoch:  3 | Batch: 40400 | Loss: 1.07\n",
      "tensor([4609.])\n",
      "2393.623806497548\n",
      "Train Epoch:  3 | Batch: 40800 | Loss: 5.98\n",
      "tensor([4609.])\n",
      "252.29708533501253\n",
      "Train Epoch:  3 | Batch: 41200 | Loss: 0.63\n",
      "tensor([4609.])\n",
      "3013.4707388700917\n",
      "Train Epoch:  3 | Batch: 41600 | Loss: 7.53\n",
      "tensor([4609.])\n",
      "88.02823277236894\n",
      "Train Epoch:  3 | Batch: 42000 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "2799.033701086417\n",
      "Train Epoch:  3 | Batch: 42400 | Loss: 7.00\n",
      "tensor([4609.])\n",
      "29.233862032648176\n",
      "Train Epoch:  3 | Batch: 42800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2267.1882310807705\n",
      "Train Epoch:  3 | Batch: 43200 | Loss: 5.67\n",
      "tensor([4609.])\n",
      "14.807058821199462\n",
      "Train Epoch:  3 | Batch: 43600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1602.20840543136\n",
      "Train Epoch:  3 | Batch: 44000 | Loss: 4.01\n",
      "tensor([4609.])\n",
      "5208.529205533443\n",
      "Train Epoch:  3 | Batch: 44400 | Loss: 13.02\n",
      "tensor([4609.])\n",
      "116.83346816897392\n",
      "Train Epoch:  3 | Batch: 44800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3142.3070380119607\n",
      "Train Epoch:  3 | Batch: 45200 | Loss: 7.86\n",
      "tensor([4609.])\n",
      "115.9364576684311\n",
      "Train Epoch:  3 | Batch: 45600 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "2776.301568471361\n",
      "Train Epoch:  3 | Batch: 46000 | Loss: 6.94\n",
      "tensor([4609.])\n",
      "632.2960450691171\n",
      "Train Epoch:  3 | Batch: 46400 | Loss: 1.58\n",
      "tensor([4609.])\n",
      "2249.460801383946\n",
      "Train Epoch:  3 | Batch: 46800 | Loss: 5.62\n",
      "tensor([4609.])\n",
      "1570.0474543590099\n",
      "Train Epoch:  3 | Batch: 47200 | Loss: 3.93\n",
      "tensor([4609.])\n",
      "1118.5291240550578\n",
      "Train Epoch:  3 | Batch: 47600 | Loss: 2.80\n",
      "tensor([4609.])\n",
      "2596.350675802678\n",
      "Train Epoch:  3 | Batch: 48000 | Loss: 6.49\n",
      "tensor([4609.])\n",
      "443.2092533605173\n",
      "Train Epoch:  3 | Batch: 48400 | Loss: 1.11\n",
      "tensor([4609.])\n",
      "2954.492846540641\n",
      "Train Epoch:  3 | Batch: 48800 | Loss: 7.39\n",
      "tensor([4609.])\n",
      "96.7018931854982\n",
      "Train Epoch:  3 | Batch: 49200 | Loss: 0.24\n",
      "tensor([4609.])\n",
      "1688.891888053622\n",
      "Train Epoch:  3 | Batch: 49600 | Loss: 4.22\n",
      "tensor([4609.])\n",
      "73.70738050318323\n",
      "Train Epoch:  3 | Batch: 50000 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "4352.095019828528\n",
      "Train Epoch:  3 | Batch: 50400 | Loss: 10.88\n",
      "tensor([4609.])\n",
      "27.701743389014155\n",
      "Train Epoch:  3 | Batch: 50800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2770.9424412176013\n",
      "Train Epoch:  3 | Batch: 51200 | Loss: 6.93\n",
      "tensor([4609.])\n",
      "27.57592989015393\n",
      "Train Epoch:  3 | Batch: 51600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3955.1491605341434\n",
      "Train Epoch:  3 | Batch: 52000 | Loss: 9.89\n",
      "tensor([4609.])\n",
      "36.40459737973288\n",
      "Train Epoch:  3 | Batch: 52400 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3555.780366484076\n",
      "Train Epoch:  3 | Batch: 52800 | Loss: 8.89\n",
      "tensor([4609.])\n",
      "47.57688175095245\n",
      "Train Epoch:  3 | Batch: 53200 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "3367.805859327782\n",
      "Train Epoch:  3 | Batch: 53600 | Loss: 8.42\n",
      "tensor([4609.])\n",
      "145.71089862240478\n",
      "Train Epoch:  3 | Batch: 54000 | Loss: 0.36\n",
      "tensor([4609.])\n",
      "2326.771863164846\n",
      "Train Epoch:  3 | Batch: 54400 | Loss: 5.82\n",
      "tensor([4609.])\n",
      "465.12092496082187\n",
      "Train Epoch:  3 | Batch: 54800 | Loss: 1.16\n",
      "tensor([4609.])\n",
      "963.6761017085519\n",
      "Train Epoch:  3 | Batch: 55200 | Loss: 2.41\n",
      "tensor([4609.])\n",
      "627.7457485867199\n",
      "Train Epoch:  3 | Batch: 55600 | Loss: 1.57\n",
      "tensor([4609.])\n",
      "843.411559375003\n",
      "Train Epoch:  3 | Batch: 56000 | Loss: 2.11\n",
      "tensor([4609.])\n",
      "3216.685257703066\n",
      "Train Epoch:  3 | Batch: 56400 | Loss: 8.04\n",
      "tensor([4609.])\n",
      "315.2671715654433\n",
      "Train Epoch:  3 | Batch: 56800 | Loss: 0.79\n",
      "tensor([4609.])\n",
      "3521.6114796875045\n",
      "Train Epoch:  3 | Batch: 57200 | Loss: 8.80\n",
      "tensor([4609.])\n",
      "217.85746056493372\n",
      "Train Epoch:  3 | Batch: 57600 | Loss: 0.54\n",
      "tensor([4609.])\n",
      "3724.264631166123\n",
      "Train Epoch:  3 | Batch: 58000 | Loss: 9.31\n",
      "tensor([4609.])\n",
      "75.71728194924071\n",
      "Train Epoch:  3 | Batch: 58400 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3877.7012757156044\n",
      "Train Epoch:  3 | Batch: 58800 | Loss: 9.69\n",
      "tensor([4609.])\n",
      "34.08353670570068\n",
      "Train Epoch:  3 | Batch: 59200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3756.208404351026\n",
      "Train Epoch:  3 | Batch: 59600 | Loss: 9.39\n",
      "tensor([4609.])\n",
      "38.84479703428224\n",
      "Train Epoch:  3 | Batch: 60000 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "4687.273588646203\n",
      "Train Epoch:  3 | Batch: 60400 | Loss: 11.72\n",
      "tensor([4609.])\n",
      "18.911606069188565\n",
      "Train Epoch:  3 | Batch: 60800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "1444.303380989004\n",
      "Train Epoch:  3 | Batch: 61200 | Loss: 3.61\n",
      "tensor([4609.])\n",
      "25.123742502881214\n",
      "Train Epoch:  3 | Batch: 61600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2774.0444765593857\n",
      "Train Epoch:  3 | Batch: 62000 | Loss: 6.94\n",
      "tensor([4609.])\n",
      "227.89257703954354\n",
      "Train Epoch:  3 | Batch: 62400 | Loss: 0.57\n",
      "tensor([4609.])\n",
      "1971.4883557430003\n",
      "Train Epoch:  3 | Batch: 62800 | Loss: 4.93\n",
      "tensor([4609.])\n",
      "316.46096717147157\n",
      "Train Epoch:  3 | Batch: 63200 | Loss: 0.79\n",
      "tensor([4609.])\n",
      "1427.77614980191\n",
      "Train Epoch:  3 | Batch: 63600 | Loss: 3.57\n",
      "tensor([4609.])\n",
      "1132.9643116188236\n",
      "Train Epoch:  3 | Batch: 64000 | Loss: 2.83\n",
      "tensor([4609.])\n",
      "737.6191201135516\n",
      "Train Epoch:  3 | Batch: 64400 | Loss: 1.84\n",
      "tensor([4609.])\n",
      "2132.704207559582\n",
      "Train Epoch:  3 | Batch: 64800 | Loss: 5.33\n",
      "tensor([4609.])\n",
      "340.91069187410176\n",
      "Train Epoch:  3 | Batch: 65200 | Loss: 0.85\n",
      "tensor([4609.])\n",
      "2184.600349567365\n",
      "Train Epoch:  3 | Batch: 65600 | Loss: 5.46\n",
      "tensor([4609.])\n",
      "211.15468040900305\n",
      "Train Epoch:  3 | Batch: 66000 | Loss: 0.53\n",
      "tensor([4609.])\n",
      "4214.912299286574\n",
      "Train Epoch:  3 | Batch: 66400 | Loss: 10.54\n",
      "tensor([4609.])\n",
      "40.28454729402438\n",
      "Train Epoch:  3 | Batch: 66800 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2120.6664262656122\n",
      "Train Epoch:  3 | Batch: 67200 | Loss: 5.30\n",
      "tensor([4609.])\n",
      "19.58677733433433\n",
      "Train Epoch:  3 | Batch: 67600 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3648.8319342322648\n",
      "Train Epoch:  3 | Batch: 68000 | Loss: 9.12\n",
      "tensor([4609.])\n",
      "25.295812781434506\n",
      "Train Epoch:  3 | Batch: 68400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "9404.342362047173\n",
      "Train Epoch:  3 | Batch: 68800 | Loss: 23.51\n",
      "tensor([4609.])\n",
      "78.29724264098331\n",
      "Train Epoch:  3 | Batch: 69200 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "2477.7031172025017\n",
      "Train Epoch:  3 | Batch: 69600 | Loss: 6.19\n",
      "tensor([4609.])\n",
      "403.0435405620374\n",
      "Train Epoch:  3 | Batch: 70000 | Loss: 1.01\n",
      "tensor([4609.])\n",
      "2062.3403363046236\n",
      "Train Epoch:  3 | Batch: 70400 | Loss: 5.16\n",
      "tensor([4609.])\n",
      "903.7764459550381\n",
      "Train Epoch:  3 | Batch: 70800 | Loss: 2.26\n",
      "tensor([4609.])\n",
      "993.5284834050108\n",
      "Train Epoch:  3 | Batch: 71200 | Loss: 2.48\n",
      "tensor([4609.])\n",
      "1628.457711327821\n",
      "Train Epoch:  3 | Batch: 71600 | Loss: 4.07\n",
      "tensor([4609.])\n",
      "227.14610740239732\n",
      "Train Epoch:  3 | Batch: 72000 | Loss: 0.57\n",
      "tensor([4609.])\n",
      "1101.2158464740496\n",
      "Train Epoch:  3 | Batch: 72400 | Loss: 2.75\n",
      "tensor([4609.])\n",
      "117.34683336713351\n",
      "Train Epoch:  3 | Batch: 72800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3249.519863560796\n",
      "Train Epoch:  3 | Batch: 73200 | Loss: 8.12\n",
      "tensor([4609.])\n",
      "31.600595235358924\n",
      "Train Epoch:  3 | Batch: 73600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2406.442381352186\n",
      "Train Epoch:  3 | Batch: 74000 | Loss: 6.02\n",
      "tensor([4609.])\n",
      "41.14047054760158\n",
      "Train Epoch:  3 | Batch: 74400 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2482.5954346498474\n",
      "Train Epoch:  3 | Batch: 74800 | Loss: 6.21\n",
      "tensor([4609.])\n",
      "25.453525435179472\n",
      "Train Epoch:  3 | Batch: 75200 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2604.631607274525\n",
      "Train Epoch:  3 | Batch: 75600 | Loss: 6.51\n",
      "tensor([4609.])\n",
      "21.320705842226744\n",
      "Train Epoch:  3 | Batch: 76000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2642.4264409951866\n",
      "Train Epoch:  3 | Batch: 76400 | Loss: 6.61\n",
      "tensor([4609.])\n",
      "26.51956441393122\n",
      "Train Epoch:  3 | Batch: 76800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2485.2997461035848\n",
      "Train Epoch:  3 | Batch: 77200 | Loss: 6.21\n",
      "tensor([4609.])\n",
      "63.253831265028566\n",
      "Train Epoch:  3 | Batch: 77600 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "1049.8042918886058\n",
      "Train Epoch:  3 | Batch: 78000 | Loss: 2.62\n",
      "tensor([4609.])\n",
      "179.65404619136825\n",
      "Train Epoch:  3 | Batch: 78400 | Loss: 0.45\n",
      "tensor([4609.])\n",
      "2352.905896055978\n",
      "Train Epoch:  3 | Batch: 78800 | Loss: 5.88\n",
      "tensor([4609.])\n",
      "2115.130755588878\n",
      "Train Epoch:  3 | Batch: 79200 | Loss: 5.29\n",
      "tensor([4609.])\n",
      "1147.2745426790789\n",
      "Train Epoch:  3 | Batch: 79600 | Loss: 2.87\n",
      "tensor([4609.])\n",
      "2106.7246251692995\n",
      "Train Epoch:  3 | Batch: 80000 | Loss: 5.27\n",
      "tensor([4609.])\n",
      "450.13760565640405\n",
      "Train Epoch:  3 | Batch: 80400 | Loss: 1.13\n",
      "tensor([4609.])\n",
      "1914.6531848548912\n",
      "Train Epoch:  3 | Batch: 80800 | Loss: 4.79\n",
      "tensor([4609.])\n",
      "88.3861364130862\n",
      "Train Epoch:  3 | Batch: 81200 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "1211.3157027848065\n",
      "Train Epoch:  3 | Batch: 81600 | Loss: 3.03\n",
      "tensor([4609.])\n",
      "43.26252984581515\n",
      "Train Epoch:  3 | Batch: 82000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "2064.4054897502065\n",
      "Train Epoch:  3 | Batch: 82400 | Loss: 5.16\n",
      "tensor([4609.])\n",
      "33.52260688645765\n",
      "Train Epoch:  3 | Batch: 82800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2458.843507242389\n",
      "Train Epoch:  3 | Batch: 83200 | Loss: 6.15\n",
      "tensor([4609.])\n",
      "15.570014574564993\n",
      "Train Epoch:  3 | Batch: 83600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1710.094495864585\n",
      "Train Epoch:  3 | Batch: 84000 | Loss: 4.28\n",
      "tensor([4609.])\n",
      "13.473083293065429\n",
      "Train Epoch:  3 | Batch: 84400 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2825.7070908974856\n",
      "Train Epoch:  3 | Batch: 84800 | Loss: 7.06\n",
      "tensor([4609.])\n",
      "55.01507210871205\n",
      "Train Epoch:  3 | Batch: 85200 | Loss: 0.14\n",
      "tensor([4609.])\n",
      "3397.4753525266424\n",
      "Train Epoch:  3 | Batch: 85600 | Loss: 8.49\n",
      "tensor([4609.])\n",
      "37.675274229608476\n",
      "Train Epoch:  3 | Batch: 86000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2776.6128875543363\n",
      "Train Epoch:  3 | Batch: 86400 | Loss: 6.94\n",
      "tensor([4609.])\n",
      "405.63923876779154\n",
      "Train Epoch:  3 | Batch: 86800 | Loss: 1.01\n",
      "tensor([4609.])\n",
      "2214.45263516251\n",
      "Train Epoch:  3 | Batch: 87200 | Loss: 5.54\n",
      "tensor([4609.])\n",
      "1506.5661863218993\n",
      "Train Epoch:  3 | Batch: 87600 | Loss: 3.77\n",
      "tensor([4609.])\n",
      "1421.267554318998\n",
      "Train Epoch:  3 | Batch: 88000 | Loss: 3.55\n",
      "tensor([4609.])\n",
      "2118.645287553314\n",
      "Train Epoch:  3 | Batch: 88400 | Loss: 5.30\n",
      "tensor([4609.])\n",
      "791.5674318042584\n",
      "Train Epoch:  3 | Batch: 88800 | Loss: 1.98\n",
      "tensor([4609.])\n",
      "3470.243176527787\n",
      "Train Epoch:  3 | Batch: 89200 | Loss: 8.68\n",
      "tensor([4609.])\n",
      "151.58722001337446\n",
      "Train Epoch:  3 | Batch: 89600 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "2628.483347444795\n",
      "Train Epoch:  3 | Batch: 90000 | Loss: 6.57\n",
      "tensor([4609.])\n",
      "99.93661880493164\n",
      "Train Epoch:  3 | Batch: 90400 | Loss: 0.25\n",
      "tensor([4609.])\n",
      "5584.067686440423\n",
      "Train Epoch:  3 | Batch: 90800 | Loss: 13.96\n",
      "tensor([4609.])\n",
      "37.52233485225588\n",
      "Train Epoch:  3 | Batch: 91200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "5172.970184038393\n",
      "Train Epoch:  3 | Batch: 91600 | Loss: 12.93\n",
      "tensor([4609.])\n",
      "79.3593613919802\n",
      "Train Epoch:  3 | Batch: 92000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "6502.760910300538\n",
      "Train Epoch:  3 | Batch: 92400 | Loss: 16.26\n",
      "tensor([4609.])\n",
      "61.97184071969241\n",
      "Train Epoch:  3 | Batch: 92800 | Loss: 0.15\n",
      "tensor([4609.])\n",
      "5520.8242930956185\n",
      "Train Epoch:  3 | Batch: 93200 | Loss: 13.80\n",
      "tensor([4609.])\n",
      "66.6485890103504\n",
      "Train Epoch:  3 | Batch: 93600 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "1862.1191550560761\n",
      "Train Epoch:  3 | Batch: 94000 | Loss: 4.66\n",
      "tensor([4609.])\n",
      "72.43063398054801\n",
      "Train Epoch:  3 | Batch: 94400 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "3566.7158395163715\n",
      "Train Epoch:  3 | Batch: 94800 | Loss: 8.92\n",
      "tensor([4609.])\n",
      "388.0340960044414\n",
      "Train Epoch:  3 | Batch: 95200 | Loss: 0.97\n",
      "tensor([4609.])\n",
      "1246.341887960676\n",
      "Train Epoch:  3 | Batch: 95600 | Loss: 3.12\n",
      "tensor([4609.])\n",
      "878.4038361101411\n",
      "Train Epoch:  3 | Batch: 96000 | Loss: 2.20\n",
      "tensor([4609.])\n",
      "1153.0032351943664\n",
      "Train Epoch:  3 | Batch: 96400 | Loss: 2.88\n",
      "tensor([4609.])\n",
      "3187.3174208397977\n",
      "Train Epoch:  3 | Batch: 96800 | Loss: 7.97\n",
      "tensor([4609.])\n",
      "489.8929895721376\n",
      "Train Epoch:  3 | Batch: 97200 | Loss: 1.22\n",
      "tensor([4609.])\n",
      "1847.3060020869598\n",
      "Train Epoch:  3 | Batch: 97600 | Loss: 4.62\n",
      "tensor([4609.])\n",
      "78.94493455579504\n",
      "Train Epoch:  3 | Batch: 98000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "1584.5984244109131\n",
      "Train Epoch:  3 | Batch: 98400 | Loss: 3.96\n",
      "tensor([4609.])\n",
      "31.132006688509136\n",
      "Train Epoch:  3 | Batch: 98800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2914.0675786379725\n",
      "Train Epoch:  3 | Batch: 99200 | Loss: 7.29\n",
      "tensor([4609.])\n",
      "33.77809220645577\n",
      "Train Epoch:  3 | Batch: 99600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2208.9185114409775\n",
      "Train Epoch:  3 | Batch: 100000 | Loss: 5.52\n",
      "tensor([4609.])\n",
      "17.10923593165353\n",
      "Train Epoch:  3 | Batch: 100400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2144.47560845688\n",
      "Train Epoch:  3 | Batch: 100800 | Loss: 5.36\n",
      "tensor([4609.])\n",
      "13.63422375661321\n",
      "Train Epoch:  3 | Batch: 101200 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2025.012719889637\n",
      "Train Epoch:  3 | Batch: 101600 | Loss: 5.06\n",
      "tensor([4609.])\n",
      "19.875592829892412\n",
      "Train Epoch:  3 | Batch: 102000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2692.061397112906\n",
      "Train Epoch:  3 | Batch: 102400 | Loss: 6.73\n",
      "tensor([4609.])\n",
      "119.77544081094675\n",
      "Train Epoch:  3 | Batch: 102800 | Loss: 0.30\n",
      "35\n",
      "\n",
      "Valid Epoch:  3 | Loss: 1.03\n",
      "model saved\n",
      "\n",
      "tensor([4609.])\n",
      "6225.9168281483\n",
      "Train Epoch:  4 | Batch:  400 | Loss: 15.56\n",
      "tensor([4609.])\n",
      "65.50434361444786\n",
      "Train Epoch:  4 | Batch:  800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "4416.174138335511\n",
      "Train Epoch:  4 | Batch: 1200 | Loss: 11.04\n",
      "tensor([4609.])\n",
      "30.658046549651772\n",
      "Train Epoch:  4 | Batch: 1600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "3104.4109791219234\n",
      "Train Epoch:  4 | Batch: 2000 | Loss: 7.76\n",
      "tensor([4609.])\n",
      "15.104454455664381\n",
      "Train Epoch:  4 | Batch: 2400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1402.2529091648757\n",
      "Train Epoch:  4 | Batch: 2800 | Loss: 3.51\n",
      "tensor([4609.])\n",
      "17.477847976377234\n",
      "Train Epoch:  4 | Batch: 3200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "3084.781981307082\n",
      "Train Epoch:  4 | Batch: 3600 | Loss: 7.71\n",
      "tensor([4609.])\n",
      "239.58855369105004\n",
      "Train Epoch:  4 | Batch: 4000 | Loss: 0.60\n",
      "tensor([4609.])\n",
      "2226.0520284565864\n",
      "Train Epoch:  4 | Batch: 4400 | Loss: 5.57\n",
      "tensor([4609.])\n",
      "199.27669574972242\n",
      "Train Epoch:  4 | Batch: 4800 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "1923.3764052456245\n",
      "Train Epoch:  4 | Batch: 5200 | Loss: 4.81\n",
      "tensor([4609.])\n",
      "67031.7771191732\n",
      "Train Epoch:  4 | Batch: 5600 | Loss: 167.58\n",
      "tensor([4609.])\n",
      "1221.2401404890697\n",
      "Train Epoch:  4 | Batch: 6000 | Loss: 3.05\n",
      "tensor([4609.])\n",
      "1940.5032775392756\n",
      "Train Epoch:  4 | Batch: 6400 | Loss: 4.85\n",
      "tensor([4609.])\n",
      "491.4519520648755\n",
      "Train Epoch:  4 | Batch: 6800 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "2014.438057150226\n",
      "Train Epoch:  4 | Batch: 7200 | Loss: 5.04\n",
      "tensor([4609.])\n",
      "198.98880437295884\n",
      "Train Epoch:  4 | Batch: 7600 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "2808.927448800765\n",
      "Train Epoch:  4 | Batch: 8000 | Loss: 7.02\n",
      "tensor([4609.])\n",
      "46.4138257377781\n",
      "Train Epoch:  4 | Batch: 8400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "1951.836705846712\n",
      "Train Epoch:  4 | Batch: 8800 | Loss: 4.88\n",
      "tensor([4609.])\n",
      "51.209402358625084\n",
      "Train Epoch:  4 | Batch: 9200 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "6946.023542560637\n",
      "Train Epoch:  4 | Batch: 9600 | Loss: 17.37\n",
      "tensor([4609.])\n",
      "44.45660782046616\n",
      "Train Epoch:  4 | Batch: 10000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "5571.491890318692\n",
      "Train Epoch:  4 | Batch: 10400 | Loss: 13.93\n",
      "tensor([4609.])\n",
      "62.51639346964657\n",
      "Train Epoch:  4 | Batch: 10800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5623.5113412849605\n",
      "Train Epoch:  4 | Batch: 11200 | Loss: 14.06\n",
      "tensor([4609.])\n",
      "88.10396961215883\n",
      "Train Epoch:  4 | Batch: 11600 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "6514.98336206004\n",
      "Train Epoch:  4 | Batch: 12000 | Loss: 16.29\n",
      "tensor([4609.])\n",
      "269.1185395382345\n",
      "Train Epoch:  4 | Batch: 12400 | Loss: 0.67\n",
      "tensor([4609.])\n",
      "4355.680924342014\n",
      "Train Epoch:  4 | Batch: 12800 | Loss: 10.89\n",
      "tensor([4609.])\n",
      "931.1046062447131\n",
      "Train Epoch:  4 | Batch: 13200 | Loss: 2.33\n",
      "tensor([4609.])\n",
      "631.903800193686\n",
      "Train Epoch:  4 | Batch: 13600 | Loss: 1.58\n",
      "tensor([4609.])\n",
      "715.8448496239726\n",
      "Train Epoch:  4 | Batch: 14000 | Loss: 1.79\n",
      "tensor([4609.])\n",
      "1232.4432396665215\n",
      "Train Epoch:  4 | Batch: 14400 | Loss: 3.08\n",
      "tensor([4609.])\n",
      "3938.4727771519683\n",
      "Train Epoch:  4 | Batch: 14800 | Loss: 9.85\n",
      "tensor([4609.])\n",
      "200.783051549457\n",
      "Train Epoch:  4 | Batch: 15200 | Loss: 0.50\n",
      "tensor([4609.])\n",
      "3807.6440034755506\n",
      "Train Epoch:  4 | Batch: 15600 | Loss: 9.52\n",
      "tensor([4609.])\n",
      "74.78593906061724\n",
      "Train Epoch:  4 | Batch: 16000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "3250.1965564079583\n",
      "Train Epoch:  4 | Batch: 16400 | Loss: 8.13\n",
      "tensor([4609.])\n",
      "49.374118734151125\n",
      "Train Epoch:  4 | Batch: 16800 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2794.490933398716\n",
      "Train Epoch:  4 | Batch: 17200 | Loss: 6.99\n",
      "tensor([4609.])\n",
      "34.64932979992591\n",
      "Train Epoch:  4 | Batch: 17600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2808.7656234069727\n",
      "Train Epoch:  4 | Batch: 18000 | Loss: 7.02\n",
      "tensor([4609.])\n",
      "25.561501054326072\n",
      "Train Epoch:  4 | Batch: 18400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2806.6577262990177\n",
      "Train Epoch:  4 | Batch: 18800 | Loss: 7.02\n",
      "tensor([4609.])\n",
      "17.635494587710127\n",
      "Train Epoch:  4 | Batch: 19200 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2762.6656753905118\n",
      "Train Epoch:  4 | Batch: 19600 | Loss: 6.91\n",
      "tensor([4609.])\n",
      "29.224991470575333\n",
      "Train Epoch:  4 | Batch: 20000 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "1340.2800985819194\n",
      "Train Epoch:  4 | Batch: 20400 | Loss: 3.35\n",
      "tensor([4609.])\n",
      "51.37515042559244\n",
      "Train Epoch:  4 | Batch: 20800 | Loss: 0.13\n",
      "tensor([4609.])\n",
      "2026.4889956065454\n",
      "Train Epoch:  4 | Batch: 21200 | Loss: 5.07\n",
      "tensor([4609.])\n",
      "678.9291195850819\n",
      "Train Epoch:  4 | Batch: 21600 | Loss: 1.70\n",
      "tensor([4609.])\n",
      "2087.9402068941854\n",
      "Train Epoch:  4 | Batch: 22000 | Loss: 5.22\n",
      "tensor([4609.])\n",
      "778.9091812884435\n",
      "Train Epoch:  4 | Batch: 22400 | Loss: 1.95\n",
      "tensor([4609.])\n",
      "870.9248267798685\n",
      "Train Epoch:  4 | Batch: 22800 | Loss: 2.18\n",
      "tensor([4609.])\n",
      "1779.1591342212632\n",
      "Train Epoch:  4 | Batch: 23200 | Loss: 4.45\n",
      "tensor([4609.])\n",
      "463.6893562916666\n",
      "Train Epoch:  4 | Batch: 23600 | Loss: 1.16\n",
      "tensor([4609.])\n",
      "2719.1408677101135\n",
      "Train Epoch:  4 | Batch: 24000 | Loss: 6.80\n",
      "tensor([4609.])\n",
      "236.3858122434467\n",
      "Train Epoch:  4 | Batch: 24400 | Loss: 0.59\n",
      "tensor([4609.])\n",
      "3453.7593927960843\n",
      "Train Epoch:  4 | Batch: 24800 | Loss: 8.63\n",
      "tensor([4609.])\n",
      "85227841918.54073\n",
      "Train Epoch:  4 | Batch: 25200 | Loss: 213069604.80\n",
      "tensor([4609.])\n",
      "549153.4583740234\n",
      "Train Epoch:  4 | Batch: 25600 | Loss: 1372.88\n",
      "tensor([4609.])\n",
      "10120.023787878454\n",
      "Train Epoch:  4 | Batch: 26000 | Loss: 25.30\n",
      "tensor([4609.])\n",
      "1724.8816760461777\n",
      "Train Epoch:  4 | Batch: 26400 | Loss: 4.31\n",
      "tensor([4609.])\n",
      "18.725799407809973\n",
      "Train Epoch:  4 | Batch: 26800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "4767.444191928953\n",
      "Train Epoch:  4 | Batch: 27200 | Loss: 11.92\n",
      "tensor([4609.])\n",
      "35.50653642369434\n",
      "Train Epoch:  4 | Batch: 27600 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3329.8641065219417\n",
      "Train Epoch:  4 | Batch: 28000 | Loss: 8.32\n",
      "tensor([4609.])\n",
      "17.197417938616127\n",
      "Train Epoch:  4 | Batch: 28400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2537.9822213696316\n",
      "Train Epoch:  4 | Batch: 28800 | Loss: 6.34\n",
      "tensor([4609.])\n",
      "151.5408567637205\n",
      "Train Epoch:  4 | Batch: 29200 | Loss: 0.38\n",
      "tensor([4609.])\n",
      "4224.192855930887\n",
      "Train Epoch:  4 | Batch: 29600 | Loss: 10.56\n",
      "tensor([4609.])\n",
      "552.6194492774084\n",
      "Train Epoch:  4 | Batch: 30000 | Loss: 1.38\n",
      "tensor([4609.])\n",
      "2499.7541121169925\n",
      "Train Epoch:  4 | Batch: 30400 | Loss: 6.25\n",
      "tensor([4609.])\n",
      "1462.058777563274\n",
      "Train Epoch:  4 | Batch: 30800 | Loss: 3.66\n",
      "tensor([4609.])\n",
      "1000.3656232482754\n",
      "Train Epoch:  4 | Batch: 31200 | Loss: 2.50\n",
      "tensor([4609.])\n",
      "2018.056308934465\n",
      "Train Epoch:  4 | Batch: 31600 | Loss: 5.05\n",
      "tensor([4609.])\n",
      "219.72810520487837\n",
      "Train Epoch:  4 | Batch: 32000 | Loss: 0.55\n",
      "tensor([4609.])\n",
      "1249.0351030896418\n",
      "Train Epoch:  4 | Batch: 32400 | Loss: 3.12\n",
      "tensor([4609.])\n",
      "120.05385308922268\n",
      "Train Epoch:  4 | Batch: 32800 | Loss: 0.30\n",
      "tensor([4609.])\n",
      "3696.663611196447\n",
      "Train Epoch:  4 | Batch: 33200 | Loss: 9.24\n",
      "tensor([4609.])\n",
      "42.444893821375445\n",
      "Train Epoch:  4 | Batch: 33600 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "3707.6890845042653\n",
      "Train Epoch:  4 | Batch: 34000 | Loss: 9.27\n",
      "tensor([4609.])\n",
      "67.62186903553084\n",
      "Train Epoch:  4 | Batch: 34400 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "4963.030337570235\n",
      "Train Epoch:  4 | Batch: 34800 | Loss: 12.41\n",
      "tensor([4609.])\n",
      "28.378556821029633\n",
      "Train Epoch:  4 | Batch: 35200 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2986.040839413181\n",
      "Train Epoch:  4 | Batch: 35600 | Loss: 7.47\n",
      "tensor([4609.])\n",
      "19.192461064085364\n",
      "Train Epoch:  4 | Batch: 36000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2171.698279596865\n",
      "Train Epoch:  4 | Batch: 36400 | Loss: 5.43\n",
      "tensor([4609.])\n",
      "22.90011584572494\n",
      "Train Epoch:  4 | Batch: 36800 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "3210.5917612602934\n",
      "Train Epoch:  4 | Batch: 37200 | Loss: 8.03\n",
      "tensor([4609.])\n",
      "103.05610639043152\n",
      "Train Epoch:  4 | Batch: 37600 | Loss: 0.26\n",
      "tensor([4609.])\n",
      "1326.6570849807467\n",
      "Train Epoch:  4 | Batch: 38000 | Loss: 3.32\n",
      "tensor([4609.])\n",
      "278.1010737591423\n",
      "Train Epoch:  4 | Batch: 38400 | Loss: 0.70\n",
      "tensor([4609.])\n",
      "1680.1936351642944\n",
      "Train Epoch:  4 | Batch: 38800 | Loss: 4.20\n",
      "tensor([4609.])\n",
      "1549.0398302837275\n",
      "Train Epoch:  4 | Batch: 39200 | Loss: 3.87\n",
      "tensor([4609.])\n",
      "2221.5887801756617\n",
      "Train Epoch:  4 | Batch: 39600 | Loss: 5.55\n",
      "tensor([4609.])\n",
      "1499.0420060525648\n",
      "Train Epoch:  4 | Batch: 40000 | Loss: 3.75\n",
      "tensor([4609.])\n",
      "421.11527214432135\n",
      "Train Epoch:  4 | Batch: 40400 | Loss: 1.05\n",
      "tensor([4609.])\n",
      "2252.2627912424505\n",
      "Train Epoch:  4 | Batch: 40800 | Loss: 5.63\n",
      "tensor([4609.])\n",
      "266.7698913468048\n",
      "Train Epoch:  4 | Batch: 41200 | Loss: 0.67\n",
      "tensor([4609.])\n",
      "2940.261801877059\n",
      "Train Epoch:  4 | Batch: 41600 | Loss: 7.35\n",
      "tensor([4609.])\n",
      "85.72297251690179\n",
      "Train Epoch:  4 | Batch: 42000 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "2727.2094579506665\n",
      "Train Epoch:  4 | Batch: 42400 | Loss: 6.82\n",
      "tensor([4609.])\n",
      "30.6113671627827\n",
      "Train Epoch:  4 | Batch: 42800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2195.2644105535\n",
      "Train Epoch:  4 | Batch: 43200 | Loss: 5.49\n",
      "tensor([4609.])\n",
      "14.668034438975155\n",
      "Train Epoch:  4 | Batch: 43600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1564.7965382486582\n",
      "Train Epoch:  4 | Batch: 44000 | Loss: 3.91\n",
      "tensor([4609.])\n",
      "4959.640710121952\n",
      "Train Epoch:  4 | Batch: 44400 | Loss: 12.40\n",
      "tensor([4609.])\n",
      "96.2758620257955\n",
      "Train Epoch:  4 | Batch: 44800 | Loss: 0.24\n",
      "tensor([4609.])\n",
      "2984.028088924475\n",
      "Train Epoch:  4 | Batch: 45200 | Loss: 7.46\n",
      "tensor([4609.])\n",
      "141.563968045637\n",
      "Train Epoch:  4 | Batch: 45600 | Loss: 0.35\n",
      "tensor([4609.])\n",
      "2772.7892945581116\n",
      "Train Epoch:  4 | Batch: 46000 | Loss: 6.93\n",
      "tensor([4609.])\n",
      "674.244689890882\n",
      "Train Epoch:  4 | Batch: 46400 | Loss: 1.69\n",
      "tensor([4609.])\n",
      "2284.132032944821\n",
      "Train Epoch:  4 | Batch: 46800 | Loss: 5.71\n",
      "tensor([4609.])\n",
      "1604.8850295376033\n",
      "Train Epoch:  4 | Batch: 47200 | Loss: 4.01\n",
      "tensor([4609.])\n",
      "1095.0287494147196\n",
      "Train Epoch:  4 | Batch: 47600 | Loss: 2.74\n",
      "tensor([4609.])\n",
      "2733.4915737183765\n",
      "Train Epoch:  4 | Batch: 48000 | Loss: 6.83\n",
      "tensor([4609.])\n",
      "448.42498392704874\n",
      "Train Epoch:  4 | Batch: 48400 | Loss: 1.12\n",
      "tensor([4609.])\n",
      "2945.747964252718\n",
      "Train Epoch:  4 | Batch: 48800 | Loss: 7.36\n",
      "tensor([4609.])\n",
      "82.85210872464813\n",
      "Train Epoch:  4 | Batch: 49200 | Loss: 0.21\n",
      "tensor([4609.])\n",
      "1631.424127991777\n",
      "Train Epoch:  4 | Batch: 49600 | Loss: 4.08\n",
      "tensor([4609.])\n",
      "74.40980462823063\n",
      "Train Epoch:  4 | Batch: 50000 | Loss: 0.19\n",
      "tensor([4609.])\n",
      "4148.6255346331745\n",
      "Train Epoch:  4 | Batch: 50400 | Loss: 10.37\n",
      "tensor([4609.])\n",
      "28.24760279385373\n",
      "Train Epoch:  4 | Batch: 50800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2611.0553905181587\n",
      "Train Epoch:  4 | Batch: 51200 | Loss: 6.53\n",
      "tensor([4609.])\n",
      "28.808137086685747\n",
      "Train Epoch:  4 | Batch: 51600 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "3995.118829268962\n",
      "Train Epoch:  4 | Batch: 52000 | Loss: 9.99\n",
      "tensor([4609.])\n",
      "35.81199647951871\n",
      "Train Epoch:  4 | Batch: 52400 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "3531.3396446742117\n",
      "Train Epoch:  4 | Batch: 52800 | Loss: 8.83\n",
      "tensor([4609.])\n",
      "48.89212216017768\n",
      "Train Epoch:  4 | Batch: 53200 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "3441.718487315811\n",
      "Train Epoch:  4 | Batch: 53600 | Loss: 8.60\n",
      "tensor([4609.])\n",
      "155.47558795288205\n",
      "Train Epoch:  4 | Batch: 54000 | Loss: 0.39\n",
      "tensor([4609.])\n",
      "2226.639852836728\n",
      "Train Epoch:  4 | Batch: 54400 | Loss: 5.57\n",
      "tensor([4609.])\n",
      "507.7246171955485\n",
      "Train Epoch:  4 | Batch: 54800 | Loss: 1.27\n",
      "tensor([4609.])\n",
      "971.021413894603\n",
      "Train Epoch:  4 | Batch: 55200 | Loss: 2.43\n",
      "tensor([4609.])\n",
      "643.9064038491342\n",
      "Train Epoch:  4 | Batch: 55600 | Loss: 1.61\n",
      "tensor([4609.])\n",
      "868.6241078460589\n",
      "Train Epoch:  4 | Batch: 56000 | Loss: 2.17\n",
      "tensor([4609.])\n",
      "3368.023527944926\n",
      "Train Epoch:  4 | Batch: 56400 | Loss: 8.42\n",
      "tensor([4609.])\n",
      "322.6213823214639\n",
      "Train Epoch:  4 | Batch: 56800 | Loss: 0.81\n",
      "tensor([4609.])\n",
      "3574.9933277517557\n",
      "Train Epoch:  4 | Batch: 57200 | Loss: 8.94\n",
      "tensor([4609.])\n",
      "206.70919600967318\n",
      "Train Epoch:  4 | Batch: 57600 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "3665.1118323421106\n",
      "Train Epoch:  4 | Batch: 58000 | Loss: 9.16\n",
      "tensor([4609.])\n",
      "79.93887091008946\n",
      "Train Epoch:  4 | Batch: 58400 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "3973.187128826976\n",
      "Train Epoch:  4 | Batch: 58800 | Loss: 9.93\n",
      "tensor([4609.])\n",
      "33.960045834537596\n",
      "Train Epoch:  4 | Batch: 59200 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "3706.817928152159\n",
      "Train Epoch:  4 | Batch: 59600 | Loss: 9.27\n",
      "tensor([4609.])\n",
      "36.30745493993163\n",
      "Train Epoch:  4 | Batch: 60000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "4841.9134110268205\n",
      "Train Epoch:  4 | Batch: 60400 | Loss: 12.10\n",
      "tensor([4609.])\n",
      "18.349451466463506\n",
      "Train Epoch:  4 | Batch: 60800 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "1426.2486926894635\n",
      "Train Epoch:  4 | Batch: 61200 | Loss: 3.57\n",
      "tensor([4609.])\n",
      "25.407750832964666\n",
      "Train Epoch:  4 | Batch: 61600 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2772.1553534884006\n",
      "Train Epoch:  4 | Batch: 62000 | Loss: 6.93\n",
      "tensor([4609.])\n",
      "249.10907152248546\n",
      "Train Epoch:  4 | Batch: 62400 | Loss: 0.62\n",
      "tensor([4609.])\n",
      "2003.047240015585\n",
      "Train Epoch:  4 | Batch: 62800 | Loss: 5.01\n",
      "tensor([4609.])\n",
      "313.7910401998088\n",
      "Train Epoch:  4 | Batch: 63200 | Loss: 0.78\n",
      "tensor([4609.])\n",
      "1432.210360258352\n",
      "Train Epoch:  4 | Batch: 63600 | Loss: 3.58\n",
      "tensor([4609.])\n",
      "1084.2660365588963\n",
      "Train Epoch:  4 | Batch: 64000 | Loss: 2.71\n",
      "tensor([4609.])\n",
      "737.799532358069\n",
      "Train Epoch:  4 | Batch: 64400 | Loss: 1.84\n",
      "tensor([4609.])\n",
      "2107.8124573803507\n",
      "Train Epoch:  4 | Batch: 64800 | Loss: 5.27\n",
      "tensor([4609.])\n",
      "337.43346017529257\n",
      "Train Epoch:  4 | Batch: 65200 | Loss: 0.84\n",
      "tensor([4609.])\n",
      "2259.5028188922442\n",
      "Train Epoch:  4 | Batch: 65600 | Loss: 5.65\n",
      "tensor([4609.])\n",
      "209.79676427319646\n",
      "Train Epoch:  4 | Batch: 66000 | Loss: 0.52\n",
      "tensor([4609.])\n",
      "4122.040355003439\n",
      "Train Epoch:  4 | Batch: 66400 | Loss: 10.31\n",
      "tensor([4609.])\n",
      "40.6182742996607\n",
      "Train Epoch:  4 | Batch: 66800 | Loss: 0.10\n",
      "tensor([4609.])\n",
      "2499.921628887765\n",
      "Train Epoch:  4 | Batch: 67200 | Loss: 6.25\n",
      "tensor([4609.])\n",
      "20.33437487576157\n",
      "Train Epoch:  4 | Batch: 67600 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "3637.0149975679815\n",
      "Train Epoch:  4 | Batch: 68000 | Loss: 9.09\n",
      "tensor([4609.])\n",
      "24.843239264562726\n",
      "Train Epoch:  4 | Batch: 68400 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "9013.907165076584\n",
      "Train Epoch:  4 | Batch: 68800 | Loss: 22.53\n",
      "tensor([4609.])\n",
      "63.72014179266989\n",
      "Train Epoch:  4 | Batch: 69200 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "2581.4329574103467\n",
      "Train Epoch:  4 | Batch: 69600 | Loss: 6.45\n",
      "tensor([4609.])\n",
      "359.6575971217826\n",
      "Train Epoch:  4 | Batch: 70000 | Loss: 0.90\n",
      "tensor([4609.])\n",
      "2066.7183079815004\n",
      "Train Epoch:  4 | Batch: 70400 | Loss: 5.17\n",
      "tensor([4609.])\n",
      "880.9467474115081\n",
      "Train Epoch:  4 | Batch: 70800 | Loss: 2.20\n",
      "tensor([4609.])\n",
      "996.5124918110669\n",
      "Train Epoch:  4 | Batch: 71200 | Loss: 2.49\n",
      "tensor([4609.])\n",
      "1711.5795946121216\n",
      "Train Epoch:  4 | Batch: 71600 | Loss: 4.28\n",
      "tensor([4609.])\n",
      "219.25196460518055\n",
      "Train Epoch:  4 | Batch: 72000 | Loss: 0.55\n",
      "tensor([4609.])\n",
      "1137.7406745168846\n",
      "Train Epoch:  4 | Batch: 72400 | Loss: 2.84\n",
      "tensor([4609.])\n",
      "116.65025714691728\n",
      "Train Epoch:  4 | Batch: 72800 | Loss: 0.29\n",
      "tensor([4609.])\n",
      "3248.7278185263276\n",
      "Train Epoch:  4 | Batch: 73200 | Loss: 8.12\n",
      "tensor([4609.])\n",
      "32.4792604414979\n",
      "Train Epoch:  4 | Batch: 73600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2482.417680446524\n",
      "Train Epoch:  4 | Batch: 74000 | Loss: 6.21\n",
      "tensor([4609.])\n",
      "47.29277772177011\n",
      "Train Epoch:  4 | Batch: 74400 | Loss: 0.12\n",
      "tensor([4609.])\n",
      "2423.600636220537\n",
      "Train Epoch:  4 | Batch: 74800 | Loss: 6.06\n",
      "tensor([4609.])\n",
      "24.33097401005216\n",
      "Train Epoch:  4 | Batch: 75200 | Loss: 0.06\n",
      "tensor([4609.])\n",
      "2601.276044827886\n",
      "Train Epoch:  4 | Batch: 75600 | Loss: 6.50\n",
      "tensor([4609.])\n",
      "20.386906484141946\n",
      "Train Epoch:  4 | Batch: 76000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2543.1850063372403\n",
      "Train Epoch:  4 | Batch: 76400 | Loss: 6.36\n",
      "tensor([4609.])\n",
      "28.133116390556097\n",
      "Train Epoch:  4 | Batch: 76800 | Loss: 0.07\n",
      "tensor([4609.])\n",
      "2457.6079476326704\n",
      "Train Epoch:  4 | Batch: 77200 | Loss: 6.14\n",
      "tensor([4609.])\n",
      "66.01120131928474\n",
      "Train Epoch:  4 | Batch: 77600 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "1024.6332412213087\n",
      "Train Epoch:  4 | Batch: 78000 | Loss: 2.56\n",
      "tensor([4609.])\n",
      "184.19729943480343\n",
      "Train Epoch:  4 | Batch: 78400 | Loss: 0.46\n",
      "tensor([4609.])\n",
      "2398.7914321064018\n",
      "Train Epoch:  4 | Batch: 78800 | Loss: 6.00\n",
      "tensor([4609.])\n",
      "2154.7990118921734\n",
      "Train Epoch:  4 | Batch: 79200 | Loss: 5.39\n",
      "tensor([4609.])\n",
      "1159.1666875029914\n",
      "Train Epoch:  4 | Batch: 79600 | Loss: 2.90\n",
      "tensor([4609.])\n",
      "2012.2589421961457\n",
      "Train Epoch:  4 | Batch: 80000 | Loss: 5.03\n",
      "tensor([4609.])\n",
      "444.687804649584\n",
      "Train Epoch:  4 | Batch: 80400 | Loss: 1.11\n",
      "tensor([4609.])\n",
      "1861.0883809784427\n",
      "Train Epoch:  4 | Batch: 80800 | Loss: 4.65\n",
      "tensor([4609.])\n",
      "89.28152198065072\n",
      "Train Epoch:  4 | Batch: 81200 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "1222.075991358608\n",
      "Train Epoch:  4 | Batch: 81600 | Loss: 3.06\n",
      "tensor([4609.])\n",
      "42.912480674218386\n",
      "Train Epoch:  4 | Batch: 82000 | Loss: 0.11\n",
      "tensor([4609.])\n",
      "2122.0487575177103\n",
      "Train Epoch:  4 | Batch: 82400 | Loss: 5.31\n",
      "tensor([4609.])\n",
      "30.80931066069752\n",
      "Train Epoch:  4 | Batch: 82800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2476.5563287390396\n",
      "Train Epoch:  4 | Batch: 83200 | Loss: 6.19\n",
      "tensor([4609.])\n",
      "14.863698044791818\n",
      "Train Epoch:  4 | Batch: 83600 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "1700.2822058368474\n",
      "Train Epoch:  4 | Batch: 84000 | Loss: 4.25\n",
      "tensor([4609.])\n",
      "13.361187876667827\n",
      "Train Epoch:  4 | Batch: 84400 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "2755.326756968163\n",
      "Train Epoch:  4 | Batch: 84800 | Loss: 6.89\n",
      "tensor([4609.])\n",
      "72.00556571781635\n",
      "Train Epoch:  4 | Batch: 85200 | Loss: 0.18\n",
      "tensor([4609.])\n",
      "3536.0117854392156\n",
      "Train Epoch:  4 | Batch: 85600 | Loss: 8.84\n",
      "tensor([4609.])\n",
      "36.002555498853326\n",
      "Train Epoch:  4 | Batch: 86000 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "2791.1982197957113\n",
      "Train Epoch:  4 | Batch: 86400 | Loss: 6.98\n",
      "tensor([4609.])\n",
      "369.2742356657982\n",
      "Train Epoch:  4 | Batch: 86800 | Loss: 0.92\n",
      "tensor([4609.])\n",
      "2229.2554225130007\n",
      "Train Epoch:  4 | Batch: 87200 | Loss: 5.57\n",
      "tensor([4609.])\n",
      "1600.232937686611\n",
      "Train Epoch:  4 | Batch: 87600 | Loss: 4.00\n",
      "tensor([4609.])\n",
      "1423.177768183872\n",
      "Train Epoch:  4 | Batch: 88000 | Loss: 3.56\n",
      "tensor([4609.])\n",
      "2140.68810237851\n",
      "Train Epoch:  4 | Batch: 88400 | Loss: 5.35\n",
      "tensor([4609.])\n",
      "810.4216407681815\n",
      "Train Epoch:  4 | Batch: 88800 | Loss: 2.03\n",
      "tensor([4609.])\n",
      "3546.1145507204346\n",
      "Train Epoch:  4 | Batch: 89200 | Loss: 8.87\n",
      "tensor([4609.])\n",
      "149.30147702619433\n",
      "Train Epoch:  4 | Batch: 89600 | Loss: 0.37\n",
      "tensor([4609.])\n",
      "2582.3425769312307\n",
      "Train Epoch:  4 | Batch: 90000 | Loss: 6.46\n",
      "tensor([4609.])\n",
      "102.2305836705491\n",
      "Train Epoch:  4 | Batch: 90400 | Loss: 0.26\n",
      "tensor([4609.])\n",
      "5597.99660827592\n",
      "Train Epoch:  4 | Batch: 90800 | Loss: 13.99\n",
      "tensor([4609.])\n",
      "37.65660246834159\n",
      "Train Epoch:  4 | Batch: 91200 | Loss: 0.09\n",
      "tensor([4609.])\n",
      "4968.002269746736\n",
      "Train Epoch:  4 | Batch: 91600 | Loss: 12.42\n",
      "tensor([4609.])\n",
      "86.26411093678325\n",
      "Train Epoch:  4 | Batch: 92000 | Loss: 0.22\n",
      "tensor([4609.])\n",
      "6147.6963475979865\n",
      "Train Epoch:  4 | Batch: 92400 | Loss: 15.37\n",
      "tensor([4609.])\n",
      "62.28865019232035\n",
      "Train Epoch:  4 | Batch: 92800 | Loss: 0.16\n",
      "tensor([4609.])\n",
      "5525.236945420504\n",
      "Train Epoch:  4 | Batch: 93200 | Loss: 13.81\n",
      "tensor([4609.])\n",
      "67.52850238373503\n",
      "Train Epoch:  4 | Batch: 93600 | Loss: 0.17\n",
      "tensor([4609.])\n",
      "1873.9710365659557\n",
      "Train Epoch:  4 | Batch: 94000 | Loss: 4.68\n",
      "tensor([4609.])\n",
      "59.49737843265757\n",
      "Train Epoch:  4 | Batch: 94400 | Loss: 0.15\n",
      "tensor([4609.])\n",
      "3631.111374336295\n",
      "Train Epoch:  4 | Batch: 94800 | Loss: 9.08\n",
      "tensor([4609.])\n",
      "386.59605656098574\n",
      "Train Epoch:  4 | Batch: 95200 | Loss: 0.97\n",
      "tensor([4609.])\n",
      "1228.8721746066585\n",
      "Train Epoch:  4 | Batch: 95600 | Loss: 3.07\n",
      "tensor([4609.])\n",
      "878.3394504231401\n",
      "Train Epoch:  4 | Batch: 96000 | Loss: 2.20\n",
      "tensor([4609.])\n",
      "1157.066662850324\n",
      "Train Epoch:  4 | Batch: 96400 | Loss: 2.89\n",
      "tensor([4609.])\n",
      "3235.343333809171\n",
      "Train Epoch:  4 | Batch: 96800 | Loss: 8.09\n",
      "tensor([4609.])\n",
      "490.5030680568889\n",
      "Train Epoch:  4 | Batch: 97200 | Loss: 1.23\n",
      "tensor([4609.])\n",
      "1861.8645982118323\n",
      "Train Epoch:  4 | Batch: 97600 | Loss: 4.65\n",
      "tensor([4609.])\n",
      "78.39032324030995\n",
      "Train Epoch:  4 | Batch: 98000 | Loss: 0.20\n",
      "tensor([4609.])\n",
      "1572.939414809458\n",
      "Train Epoch:  4 | Batch: 98400 | Loss: 3.93\n",
      "tensor([4609.])\n",
      "30.516870173625648\n",
      "Train Epoch:  4 | Batch: 98800 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2947.9716486688703\n",
      "Train Epoch:  4 | Batch: 99200 | Loss: 7.37\n",
      "tensor([4609.])\n",
      "33.56528870668262\n",
      "Train Epoch:  4 | Batch: 99600 | Loss: 0.08\n",
      "tensor([4609.])\n",
      "2190.0577589683235\n",
      "Train Epoch:  4 | Batch: 100000 | Loss: 5.48\n",
      "tensor([4609.])\n",
      "15.463719112798572\n",
      "Train Epoch:  4 | Batch: 100400 | Loss: 0.04\n",
      "tensor([4609.])\n",
      "2209.594932024367\n",
      "Train Epoch:  4 | Batch: 100800 | Loss: 5.52\n",
      "tensor([4609.])\n",
      "12.345738854259253\n",
      "Train Epoch:  4 | Batch: 101200 | Loss: 0.03\n",
      "tensor([4609.])\n",
      "1995.5562569787726\n",
      "Train Epoch:  4 | Batch: 101600 | Loss: 4.99\n",
      "tensor([4609.])\n",
      "20.280582022853196\n",
      "Train Epoch:  4 | Batch: 102000 | Loss: 0.05\n",
      "tensor([4609.])\n",
      "2847.329929766245\n",
      "Train Epoch:  4 | Batch: 102400 | Loss: 7.12\n",
      "tensor([4609.])\n",
      "118.83711366006173\n",
      "Train Epoch:  4 | Batch: 102800 | Loss: 0.30\n",
      "35\n",
      "\n",
      "Valid Epoch:  4 | Loss: 1.07\n"
     ]
    }
   ],
   "source": [
    "train_batch_loss = 0.0      # 400 batch마다 평균 training loss를 확인한 다음, train_batch_loss를 0으로 갱신해줄 것\n",
    "train_epoch_loss = 0.0      # 1 epoch마다 평균 training loss를 확인한 다음, train_epoch_loss를 0으로 갱신해줄 것\n",
    "\n",
    "valid_epoch_loss = 0.0      # 1 epoch마다 평균 validation loss를 확인한 다음, valid_epoch_loss를 0으로 갱신해줄 것\n",
    "valid_min_epoch_loss = np.inf      # 초기 loss를 마이너스 무한대로 설정해두고, validation epoch loss가 낮아질 때마다 갱신해줄 것\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    " \n",
    "    model.train()      # 모델을 train mode로 전환. train mode일 때만 적용되어야 하는 drop out 등이 적용될 수 있게 하기 위함 \n",
    "\n",
    "    for iter_, sample in enumerate(train_dataloader):      # enumerate 함수를 통해 train_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "        _, _,(input_data, output_data) = sample      # train_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨.\n",
    "                                                      # 학습에는 [[input_data], [output_data]]만 사용\n",
    "        \n",
    "        input_data = input_data.to(device)      #unsqueeze 차원 맞춰주는 것\n",
    "        output_data = output_data.to(device)   \n",
    "     \n",
    "        pred = model(input_data)\n",
    "        \n",
    "        loss = criterion(pred, output_data)\n",
    "\n",
    "        model.zero_grad()    # 파라미터 업데이트는 batch 단위로 이루어지고, 매 batch마다 gradient를 초기화해주어야 함 \n",
    "        loss.backward()   # backpropagation\n",
    "        \n",
    "        optimizer.step()      # 파라미터 업데이트\n",
    "        \n",
    "        train_batch_loss += loss.item() #item은 tensor에서 스칼라 값을 가져오기위한 함수\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        if iter_ % 400 == 399:      # 400개의 batch마다 training Loss 출력\n",
    "            print(norm)             # norm 크기 출력\n",
    "            print(train_batch_loss)   # train_batch 배치\n",
    "            print('Train Epoch: {:2} | Batch: {:4} | Loss: {:1.2f}'.format(epoch, iter_+1, train_batch_loss/400))\n",
    "            train_batch_loss = 0\n",
    "            \n",
    "    train_epoch_loss = 0.0      # training epoch마다 train_epoch_loss 새로 구해줄 것\n",
    "\n",
    "    \n",
    "    model.eval()      # 모델을 eval mode로 전환. eval mode에서 적용되면 안되는 drop out 등이 적용되지 않게 하기 위함\n",
    "\n",
    "    with torch.no_grad():      # validation / test set에 대해서는 weight 및 bias의 update, 즉, gradient descent가 일어나지 않도록 no_grad()를 선언\n",
    "\n",
    "        for iter_, sample in enumerate(validate_dataloader):      # enumerate 함수를 통해 validate_dataloader에서 'batch의 index'와 'batch'를 순서대로 호출\n",
    "\n",
    "            _, _, (input_data, output_data) = sample      # validate_dataloader에서 불러온 sample은 [[날짜, 시간], [도로], [[input_data],[output_data]]]로 구성됨. validation에는 [[input_data], [output_data]]만 사용\n",
    "\n",
    "            input_data = input_data.to(device)\n",
    "            output_data = output_data.to(device)\n",
    "\n",
    "            pred = model(input_data)\n",
    "            loss = criterion(pred, output_data)\n",
    "            valid_epoch_loss += loss.item()\n",
    "        print(len(validate_dataloader))\n",
    "        print('\\nValid Epoch: {:2} | Loss: {:1.2f}'.format(epoch, valid_epoch_loss/len(validate_dataloader)))\n",
    "\n",
    "        if valid_epoch_loss < valid_min_epoch_loss:\n",
    "            save_model('best', model, optimizer)\n",
    "            valid_min_epoch_loss = valid_epoch_loss\n",
    "\n",
    "        valid_epoch_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:06:44.091936Z",
     "iopub.status.busy": "2022-08-30T05:06:44.091051Z",
     "iopub.status.idle": "2022-08-30T05:06:44.107780Z",
     "shell.execute_reply": "2022-08-30T05:06:44.106435Z",
     "shell.execute_reply.started": "2022-08-30T05:06:44.091876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([20200518]), tensor([0])],\n",
       " ('10',),\n",
       " [tensor([[0.5714, 0.3569, 0.2717, 0.2854, 0.5422, 1.3218, 2.1454, 2.4800, 2.2437,\n",
       "           2.0888, 2.2029, 2.2296, 2.1082, 2.0861, 2.1808, 2.2540, 2.1194, 2.2104,\n",
       "           2.1350, 1.6241, 1.3489, 1.2446, 0.9779, 0.6762, 0.4821, 0.3536, 0.2887,\n",
       "           0.2989, 0.4253, 0.8657, 1.5450, 2.0152, 2.1208, 2.0729, 2.1758, 2.1673,\n",
       "           2.0627, 2.1681, 2.1896, 2.2954, 2.3733, 2.4782, 2.2479, 1.7076, 1.4651,\n",
       "           1.3372, 1.1099, 0.8108, 0.5525, 0.3947, 0.3288, 0.3255, 0.4891, 1.0615,\n",
       "           1.7959, 2.2485, 2.2894, 2.2710, 2.3655, 2.3348, 2.2966, 2.2715, 2.2995,\n",
       "           2.3754, 2.4849, 2.6436, 2.3693, 1.7643, 1.5113, 1.4088, 1.1209, 0.8541,\n",
       "           0.5833, 0.3982, 0.3180, 0.3298, 0.4819, 1.0206, 1.7435, 2.2069, 2.3098,\n",
       "           2.2420, 2.3930, 2.3667, 2.2472, 2.2970, 2.3280, 2.4211, 2.4911, 2.6195,\n",
       "           2.4457, 1.9193, 1.6799, 1.5350, 1.2201, 0.8786, 0.5846, 0.4183, 0.3389,\n",
       "           0.3384, 0.5014, 0.9991, 1.6906, 2.1809, 2.2360, 2.2051, 2.3808, 2.4384,\n",
       "           2.4035, 2.5190, 2.6763, 2.8224, 2.9297, 3.0581, 2.9186, 2.5648, 2.2295,\n",
       "           2.0238, 1.5226, 1.0636, 0.6967, 0.4634, 0.3521, 0.3315, 0.4640, 0.9353,\n",
       "           1.4266, 1.6871, 2.0714, 2.3745, 2.5890, 2.6107, 2.4431, 2.4590, 2.5436,\n",
       "           2.6647, 2.6676, 2.6800, 2.3958, 2.0298, 1.7713, 1.6574, 1.2923, 0.8642,\n",
       "           0.5338, 0.3298, 0.2305, 0.1759, 0.2110, 0.4667, 0.7341, 0.9061, 1.1913,\n",
       "           1.6531, 2.1037, 2.2613, 2.2552, 2.4340, 2.6599, 2.7544, 2.7644, 2.5809,\n",
       "           2.3911, 2.1880, 2.0890, 2.1179, 1.6485, 1.0000]]),\n",
       "  []]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#마지막 아웃풋을 다음 인풋으로 받기 위해선 stateful=True로 성정해주어야하며, 이런 stateful 네트워크에선 고정된 배치 사이즈를 정해놓는 것이 좋습니다. 3D텐서는 (batch_size, timesteps, features)의 순서로 넣으면 됩니다. *배치 사이즈는 반드시 데이터 셋으로 나누어 떨어지는 숫자여야합니다. train, val, test에서 모두에 대해 나누어 떨어져야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T01:19:57.186008Z",
     "iopub.status.busy": "2022-08-30T01:19:57.185083Z",
     "iopub.status.idle": "2022-08-30T01:19:58.657552Z",
     "shell.execute_reply": "2022-08-30T01:19:58.655602Z",
     "shell.execute_reply.started": "2022-08-30T01:19:57.185943Z"
    },
    "id": "fnWQ3bCfFqda",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 30 10:19:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    36W / 250W |  12787MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    25W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    26W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    25W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4bfrlEEzR_e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T11:52:56.759844Z",
     "iopub.status.busy": "2022-08-29T11:52:56.759341Z",
     "iopub.status.idle": "2022-08-29T11:52:56.766311Z",
     "shell.execute_reply": "2022-08-29T11:52:56.765509Z",
     "shell.execute_reply.started": "2022-08-29T11:52:56.759812Z"
    },
    "id": "eb34WBmFzR_e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "seq_len = 7\n",
    "input_size = seq_len * 24\n",
    "hidden_size = 1024\n",
    "output_size = input_size\n",
    "batch_size = 1\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:19.329550Z",
     "iopub.status.busy": "2022-08-29T09:14:19.329129Z",
     "iopub.status.idle": "2022-08-29T09:14:19.890424Z",
     "shell.execute_reply": "2022-08-29T09:14:19.889402Z",
     "shell.execute_reply.started": "2022-08-29T09:14:19.329513Z"
    },
    "id": "GCrZisLFzR_e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataloader = data_loader(root=DATASET_PATH,\n",
    "                              phase='test',\n",
    "                              batch_size=batch_size,\n",
    "                                  seq_len=seq_len,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:19.893396Z",
     "iopub.status.busy": "2022-08-29T09:14:19.892920Z",
     "iopub.status.idle": "2022-08-29T09:14:23.629087Z",
     "shell.execute_reply": "2022-08-29T09:14:23.628207Z",
     "shell.execute_reply.started": "2022-08-29T09:14:19.893371Z"
    },
    "id": "1wyz-z-vzR_f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNet(input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                batch_size=batch_size,\n",
    "                num_layers=num_layers)\n",
    "\n",
    "# model\n",
    "model_name = 'log/best.pth'\n",
    "\n",
    "load_model(model_name, model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:10:08.064274Z",
     "iopub.status.busy": "2022-08-30T05:10:08.063368Z",
     "iopub.status.idle": "2022-08-30T05:10:08.074996Z",
     "shell.execute_reply": "2022-08-30T05:10:08.073296Z",
     "shell.execute_reply.started": "2022-08-30T05:10:08.064209Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 inputsize\n",
      "1024 hiddensize\n",
      "168 outputsize\n",
      "1 batchsize\n",
      "6 numlayers\n"
     ]
    }
   ],
   "source": [
    "print(input_size,'inputsize')\n",
    "print(hidden_size,'hiddensize')\n",
    "print(output_size,'outputsize')\n",
    "print(batch_size,'batchsize')\n",
    "print(num_layers,'numlayers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.630716Z",
     "iopub.status.busy": "2022-08-29T09:14:23.630406Z",
     "iopub.status.idle": "2022-08-29T09:14:23.640202Z",
     "shell.execute_reply": "2022-08-29T09:14:23.639477Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.630690Z"
    },
    "id": "QNSxHFwQzR_f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\n",
    "submission_table = pd.read_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:05:47.425637Z",
     "iopub.status.busy": "2022-08-30T05:05:47.424447Z",
     "iopub.status.idle": "2022-08-30T05:05:47.555005Z",
     "shell.execute_reply": "2022-08-30T05:05:47.554136Z",
     "shell.execute_reply.started": "2022-08-30T05:05:47.425521Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200525_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200525_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200525_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200525_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200525_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>20200531_19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>20200531_20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20200531_21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20200531_22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>20200531_23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp   10  100  101  120  121  140  150  160  200  ...  1020  \\\n",
       "0     20200525_0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1     20200525_1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2     20200525_2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3     20200525_3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4     20200525_4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "..           ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
       "163  20200531_19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "164  20200531_20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "165  20200531_21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "166  20200531_22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "167  20200531_23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "     1040  1100  1200  1510  2510  3000  4510  5510  6000  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "163   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "164   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "165   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "166   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "167   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[168 rows x 36 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.683381Z",
     "iopub.status.busy": "2022-08-29T09:14:23.682983Z",
     "iopub.status.idle": "2022-08-29T09:14:23.687829Z",
     "shell.execute_reply": "2022-08-29T09:14:23.687135Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.683359Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "6\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(hidden_size)\n",
    "print(num_layers)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.689952Z",
     "iopub.status.busy": "2022-08-29T09:14:23.689501Z",
     "iopub.status.idle": "2022-08-29T09:14:23.700155Z",
     "shell.execute_reply": "2022-08-29T09:14:23.699423Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.689923Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5071, 0.3927, 0.3320, 0.4083, 0.7932, 2.0482, 4.2580, 5.9013, 5.3964,\n",
       "          4.7631, 4.8976, 4.8128, 4.2369, 4.2194, 4.5353, 4.6329, 4.9514, 5.3142,\n",
       "          4.9761, 3.4443, 2.2957, 1.5971, 1.1510, 0.8125, 0.5372, 0.3617, 0.3695,\n",
       "          0.3784, 0.7785, 1.6134, 3.3087, 5.3044, 5.1465, 4.9143, 5.0022, 4.8282,\n",
       "          4.2536, 4.1690, 4.6828, 4.8937, 5.1057, 5.4680, 5.0473, 3.1668, 2.2784,\n",
       "          1.5771, 1.1291, 0.8154, 0.4845, 0.3669, 0.3003, 0.4183, 0.7409, 1.7492,\n",
       "          3.3771, 5.2348, 5.2690, 4.8190, 5.1623, 4.9883, 4.3187, 4.3296, 4.7125,\n",
       "          4.8329, 5.1211, 5.3903, 3.5450, 3.4561, 2.2823, 1.7227, 1.2237, 0.8177,\n",
       "          0.5689, 0.3278, 0.3567, 0.3882, 0.6392, 1.6971, 3.3070, 5.1911, 5.0501,\n",
       "          4.7839, 5.1530, 4.7650, 4.2469, 4.4200, 4.7381, 4.7837, 5.2838, 5.7609,\n",
       "          5.5491, 3.5689, 2.6900, 1.8447, 1.3552, 0.9356, 0.5138, 0.3680, 0.3450,\n",
       "          0.4049, 0.5800, 0.0000, 2.7084, 4.4543, 4.7722, 4.2959, 4.6446, 4.5374,\n",
       "          4.1914, 4.3378, 4.7125, 5.2098, 5.6134, 6.1306, 6.1202, 4.4077, 2.8568,\n",
       "          1.9688, 1.3118, 0.7987, 0.5400, 0.4387, 0.3439, 0.3285, 0.5674, 1.1465,\n",
       "          2.1430, 2.7915, 3.3767, 4.4708, 5.6479, 5.9618, 5.8765, 5.8546, 6.4053,\n",
       "          6.4085, 6.3662, 6.2701, 5.2582, 3.7314, 2.8390, 2.4400, 1.7969, 1.0193,\n",
       "          0.5856, 0.4791, 0.3419, 0.2994, 0.3669, 0.7754, 1.3751, 1.6073, 2.1608,\n",
       "          3.2456, 4.4120, 5.4040, 5.1775, 5.3957, 6.0588, 6.3513, 6.5405, 6.0621,\n",
       "          5.5437, 4.3814, 3.8972, 3.0443, 1.9286, 0.9993]]),\n",
       " tensor([[0.5021, 0.3832, 0.2840, 0.4148, 0.7874, 2.0482, 4.1384, 5.8015, 5.3107,\n",
       "          4.7388, 4.7713, 4.4934, 4.3055, 4.1024, 4.4172, 4.4177, 4.6479, 5.1523,\n",
       "          4.6761, 2.9100, 2.0560, 1.4292, 1.0024, 0.6529, 0.4333, 0.3174, 0.2777,\n",
       "          0.3554, 0.6609, 1.5787, 3.1948, 5.1143, 5.1859, 4.7746, 4.9414, 4.7286,\n",
       "          4.4893, 4.3790, 4.7908, 4.5374, 5.1076, 5.4274, 4.9197, 3.1857, 2.2447,\n",
       "          1.6062, 1.2131, 0.7826, 0.4608, 0.3552, 0.2944, 0.3678, 0.7166, 1.6257,\n",
       "          3.2851, 5.1623, 5.1224, 4.8418, 5.0963, 4.8321, 3.5680, 4.0293, 4.5925,\n",
       "          4.8750, 5.1143, 6.1174, 5.3205, 3.2411, 2.2463, 1.6507, 1.1671, 0.8269,\n",
       "          0.5311, 0.3571, 0.3040, 0.3760, 0.7032, 1.6594, 3.2224, 5.1365, 5.2411,\n",
       "          4.8490, 5.1842, 4.8963, 4.2280, 4.2300, 4.7529, 4.7574, 5.1968, 5.7516,\n",
       "          5.2159, 3.5238, 2.2775, 1.7353, 1.2766, 0.8778, 0.5604, 0.3775, 0.3255,\n",
       "          0.4352, 0.7689, 1.6889, 3.3307, 5.0323, 4.9737, 4.8646, 5.2762, 5.1901,\n",
       "          4.4075, 4.7360, 5.3291, 5.7240, 6.1807, 6.8353, 6.6414, 5.3617, 3.4585,\n",
       "          2.4860, 1.6357, 1.1052, 0.7171, 0.5186, 0.3871, 0.4027, 0.5921, 1.4179,\n",
       "          2.5394, 3.2721, 3.9761, 5.2428, 6.2048, 6.4641, 6.4697, 6.0336, 6.6418,\n",
       "          6.6706, 6.8314, 6.6043, 5.8266, 4.3398, 3.4042, 2.7175, 1.9215, 1.1287,\n",
       "          0.6737, 0.4587, 0.3274, 0.2712, 0.3845, 0.7093, 1.2269, 1.6863, 2.2421,\n",
       "          3.5613, 4.8611, 5.5635, 5.5517, 5.7229, 6.1721, 6.7826, 6.3652, 6.2623,\n",
       "          5.5400, 4.5593, 3.8592, 3.0464, 1.7856, 1.0176]])]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.702146Z",
     "iopub.status.busy": "2022-08-29T09:14:23.701724Z",
     "iopub.status.idle": "2022-08-29T09:14:23.712006Z",
     "shell.execute_reply": "2022-08-29T09:14:23.711281Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.702121Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5441, 0.3794, 0.3728, 0.4428, 0.7970, 1.9142, 3.5975, 5.1070,\n",
       "          4.5859, 3.5920, 3.6960, 3.8200, 3.5845, 3.7864, 4.1695, 4.0903,\n",
       "          4.7572, 5.7183, 5.7366, 3.6190, 2.3775, 1.9218, 1.5206, 1.1080,\n",
       "          0.7198, 0.4846, 0.3397, 0.3863, 0.7205, 1.7156, 3.4970, 5.4290,\n",
       "          5.4632, 4.6036, 4.5190, 4.1889, 3.7299, 3.6632, 4.1288, 4.3790,\n",
       "          4.9990, 5.7970, 5.8878, 3.6810, 2.5977, 2.0146, 1.6468, 1.1462,\n",
       "          0.7446, 0.5317, 0.3813, 0.3851, 0.6939, 1.7266, 3.5750, 5.5264,\n",
       "          5.5683, 4.6460, 4.2175, 3.9001, 3.3266, 3.5774, 4.0038, 4.2322,\n",
       "          4.8410, 5.5570, 5.0113, 3.6510, 2.4199, 1.8146, 1.5054, 1.1151,\n",
       "          0.7350, 0.5716, 0.4303, 0.3996, 0.6866, 1.6642, 3.5181, 5.5518,\n",
       "          5.4681, 4.3882, 4.4155, 4.1012, 3.5503, 3.8297, 4.3237, 4.5132,\n",
       "          4.8081, 5.6636, 5.5905, 3.6523, 2.4636, 1.7352, 1.4180, 1.1153,\n",
       "          0.6901, 0.5334, 0.3749, 0.3620, 0.5912, 0.0172, 2.6701, 4.6328,\n",
       "          4.6925, 3.9952, 3.7936, 3.7465, 3.5087, 3.6889, 4.2898, 4.5119,\n",
       "          5.1297, 5.7256, 5.5985, 4.2116, 3.0484, 1.9584, 1.3960, 1.1105,\n",
       "          0.8489, 0.6544, 0.5553, 0.5039, 0.6345, 1.3005, 2.1723, 2.9882,\n",
       "          3.5090, 4.2636, 4.9513, 5.1592, 4.8886, 4.8639, 5.2954, 5.6610,\n",
       "          5.9337, 6.2578, 5.3760, 3.8078, 2.5719, 1.7948, 1.3481, 0.7481,\n",
       "          0.4115, 0.5762, 0.5138, 0.4081, 0.4469, 0.7450, 1.1119, 1.4896,\n",
       "          1.9311, 2.7191, 3.7237, 4.2933, 4.3039, 4.4753, 5.1692, 5.6234,\n",
       "          5.9352, 5.8044, 5.4486, 4.4745, 3.6457, 2.6942, 2.0168, 1.0137]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.713953Z",
     "iopub.status.busy": "2022-08-29T09:14:23.713528Z",
     "iopub.status.idle": "2022-08-29T09:14:23.723281Z",
     "shell.execute_reply": "2022-08-29T09:14:23.722036Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.713929Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5071, 0.3927, 0.3320, 0.4083, 0.7932, 2.0482, 4.2580, 5.9013, 5.3964,\n",
       "          4.7631, 4.8976, 4.8128, 4.2369, 4.2194, 4.5353, 4.6329, 4.9514, 5.3142,\n",
       "          4.9761, 3.4443, 2.2957, 1.5971, 1.1510, 0.8125, 0.5372, 0.3617, 0.3695,\n",
       "          0.3784, 0.7785, 1.6134, 3.3087, 5.3044, 5.1465, 4.9143, 5.0022, 4.8282,\n",
       "          4.2536, 4.1690, 4.6828, 4.8937, 5.1057, 5.4680, 5.0473, 3.1668, 2.2784,\n",
       "          1.5771, 1.1291, 0.8154, 0.4845, 0.3669, 0.3003, 0.4183, 0.7409, 1.7492,\n",
       "          3.3771, 5.2348, 5.2690, 4.8190, 5.1623, 4.9883, 4.3187, 4.3296, 4.7125,\n",
       "          4.8329, 5.1211, 5.3903, 3.5450, 3.4561, 2.2823, 1.7227, 1.2237, 0.8177,\n",
       "          0.5689, 0.3278, 0.3567, 0.3882, 0.6392, 1.6971, 3.3070, 5.1911, 5.0501,\n",
       "          4.7839, 5.1530, 4.7650, 4.2469, 4.4200, 4.7381, 4.7837, 5.2838, 5.7609,\n",
       "          5.5491, 3.5689, 2.6900, 1.8447, 1.3552, 0.9356, 0.5138, 0.3680, 0.3450,\n",
       "          0.4049, 0.5800, 0.0000, 2.7084, 4.4543, 4.7722, 4.2959, 4.6446, 4.5374,\n",
       "          4.1914, 4.3378, 4.7125, 5.2098, 5.6134, 6.1306, 6.1202, 4.4077, 2.8568,\n",
       "          1.9688, 1.3118, 0.7987, 0.5400, 0.4387, 0.3439, 0.3285, 0.5674, 1.1465,\n",
       "          2.1430, 2.7915, 3.3767, 4.4708, 5.6479, 5.9618, 5.8765, 5.8546, 6.4053,\n",
       "          6.4085, 6.3662, 6.2701, 5.2582, 3.7314, 2.8390, 2.4400, 1.7969, 1.0193,\n",
       "          0.5856, 0.4791, 0.3419, 0.2994, 0.3669, 0.7754, 1.3751, 1.6073, 2.1608,\n",
       "          3.2456, 4.4120, 5.4040, 5.1775, 5.3957, 6.0588, 6.3513, 6.5405, 6.0621,\n",
       "          5.5437, 4.3814, 3.8972, 3.0443, 1.9286, 0.9993]]),\n",
       " tensor([[0.5021, 0.3832, 0.2840, 0.4148, 0.7874, 2.0482, 4.1384, 5.8015, 5.3107,\n",
       "          4.7388, 4.7713, 4.4934, 4.3055, 4.1024, 4.4172, 4.4177, 4.6479, 5.1523,\n",
       "          4.6761, 2.9100, 2.0560, 1.4292, 1.0024, 0.6529, 0.4333, 0.3174, 0.2777,\n",
       "          0.3554, 0.6609, 1.5787, 3.1948, 5.1143, 5.1859, 4.7746, 4.9414, 4.7286,\n",
       "          4.4893, 4.3790, 4.7908, 4.5374, 5.1076, 5.4274, 4.9197, 3.1857, 2.2447,\n",
       "          1.6062, 1.2131, 0.7826, 0.4608, 0.3552, 0.2944, 0.3678, 0.7166, 1.6257,\n",
       "          3.2851, 5.1623, 5.1224, 4.8418, 5.0963, 4.8321, 3.5680, 4.0293, 4.5925,\n",
       "          4.8750, 5.1143, 6.1174, 5.3205, 3.2411, 2.2463, 1.6507, 1.1671, 0.8269,\n",
       "          0.5311, 0.3571, 0.3040, 0.3760, 0.7032, 1.6594, 3.2224, 5.1365, 5.2411,\n",
       "          4.8490, 5.1842, 4.8963, 4.2280, 4.2300, 4.7529, 4.7574, 5.1968, 5.7516,\n",
       "          5.2159, 3.5238, 2.2775, 1.7353, 1.2766, 0.8778, 0.5604, 0.3775, 0.3255,\n",
       "          0.4352, 0.7689, 1.6889, 3.3307, 5.0323, 4.9737, 4.8646, 5.2762, 5.1901,\n",
       "          4.4075, 4.7360, 5.3291, 5.7240, 6.1807, 6.8353, 6.6414, 5.3617, 3.4585,\n",
       "          2.4860, 1.6357, 1.1052, 0.7171, 0.5186, 0.3871, 0.4027, 0.5921, 1.4179,\n",
       "          2.5394, 3.2721, 3.9761, 5.2428, 6.2048, 6.4641, 6.4697, 6.0336, 6.6418,\n",
       "          6.6706, 6.8314, 6.6043, 5.8266, 4.3398, 3.4042, 2.7175, 1.9215, 1.1287,\n",
       "          0.6737, 0.4587, 0.3274, 0.2712, 0.3845, 0.7093, 1.2269, 1.6863, 2.2421,\n",
       "          3.5613, 4.8611, 5.5635, 5.5517, 5.7229, 6.1721, 6.7826, 6.3652, 6.2623,\n",
       "          5.5400, 4.5593, 3.8592, 3.0464, 1.7856, 1.0176]])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T23:23:50.853186Z",
     "iopub.status.busy": "2022-08-29T23:23:50.852276Z",
     "iopub.status.idle": "2022-08-29T23:23:50.873987Z",
     "shell.execute_reply": "2022-08-29T23:23:50.872436Z",
     "shell.execute_reply.started": "2022-08-29T23:23:50.853122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:12:47.199576Z",
     "iopub.status.busy": "2022-08-30T05:12:47.198676Z",
     "iopub.status.idle": "2022-08-30T05:12:47.210326Z",
     "shell.execute_reply": "2022-08-30T05:12:47.208841Z",
     "shell.execute_reply.started": "2022-08-30T05:12:47.199515Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:21:16.389780Z",
     "iopub.status.busy": "2022-08-30T05:21:16.388812Z",
     "iopub.status.idle": "2022-08-30T05:21:16.402262Z",
     "shell.execute_reply": "2022-08-30T05:21:16.401203Z",
     "shell.execute_reply.started": "2022-08-30T05:21:16.389711Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.739279Z",
     "iopub.status.busy": "2022-08-29T09:14:23.738916Z",
     "iopub.status.idle": "2022-08-29T09:14:23.752362Z",
     "shell.execute_reply": "2022-08-29T09:14:23.750829Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.739246Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([20200511]), tensor([0])],\n",
       " ('6000',),\n",
       " [tensor([[0.5071, 0.3927, 0.3320, 0.4083, 0.7932, 2.0482, 4.2580, 5.9013, 5.3964,\n",
       "           4.7631, 4.8976, 4.8128, 4.2369, 4.2194, 4.5353, 4.6329, 4.9514, 5.3142,\n",
       "           4.9761, 3.4443, 2.2957, 1.5971, 1.1510, 0.8125, 0.5372, 0.3617, 0.3695,\n",
       "           0.3784, 0.7785, 1.6134, 3.3087, 5.3044, 5.1465, 4.9143, 5.0022, 4.8282,\n",
       "           4.2536, 4.1690, 4.6828, 4.8937, 5.1057, 5.4680, 5.0473, 3.1668, 2.2784,\n",
       "           1.5771, 1.1291, 0.8154, 0.4845, 0.3669, 0.3003, 0.4183, 0.7409, 1.7492,\n",
       "           3.3771, 5.2348, 5.2690, 4.8190, 5.1623, 4.9883, 4.3187, 4.3296, 4.7125,\n",
       "           4.8329, 5.1211, 5.3903, 3.5450, 3.4561, 2.2823, 1.7227, 1.2237, 0.8177,\n",
       "           0.5689, 0.3278, 0.3567, 0.3882, 0.6392, 1.6971, 3.3070, 5.1911, 5.0501,\n",
       "           4.7839, 5.1530, 4.7650, 4.2469, 4.4200, 4.7381, 4.7837, 5.2838, 5.7609,\n",
       "           5.5491, 3.5689, 2.6900, 1.8447, 1.3552, 0.9356, 0.5138, 0.3680, 0.3450,\n",
       "           0.4049, 0.5800, 0.0000, 2.7084, 4.4543, 4.7722, 4.2959, 4.6446, 4.5374,\n",
       "           4.1914, 4.3378, 4.7125, 5.2098, 5.6134, 6.1306, 6.1202, 4.4077, 2.8568,\n",
       "           1.9688, 1.3118, 0.7987, 0.5400, 0.4387, 0.3439, 0.3285, 0.5674, 1.1465,\n",
       "           2.1430, 2.7915, 3.3767, 4.4708, 5.6479, 5.9618, 5.8765, 5.8546, 6.4053,\n",
       "           6.4085, 6.3662, 6.2701, 5.2582, 3.7314, 2.8390, 2.4400, 1.7969, 1.0193,\n",
       "           0.5856, 0.4791, 0.3419, 0.2994, 0.3669, 0.7754, 1.3751, 1.6073, 2.1608,\n",
       "           3.2456, 4.4120, 5.4040, 5.1775, 5.3957, 6.0588, 6.3513, 6.5405, 6.0621,\n",
       "           5.5437, 4.3814, 3.8972, 3.0443, 1.9286, 0.9993]]),\n",
       "  tensor([[0.5021, 0.3832, 0.2840, 0.4148, 0.7874, 2.0482, 4.1384, 5.8015, 5.3107,\n",
       "           4.7388, 4.7713, 4.4934, 4.3055, 4.1024, 4.4172, 4.4177, 4.6479, 5.1523,\n",
       "           4.6761, 2.9100, 2.0560, 1.4292, 1.0024, 0.6529, 0.4333, 0.3174, 0.2777,\n",
       "           0.3554, 0.6609, 1.5787, 3.1948, 5.1143, 5.1859, 4.7746, 4.9414, 4.7286,\n",
       "           4.4893, 4.3790, 4.7908, 4.5374, 5.1076, 5.4274, 4.9197, 3.1857, 2.2447,\n",
       "           1.6062, 1.2131, 0.7826, 0.4608, 0.3552, 0.2944, 0.3678, 0.7166, 1.6257,\n",
       "           3.2851, 5.1623, 5.1224, 4.8418, 5.0963, 4.8321, 3.5680, 4.0293, 4.5925,\n",
       "           4.8750, 5.1143, 6.1174, 5.3205, 3.2411, 2.2463, 1.6507, 1.1671, 0.8269,\n",
       "           0.5311, 0.3571, 0.3040, 0.3760, 0.7032, 1.6594, 3.2224, 5.1365, 5.2411,\n",
       "           4.8490, 5.1842, 4.8963, 4.2280, 4.2300, 4.7529, 4.7574, 5.1968, 5.7516,\n",
       "           5.2159, 3.5238, 2.2775, 1.7353, 1.2766, 0.8778, 0.5604, 0.3775, 0.3255,\n",
       "           0.4352, 0.7689, 1.6889, 3.3307, 5.0323, 4.9737, 4.8646, 5.2762, 5.1901,\n",
       "           4.4075, 4.7360, 5.3291, 5.7240, 6.1807, 6.8353, 6.6414, 5.3617, 3.4585,\n",
       "           2.4860, 1.6357, 1.1052, 0.7171, 0.5186, 0.3871, 0.4027, 0.5921, 1.4179,\n",
       "           2.5394, 3.2721, 3.9761, 5.2428, 6.2048, 6.4641, 6.4697, 6.0336, 6.6418,\n",
       "           6.6706, 6.8314, 6.6043, 5.8266, 4.3398, 3.4042, 2.7175, 1.9215, 1.1287,\n",
       "           0.6737, 0.4587, 0.3274, 0.2712, 0.3845, 0.7093, 1.2269, 1.6863, 2.2421,\n",
       "           3.5613, 4.8611, 5.5635, 5.5517, 5.7229, 6.1721, 6.7826, 6.3652, 6.2623,\n",
       "           5.5400, 4.5593, 3.8592, 3.0464, 1.7856, 1.0176]])]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T09:14:23.760001Z",
     "iopub.status.busy": "2022-08-29T09:14:23.759257Z",
     "iopub.status.idle": "2022-08-29T09:14:23.770067Z",
     "shell.execute_reply": "2022-08-29T09:14:23.768309Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.759945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMNet(\n",
       "  (lstm1): LSTM(168, 1024, num_layers=6, dropout=0.2)\n",
       "  (lstm2): LSTM(1024, 1024, num_layers=6, dropout=0.2)\n",
       "  (linear): Linear(in_features=1024, out_features=168, bias=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:05.762308Z",
     "iopub.status.busy": "2022-08-30T05:14:05.761750Z",
     "iopub.status.idle": "2022-08-30T05:14:05.769554Z",
     "shell.execute_reply": "2022-08-30T05:14:05.768582Z",
     "shell.execute_reply.started": "2022-08-30T05:14:05.762268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 168])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.unsqueeze(0).to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:21:34.080695Z",
     "iopub.status.busy": "2022-08-30T05:21:34.079798Z",
     "iopub.status.idle": "2022-08-30T05:21:34.171003Z",
     "shell.execute_reply": "2022-08-30T05:21:34.169836Z",
     "shell.execute_reply.started": "2022-08-30T05:21:34.080633Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.0115e-01, 4.0654e-01, 3.3562e-01, 3.3501e-01, 6.0530e-01,\n",
       "          1.7838e+00, 3.4000e+00, 4.6070e+00, 4.1054e+00, 3.5706e+00,\n",
       "          3.9485e+00, 4.0148e+00, 3.6865e+00, 3.8148e+00, 4.0195e+00,\n",
       "          3.8480e+00, 3.9731e+00, 4.2251e+00, 4.1114e+00, 3.3840e+00,\n",
       "          2.7890e+00, 2.1829e+00, 1.5696e+00, 1.0544e+00, 6.5339e-01,\n",
       "          4.3450e-01, 3.2338e-01, 3.2425e-01, 5.2340e-01, 1.4407e+00,\n",
       "          2.8307e+00, 3.9188e+00, 3.9647e+00, 3.7437e+00, 4.0676e+00,\n",
       "          4.0433e+00, 3.7248e+00, 3.7127e+00, 3.9253e+00, 3.8562e+00,\n",
       "          3.9897e+00, 4.3547e+00, 4.3503e+00, 3.5508e+00, 2.8340e+00,\n",
       "          2.2891e+00, 1.6891e+00, 1.0966e+00, 6.8289e-01, 4.5547e-01,\n",
       "          3.3967e-01, 3.1271e-01, 5.2808e-01, 1.4480e+00, 2.7657e+00,\n",
       "          3.8765e+00, 3.9229e+00, 3.7141e+00, 4.0341e+00, 3.9140e+00,\n",
       "          3.4911e+00, 3.7501e+00, 4.0235e+00, 3.9476e+00, 4.0857e+00,\n",
       "          4.3532e+00, 3.7619e+00, 3.4212e+00, 2.6421e+00, 2.2337e+00,\n",
       "          1.6395e+00, 1.1146e+00, 6.7838e-01, 4.7431e-01, 3.7010e-01,\n",
       "          3.2360e-01, 5.3429e-01, 1.4282e+00, 2.7990e+00, 3.9637e+00,\n",
       "          3.9620e+00, 3.6706e+00, 4.1242e+00, 4.0437e+00, 3.6316e+00,\n",
       "          3.9256e+00, 4.1282e+00, 4.0714e+00, 4.1446e+00, 4.4949e+00,\n",
       "          4.3529e+00, 3.4786e+00, 2.8662e+00, 2.2244e+00, 1.6839e+00,\n",
       "          1.0990e+00, 6.4717e-01, 4.7003e-01, 3.2748e-01, 2.9727e-01,\n",
       "          4.6477e-01, 2.2990e-03, 2.2460e+00, 3.4513e+00, 3.5113e+00,\n",
       "          3.3352e+00, 3.5800e+00, 3.8244e+00, 3.7056e+00, 3.9497e+00,\n",
       "          4.3656e+00, 4.4206e+00, 4.6984e+00, 4.9648e+00, 4.5571e+00,\n",
       "          3.7316e+00, 3.1861e+00, 2.3438e+00, 1.7904e+00, 1.1994e+00,\n",
       "          8.3523e-01, 5.8597e-01, 4.5589e-01, 3.9727e-01, 4.8314e-01,\n",
       "          1.0981e+00, 1.8358e+00, 2.4740e+00, 3.1498e+00, 4.2692e+00,\n",
       "          5.2721e+00, 5.5872e+00, 5.1586e+00, 5.1530e+00, 5.4737e+00,\n",
       "          5.6084e+00, 5.4442e+00, 5.2075e+00, 4.1201e+00, 3.2534e+00,\n",
       "          3.1004e+00, 2.7971e+00, 2.0232e+00, 1.1630e+00, 8.0506e-01,\n",
       "          6.3838e-01, 4.4894e-01, 3.3648e-01, 3.4715e-01, 6.7886e-01,\n",
       "          1.0892e+00, 1.4661e+00, 1.9638e+00, 2.9789e+00, 4.1881e+00,\n",
       "          4.8276e+00, 4.7050e+00, 4.9675e+00, 5.5808e+00, 5.7836e+00,\n",
       "          5.6788e+00, 5.1014e+00, 4.3127e+00, 3.5002e+00, 3.5478e+00,\n",
       "          3.0185e+00, 1.9487e+00, 1.0179e+00]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-30T08:51:24.594680Z",
     "iopub.status.idle": "2022-08-30T08:51:24.594994Z",
     "shell.execute_reply": "2022-08-30T08:51:24.594848Z",
     "shell.execute_reply.started": "2022-08-30T08:51:24.594834Z"
    },
    "id": "F7imwUUCzR_f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n",
    "                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n",
    "\n",
    "for iter_, sample in enumerate(test_dataloader):\n",
    "\n",
    "    timestamp, category, (input_data, output_data) = sample\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "    pred = model(input_data,category)\n",
    "\n",
    "    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n",
    "        for cat, row in zip(category, pred[0]):\n",
    "            cat = f'{cat}'\n",
    "            submission_table[cat] = row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-30T08:51:24.596112Z",
     "iopub.status.idle": "2022-08-30T08:51:24.596418Z",
     "shell.execute_reply": "2022-08-30T08:51:24.596277Z",
     "shell.execute_reply.started": "2022-08-30T08:51:24.596263Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:23:52.553680Z",
     "iopub.status.busy": "2022-08-30T05:23:52.552756Z",
     "iopub.status.idle": "2022-08-30T05:23:52.573816Z",
     "shell.execute_reply": "2022-08-30T05:23:52.572373Z",
     "shell.execute_reply.started": "2022-08-30T05:23:52.553617Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_table.iloc[:,1:]=submission_table.iloc[:,1:]*4609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:25:04.037376Z",
     "iopub.status.busy": "2022-08-30T05:25:04.036464Z",
     "iopub.status.idle": "2022-08-30T05:25:04.340832Z",
     "shell.execute_reply": "2022-08-30T05:25:04.339988Z",
     "shell.execute_reply.started": "2022-08-30T05:25:04.037313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc328e442e0>]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1058.4x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(submission_table['10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:26:48.981508Z",
     "iopub.status.busy": "2022-08-30T05:26:48.980935Z",
     "iopub.status.idle": "2022-08-30T05:26:49.023989Z",
     "shell.execute_reply": "2022-08-30T05:26:49.023126Z",
     "shell.execute_reply.started": "2022-08-30T05:26:48.981472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>160</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>1020</th>\n",
       "      <th>1040</th>\n",
       "      <th>1100</th>\n",
       "      <th>1200</th>\n",
       "      <th>1510</th>\n",
       "      <th>2510</th>\n",
       "      <th>3000</th>\n",
       "      <th>4510</th>\n",
       "      <th>5510</th>\n",
       "      <th>6000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200525_0</td>\n",
       "      <td>2309.808097</td>\n",
       "      <td>2309.632828</td>\n",
       "      <td>2309.599861</td>\n",
       "      <td>2309.607279</td>\n",
       "      <td>2309.569642</td>\n",
       "      <td>2309.587774</td>\n",
       "      <td>2309.741616</td>\n",
       "      <td>2309.659200</td>\n",
       "      <td>2309.653157</td>\n",
       "      <td>...</td>\n",
       "      <td>2309.572390</td>\n",
       "      <td>2309.653157</td>\n",
       "      <td>2309.636399</td>\n",
       "      <td>2309.720188</td>\n",
       "      <td>2309.650959</td>\n",
       "      <td>2309.657277</td>\n",
       "      <td>2309.554808</td>\n",
       "      <td>2309.600960</td>\n",
       "      <td>2309.600136</td>\n",
       "      <td>2309.606180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200525_1</td>\n",
       "      <td>1873.756900</td>\n",
       "      <td>1873.576136</td>\n",
       "      <td>1873.541796</td>\n",
       "      <td>1873.550312</td>\n",
       "      <td>1873.512676</td>\n",
       "      <td>1873.530807</td>\n",
       "      <td>1873.687671</td>\n",
       "      <td>1873.603333</td>\n",
       "      <td>1873.596877</td>\n",
       "      <td>...</td>\n",
       "      <td>1873.516110</td>\n",
       "      <td>1873.596877</td>\n",
       "      <td>1873.579982</td>\n",
       "      <td>1873.665694</td>\n",
       "      <td>1873.594542</td>\n",
       "      <td>1873.601272</td>\n",
       "      <td>1873.497566</td>\n",
       "      <td>1873.543994</td>\n",
       "      <td>1873.543170</td>\n",
       "      <td>1873.549076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200525_2</td>\n",
       "      <td>1546.873065</td>\n",
       "      <td>1546.614830</td>\n",
       "      <td>1546.566343</td>\n",
       "      <td>1546.577331</td>\n",
       "      <td>1546.523899</td>\n",
       "      <td>1546.549722</td>\n",
       "      <td>1546.774304</td>\n",
       "      <td>1546.652879</td>\n",
       "      <td>1546.644637</td>\n",
       "      <td>...</td>\n",
       "      <td>1546.527745</td>\n",
       "      <td>1546.644363</td>\n",
       "      <td>1546.619363</td>\n",
       "      <td>1546.742299</td>\n",
       "      <td>1546.641341</td>\n",
       "      <td>1546.650681</td>\n",
       "      <td>1546.502471</td>\n",
       "      <td>1546.568678</td>\n",
       "      <td>1546.567304</td>\n",
       "      <td>1546.576370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200525_3</td>\n",
       "      <td>1544.068471</td>\n",
       "      <td>1543.540326</td>\n",
       "      <td>1543.441153</td>\n",
       "      <td>1543.464367</td>\n",
       "      <td>1543.354479</td>\n",
       "      <td>1543.408874</td>\n",
       "      <td>1543.866828</td>\n",
       "      <td>1543.619994</td>\n",
       "      <td>1543.601313</td>\n",
       "      <td>...</td>\n",
       "      <td>1543.363408</td>\n",
       "      <td>1543.601039</td>\n",
       "      <td>1543.551177</td>\n",
       "      <td>1543.802682</td>\n",
       "      <td>1543.595544</td>\n",
       "      <td>1543.613676</td>\n",
       "      <td>1543.312036</td>\n",
       "      <td>1543.446373</td>\n",
       "      <td>1543.444724</td>\n",
       "      <td>1543.462444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200525_4</td>\n",
       "      <td>2789.849720</td>\n",
       "      <td>2788.958261</td>\n",
       "      <td>2788.789035</td>\n",
       "      <td>2788.828869</td>\n",
       "      <td>2788.641236</td>\n",
       "      <td>2788.733267</td>\n",
       "      <td>2789.510444</td>\n",
       "      <td>2789.092323</td>\n",
       "      <td>2789.061280</td>\n",
       "      <td>...</td>\n",
       "      <td>2788.657170</td>\n",
       "      <td>2789.061280</td>\n",
       "      <td>2788.975293</td>\n",
       "      <td>2789.401106</td>\n",
       "      <td>2789.051115</td>\n",
       "      <td>2789.082159</td>\n",
       "      <td>2788.568711</td>\n",
       "      <td>2788.798375</td>\n",
       "      <td>2788.793980</td>\n",
       "      <td>2788.823649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>20200531_19</td>\n",
       "      <td>16132.329648</td>\n",
       "      <td>16132.713154</td>\n",
       "      <td>16132.788976</td>\n",
       "      <td>16132.770295</td>\n",
       "      <td>16132.851612</td>\n",
       "      <td>16132.812052</td>\n",
       "      <td>16132.473600</td>\n",
       "      <td>16132.656013</td>\n",
       "      <td>16132.671397</td>\n",
       "      <td>...</td>\n",
       "      <td>16132.845018</td>\n",
       "      <td>16132.669199</td>\n",
       "      <td>16132.707659</td>\n",
       "      <td>16132.521950</td>\n",
       "      <td>16132.673594</td>\n",
       "      <td>16132.659309</td>\n",
       "      <td>16132.885677</td>\n",
       "      <td>16132.782383</td>\n",
       "      <td>16132.783482</td>\n",
       "      <td>16132.771394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>20200531_20</td>\n",
       "      <td>16351.644566</td>\n",
       "      <td>16352.252242</td>\n",
       "      <td>16352.372019</td>\n",
       "      <td>16352.343448</td>\n",
       "      <td>16352.473115</td>\n",
       "      <td>16352.410479</td>\n",
       "      <td>16351.875329</td>\n",
       "      <td>16352.161035</td>\n",
       "      <td>16352.183013</td>\n",
       "      <td>...</td>\n",
       "      <td>16352.464324</td>\n",
       "      <td>16352.183013</td>\n",
       "      <td>16352.242352</td>\n",
       "      <td>16351.947854</td>\n",
       "      <td>16352.189606</td>\n",
       "      <td>16352.167629</td>\n",
       "      <td>16352.524762</td>\n",
       "      <td>16352.364326</td>\n",
       "      <td>16352.367623</td>\n",
       "      <td>16352.346745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20200531_21</td>\n",
       "      <td>13912.106478</td>\n",
       "      <td>13912.654815</td>\n",
       "      <td>13912.759207</td>\n",
       "      <td>13912.736131</td>\n",
       "      <td>13912.850414</td>\n",
       "      <td>13912.795470</td>\n",
       "      <td>13912.313066</td>\n",
       "      <td>13912.572399</td>\n",
       "      <td>13912.591080</td>\n",
       "      <td>...</td>\n",
       "      <td>13912.842722</td>\n",
       "      <td>13912.591080</td>\n",
       "      <td>13912.642727</td>\n",
       "      <td>13912.381196</td>\n",
       "      <td>13912.595476</td>\n",
       "      <td>13912.576795</td>\n",
       "      <td>13912.896566</td>\n",
       "      <td>13912.753713</td>\n",
       "      <td>13912.755911</td>\n",
       "      <td>13912.737230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20200531_22</td>\n",
       "      <td>8981.435551</td>\n",
       "      <td>8981.717411</td>\n",
       "      <td>8981.771805</td>\n",
       "      <td>8981.759168</td>\n",
       "      <td>8981.820156</td>\n",
       "      <td>8981.789937</td>\n",
       "      <td>8981.541042</td>\n",
       "      <td>8981.674555</td>\n",
       "      <td>8981.684445</td>\n",
       "      <td>...</td>\n",
       "      <td>8981.814112</td>\n",
       "      <td>8981.684445</td>\n",
       "      <td>8981.711367</td>\n",
       "      <td>8981.576756</td>\n",
       "      <td>8981.688291</td>\n",
       "      <td>8981.678401</td>\n",
       "      <td>8981.843232</td>\n",
       "      <td>8981.768509</td>\n",
       "      <td>8981.770707</td>\n",
       "      <td>8981.760817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>20200531_23</td>\n",
       "      <td>4691.543361</td>\n",
       "      <td>4691.610941</td>\n",
       "      <td>4691.624128</td>\n",
       "      <td>4691.621381</td>\n",
       "      <td>4691.635666</td>\n",
       "      <td>4691.629073</td>\n",
       "      <td>4691.569734</td>\n",
       "      <td>4691.601601</td>\n",
       "      <td>4691.603799</td>\n",
       "      <td>...</td>\n",
       "      <td>4691.634567</td>\n",
       "      <td>4691.603799</td>\n",
       "      <td>4691.609843</td>\n",
       "      <td>4691.577975</td>\n",
       "      <td>4691.604348</td>\n",
       "      <td>4691.602150</td>\n",
       "      <td>4691.641160</td>\n",
       "      <td>4691.623578</td>\n",
       "      <td>4691.624128</td>\n",
       "      <td>4691.621381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp            10           100           101           120  \\\n",
       "0     20200525_0   2309.808097   2309.632828   2309.599861   2309.607279   \n",
       "1     20200525_1   1873.756900   1873.576136   1873.541796   1873.550312   \n",
       "2     20200525_2   1546.873065   1546.614830   1546.566343   1546.577331   \n",
       "3     20200525_3   1544.068471   1543.540326   1543.441153   1543.464367   \n",
       "4     20200525_4   2789.849720   2788.958261   2788.789035   2788.828869   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "163  20200531_19  16132.329648  16132.713154  16132.788976  16132.770295   \n",
       "164  20200531_20  16351.644566  16352.252242  16352.372019  16352.343448   \n",
       "165  20200531_21  13912.106478  13912.654815  13912.759207  13912.736131   \n",
       "166  20200531_22   8981.435551   8981.717411   8981.771805   8981.759168   \n",
       "167  20200531_23   4691.543361   4691.610941   4691.624128   4691.621381   \n",
       "\n",
       "              121           140           150           160           200  \\\n",
       "0     2309.569642   2309.587774   2309.741616   2309.659200   2309.653157   \n",
       "1     1873.512676   1873.530807   1873.687671   1873.603333   1873.596877   \n",
       "2     1546.523899   1546.549722   1546.774304   1546.652879   1546.644637   \n",
       "3     1543.354479   1543.408874   1543.866828   1543.619994   1543.601313   \n",
       "4     2788.641236   2788.733267   2789.510444   2789.092323   2789.061280   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "163  16132.851612  16132.812052  16132.473600  16132.656013  16132.671397   \n",
       "164  16352.473115  16352.410479  16351.875329  16352.161035  16352.183013   \n",
       "165  13912.850414  13912.795470  13912.313066  13912.572399  13912.591080   \n",
       "166   8981.820156   8981.789937   8981.541042   8981.674555   8981.684445   \n",
       "167   4691.635666   4691.629073   4691.569734   4691.601601   4691.603799   \n",
       "\n",
       "     ...          1020          1040          1100          1200  \\\n",
       "0    ...   2309.572390   2309.653157   2309.636399   2309.720188   \n",
       "1    ...   1873.516110   1873.596877   1873.579982   1873.665694   \n",
       "2    ...   1546.527745   1546.644363   1546.619363   1546.742299   \n",
       "3    ...   1543.363408   1543.601039   1543.551177   1543.802682   \n",
       "4    ...   2788.657170   2789.061280   2788.975293   2789.401106   \n",
       "..   ...           ...           ...           ...           ...   \n",
       "163  ...  16132.845018  16132.669199  16132.707659  16132.521950   \n",
       "164  ...  16352.464324  16352.183013  16352.242352  16351.947854   \n",
       "165  ...  13912.842722  13912.591080  13912.642727  13912.381196   \n",
       "166  ...   8981.814112   8981.684445   8981.711367   8981.576756   \n",
       "167  ...   4691.634567   4691.603799   4691.609843   4691.577975   \n",
       "\n",
       "             1510          2510          3000          4510          5510  \\\n",
       "0     2309.650959   2309.657277   2309.554808   2309.600960   2309.600136   \n",
       "1     1873.594542   1873.601272   1873.497566   1873.543994   1873.543170   \n",
       "2     1546.641341   1546.650681   1546.502471   1546.568678   1546.567304   \n",
       "3     1543.595544   1543.613676   1543.312036   1543.446373   1543.444724   \n",
       "4     2789.051115   2789.082159   2788.568711   2788.798375   2788.793980   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "163  16132.673594  16132.659309  16132.885677  16132.782383  16132.783482   \n",
       "164  16352.189606  16352.167629  16352.524762  16352.364326  16352.367623   \n",
       "165  13912.595476  13912.576795  13912.896566  13912.753713  13912.755911   \n",
       "166   8981.688291   8981.678401   8981.843232   8981.768509   8981.770707   \n",
       "167   4691.604348   4691.602150   4691.641160   4691.623578   4691.624128   \n",
       "\n",
       "             6000  \n",
       "0     2309.606180  \n",
       "1     1873.549076  \n",
       "2     1546.576370  \n",
       "3     1543.462444  \n",
       "4     2788.823649  \n",
       "..            ...  \n",
       "163  16132.771394  \n",
       "164  16352.346745  \n",
       "165  13912.737230  \n",
       "166   8981.760817  \n",
       "167   4691.621381  \n",
       "\n",
       "[168 rows x 36 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:11:10.560579Z",
     "iopub.status.busy": "2022-08-30T05:11:10.559666Z",
     "iopub.status.idle": "2022-08-30T05:11:10.653171Z",
     "shell.execute_reply": "2022-08-30T05:11:10.651676Z",
     "shell.execute_reply.started": "2022-08-30T05:11:10.560517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skxCavbHcihQ"
   },
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-29T09:14:23.823021Z",
     "iopub.status.idle": "2022-08-29T09:14:23.823342Z",
     "shell.execute_reply": "2022-08-29T09:14:23.823196Z",
     "shell.execute_reply.started": "2022-08-29T09:14:23.823179Z"
    },
    "id": "z5NYGpaXzR_f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_table.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A02xb7X7zR_f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline_LSTM_EDA.ipynb",
   "provenance": [
    {
     "file_id": "1vSmFmhSZCYcsZ-0WHufbMvcFErsCEoaO",
     "timestamp": 1658285407292
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
